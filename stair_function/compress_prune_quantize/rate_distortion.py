{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Set, Tuple\n",
    "import random\n",
    "from functools import partial\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from mpi4py import MPI\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, d: int, hidden_size: int, depth: int, mode: str = 'special', gamma: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        # ... rest of initialization code stays the same ...\n",
    "\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dim = d\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = d\n",
    "        self.layer_lrs = []  # Store layerwise learning rates\n",
    "        \n",
    "        for layer_idx in range(depth):\n",
    "            linear = nn.Linear(prev_dim, hidden_size)\n",
    "            \n",
    "            if mode == 'special':\n",
    "                # Special initialization as in original code\n",
    "                gain = nn.init.calculate_gain('relu')\n",
    "                std = gain / np.sqrt(prev_dim)\n",
    "                nn.init.normal_(linear.weight, mean=0.0, std=std)\n",
    "                nn.init.zeros_(linear.bias)\n",
    "                self.layer_lrs.append(1.0)\n",
    "                \n",
    "            elif mode == 'spectral':\n",
    "                # Implement spectral initialization\n",
    "                fan_in = prev_dim\n",
    "                fan_out = hidden_size\n",
    "                std = (1.0 / np.sqrt(fan_in)) * min(1.0, np.sqrt(fan_out / fan_in))\n",
    "                nn.init.normal_(linear.weight, mean=0.0, std=std)\n",
    "                nn.init.zeros_(linear.bias)\n",
    "                self.layer_lrs.append(float(fan_out) / fan_in)\n",
    "                \n",
    "            elif mode == 'mup_pennington':\n",
    "                # muP initialization and learning rates from the paper\n",
    "                if layer_idx == 0:  # Embedding layer\n",
    "                    std = 1.0 / np.sqrt(prev_dim)\n",
    "                    lr_scale = 1.0  # O(1) learning rate for embedding\n",
    "                else:  # Hidden layers\n",
    "                    std = 1.0 / np.sqrt(prev_dim)\n",
    "                    lr_scale = 1.0 / prev_dim  # O(1/n) learning rate for hidden\n",
    "                nn.init.normal_(linear.weight, mean=0.0, std=std)\n",
    "                nn.init.zeros_(linear.bias)\n",
    "                self.layer_lrs.append(lr_scale)\n",
    "                \n",
    "            else:  # standard\n",
    "                nn.init.xavier_uniform_(linear.weight)\n",
    "                nn.init.zeros_(linear.bias)\n",
    "                self.layer_lrs.append(1.0)\n",
    "            \n",
    "            layers.extend([\n",
    "                linear,\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_size\n",
    "        \n",
    "        # Final layer\n",
    "        final_layer = nn.Linear(prev_dim, 1)\n",
    "        if mode == 'special':\n",
    "            nn.init.normal_(final_layer.weight, std=0.01)\n",
    "            self.layer_lrs.append(1.0)\n",
    "        elif mode == 'spectral':\n",
    "            fan_in = prev_dim\n",
    "            fan_out = 1\n",
    "            std = (1.0 / np.sqrt(fan_in)) * min(1.0, np.sqrt(fan_out / fan_in))\n",
    "            nn.init.normal_(final_layer.weight, std=std)\n",
    "            self.layer_lrs.append(float(fan_out) / fan_in)\n",
    "        elif mode == 'mup_pennington':\n",
    "            std = 1.0 / np.sqrt(prev_dim)\n",
    "            lr_scale = 1.0 / prev_dim  # O(1/n) learning rate for readout\n",
    "            nn.init.normal_(final_layer.weight, std=std)\n",
    "            self.layer_lrs.append(lr_scale)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(final_layer.weight)\n",
    "            self.layer_lrs.append(1.0)\n",
    "            \n",
    "        nn.init.zeros_(final_layer.bias)\n",
    "        layers.append(final_layer)\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gamma = float(self.gamma)  # Ensure gamma is a float\n",
    "        return self.network(x).squeeze() / gamma\n",
    "    \n",
    "    def get_layer_learning_rates(self, base_lr: float) -> List[float]:\n",
    "        \"\"\"Return list of learning rates for each layer\"\"\"\n",
    "        return [base_lr * lr for lr in self.layer_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing models with parameters:\n",
      "hidden_sizes: [400]\n",
      "depths: [4]\n",
      "n_train_sizes: [10, 100, 1000, 5000, 10000, 20000]\n",
      "learning_rates: [0.005]\n",
      "\n",
      "Analyzing model: h400_d4_n10_lr0.005_g1.0_mup_pennington\n",
      "Analyzing layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_237433/331345753.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_dict = torch.load(data_path)\n",
      "/tmp/ipykernel_237433/331345753.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "/tmp/ipykernel_237433/331345753.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_dict = torch.load(data_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from torch.distributions import Normal, kl_divergence, Laplace, StudentT\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "from pathlib import Path\n",
    "\n",
    "class PriorDistribution:\n",
    "    \"\"\"Different choices of prior distributions\"\"\"\n",
    "    @staticmethod\n",
    "    def get_prior(name: str, latent_dim: int, device: torch.device):\n",
    "        if name == \"normal\":\n",
    "            return Normal(torch.zeros(latent_dim).to(device), torch.ones(latent_dim).to(device))\n",
    "        elif name == \"laplace\":\n",
    "            return Laplace(torch.zeros(latent_dim).to(device), torch.ones(latent_dim).to(device))\n",
    "        elif name == \"student\":\n",
    "            # Student's t with 5 degrees of freedom\n",
    "            return StudentT(5, torch.zeros(latent_dim).to(device), torch.ones(latent_dim).to(device))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prior: {name}\")\n",
    "\n",
    "class RateDistortionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, prior: str = \"normal\"):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.prior_name = prior\n",
    "        \n",
    "        # More sophisticated encoder architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2*input_dim),\n",
    "            nn.LayerNorm(2*input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*input_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, 2 * latent_dim)  # mu and logvar\n",
    "        )\n",
    "        \n",
    "        # More sophisticated decoder architecture\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim),\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, 2*input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*input_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> tuple:\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=-1)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, beta: float) -> tuple:\n",
    "        mu, logvar = self.encode(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        \n",
    "        # Get prior\n",
    "        prior = PriorDistribution.get_prior(self.prior_name, self.latent_dim, x.device)\n",
    "        \n",
    "        # Sample using reparameterization trick\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        # Decode\n",
    "        recon = self.decode(z)\n",
    "        \n",
    "        # Compute KL divergence based on prior\n",
    "        if self.prior_name == \"normal\":\n",
    "            rate = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n",
    "        else:\n",
    "            q_z = Normal(mu, std)\n",
    "            rate = kl_divergence(q_z, prior).sum(dim=1).mean()\n",
    "        \n",
    "        # Compute distortion\n",
    "        distortion = torch.mean((recon - x).pow(2))\n",
    "        \n",
    "        loss = rate + beta * distortion\n",
    "        return loss, rate.item(), distortion.item()\n",
    "\n",
    "def verify_dataset_match(data_path: str, n_train: int, rank: int) -> bool:\n",
    "    \"\"\"Verify that dataset matches expected parameters\"\"\"\n",
    "    try:\n",
    "        data_dict = torch.load(data_path)\n",
    "        return (data_dict['shape_X'][0] == n_train and \n",
    "                data_dict['saved_by_rank'] == rank)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def load_model_and_data(model_path: str, data_path: str, n_train: int, rank: int,\n",
    "                       d: int, hidden_size: int, depth: int, mode: str, gamma: float) -> Tuple[nn.Module, torch.Tensor]:\n",
    "    \"\"\"Load model and corresponding dataset with verification\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Verify paths exist\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Dataset not found: {data_path}\")\n",
    "    \n",
    "    # Verify dataset matches parameters\n",
    "    if not verify_dataset_match(data_path, n_train, rank):\n",
    "        raise ValueError(f\"Dataset mismatch for {data_path}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = DeepNN(d, hidden_size, depth, mode=mode, gamma=gamma).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Load dataset\n",
    "    data_dict = torch.load(data_path)\n",
    "    X = data_dict['X'].to(device)\n",
    "    \n",
    "    return model, X\n",
    "\n",
    "def plot_rd_curves(results: Dict, save_dir: str):\n",
    "    \"\"\"Plot Rate-Distortion curves with comparisons\"\"\"\n",
    "    # Create save directory\n",
    "    save_dir = Path(save_dir) / \"rd_plots\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('seaborn')\n",
    "    \n",
    "    # Group results by model configuration\n",
    "    configs = {}\n",
    "    for result in results:\n",
    "        config = (\n",
    "            result['model_config']['hidden_size'],\n",
    "            result['model_config']['depth'],\n",
    "            result['model_config']['n_train']\n",
    "        )\n",
    "        if config not in configs:\n",
    "            configs[config] = []\n",
    "        configs[config].append(result)\n",
    "    \n",
    "    # Plot for each configuration\n",
    "    for (hidden_size, depth, n_train), model_results in configs.items():\n",
    "        # Plot R(D) curves for each layer\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        for result in model_results:\n",
    "            for layer_data in result['layer_results']:\n",
    "                rd_points = layer_data['rd_points']\n",
    "                rates = [p['rate'] for p in rd_points]\n",
    "                distortions = [p['distortion'] for p in rd_points]\n",
    "                \n",
    "                ax.scatter(distortions, rates, alpha=0.5, \n",
    "                          label=f\"Layer {layer_data['layer_idx']+1}\")\n",
    "                \n",
    "                # Fit and plot curve\n",
    "                try:\n",
    "                    z = np.polyfit(distortions, rates, 2)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_new = np.linspace(min(distortions), max(distortions), 100)\n",
    "                    ax.plot(x_new, p(x_new), '--', alpha=0.5)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        ax.set_xlabel('Distortion')\n",
    "        ax.set_ylabel('Rate (bits)')\n",
    "        ax.set_title(f'R(D) Curves (h={hidden_size}, d={depth}, n={n_train})')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(save_dir / f'rd_curve_h{hidden_size}_d{depth}_n{n_train}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot comparisons across training sizes\n",
    "    for hidden_size in set(c[0] for c in configs):\n",
    "        for depth in set(c[1] for c in configs):\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            for n_train in sorted(set(c[2] for c in configs)):\n",
    "                config = (hidden_size, depth, n_train)\n",
    "                if config not in configs:\n",
    "                    continue\n",
    "                \n",
    "                # Average over layers\n",
    "                rates = []\n",
    "                distortions = []\n",
    "                for result in configs[config]:\n",
    "                    for layer_data in result['layer_results']:\n",
    "                        rd_points = layer_data['rd_points']\n",
    "                        rates.extend(p['rate'] for p in rd_points)\n",
    "                        distortions.extend(p['distortion'] for p in rd_points)\n",
    "                \n",
    "                # Plot density\n",
    "                xy = np.vstack([distortions, rates])\n",
    "                z = gaussian_kde(xy)(xy)\n",
    "                \n",
    "                ax.scatter(distortions, rates, c=z, s=100, alpha=0.5,\n",
    "                          label=f'n={n_train}')\n",
    "            \n",
    "            ax.set_xlabel('Distortion')\n",
    "            ax.set_ylabel('Rate (bits)')\n",
    "            ax.set_title(f'R(D) Comparison (h={hidden_size}, d={depth})')\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.savefig(save_dir / f'rd_comparison_h{hidden_size}_d{depth}.png')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def compute_layer_activations(model: nn.Module, X: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\"Get activations for each layer\"\"\"\n",
    "    activations = []\n",
    "    x = X\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for layer in model.network[:-1]:  # Exclude final layer\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = layer(x)\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                x = layer(x)\n",
    "                activations.append(x)\n",
    "    \n",
    "    return activations\n",
    "\n",
    "\n",
    "def analyze_model_rd(model_path: str, data_path: str, n_train: int, rank: int,\n",
    "                    d: int, hidden_size: int, depth: int, mode: str, gamma: float,\n",
    "                    priors: List[str] = [\"normal\", \"laplace\", \"student\"]) -> Dict:\n",
    "    \"\"\"Analyze Rate-Distortion with multiple priors\"\"\"\n",
    "    # Load model and data\n",
    "    model, X = load_model_and_data(model_path, data_path, n_train, rank,\n",
    "                                  d, hidden_size, depth, mode, gamma)\n",
    "    \n",
    "    # Get activations\n",
    "    activations_list = compute_layer_activations(model, X)\n",
    "    \n",
    "    # Parameters for R(D) analysis\n",
    "    latent_dims = [2, 4, 8, 16, 32]\n",
    "    betas = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "    \n",
    "    # Analyze each layer\n",
    "    layer_results = []\n",
    "    for layer_idx, activations in enumerate(activations_list):\n",
    "        print(f\"Analyzing layer {layer_idx+1}\")\n",
    "        \n",
    "        layer_rd_points = []\n",
    "        for prior in priors:\n",
    "            rd_points = compute_rd_curve(activations, latent_dims, betas, prior=prior)\n",
    "            for point in rd_points:\n",
    "                point['prior'] = prior\n",
    "            layer_rd_points.extend(rd_points)\n",
    "        \n",
    "        layer_results.append({\n",
    "            'layer_idx': layer_idx,\n",
    "            'rd_points': layer_rd_points,\n",
    "            'activation_shape': list(activations.shape)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'model_path': model_path,\n",
    "        'data_path': data_path,\n",
    "        'model_config': {\n",
    "            'd': d,\n",
    "            'hidden_size': hidden_size,\n",
    "            'depth': depth,\n",
    "            'mode': mode,\n",
    "            'gamma': gamma,\n",
    "            'n_train': n_train\n",
    "        },\n",
    "        'layer_results': layer_results\n",
    "    }\n",
    "\n",
    "def compute_rd_curve(activations: torch.Tensor, latent_dims: List[int],\n",
    "                    betas: List[float], prior: str = \"normal\",\n",
    "                    epochs: int = 100) -> List[Dict]:\n",
    "    \"\"\"Compute rate-distortion curve with specified prior\"\"\"\n",
    "    device = activations.device\n",
    "    input_dim = activations.shape[1]\n",
    "    rd_points = []\n",
    "    \n",
    "    for latent_dim in latent_dims:\n",
    "        encoder = RateDistortionEncoder(input_dim, latent_dim, prior=prior).to(device)\n",
    "        optimizer = torch.optim.Adam(encoder.parameters())\n",
    "        \n",
    "        for beta in betas:\n",
    "            # Train VAE\n",
    "            for epoch in range(epochs):\n",
    "                optimizer.zero_grad()\n",
    "                loss, rate, distortion = encoder(activations, beta)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Record final values\n",
    "            with torch.no_grad():\n",
    "                _, rate, distortion = encoder(activations, beta)\n",
    "                rd_points.append({\n",
    "                    'latent_dim': latent_dim,\n",
    "                    'beta': beta,\n",
    "                    'rate': rate,\n",
    "                    'distortion': distortion,\n",
    "                    'prior': prior\n",
    "                })\n",
    "    \n",
    "    return rd_points\n",
    "\n",
    "\n",
    "def analyze_directory(results_dir: str, timestamp: str, \n",
    "                     filter_params: Dict[str, List] = None):\n",
    "    \"\"\"\n",
    "    Analyze models with specified hyperparameters\n",
    "    \n",
    "    Args:\n",
    "        results_dir: Directory containing results\n",
    "        timestamp: Timestamp of experiment\n",
    "        filter_params: Dict containing hyperparameters to filter by, e.g.,\n",
    "            {\n",
    "                'hidden_sizes': [400],\n",
    "                'depths': [1],\n",
    "                'n_train_sizes': [1000, 5000],\n",
    "                'learning_rates': [0.005]\n",
    "            }\n",
    "    \"\"\"\n",
    "    # Load hyperparameters\n",
    "    with open(os.path.join(results_dir, f'hyperparameters_{timestamp}.json'), 'r') as f:\n",
    "        hyperparams = json.load(f)\n",
    "    \n",
    "    # Use filtered or full hyperparameters\n",
    "    search_space = {\n",
    "        'hidden_sizes': filter_params.get('hidden_sizes', hyperparams['hidden_sizes']),\n",
    "        'depths': filter_params.get('depths', hyperparams['depths']),\n",
    "        'n_train_sizes': filter_params.get('n_train_sizes', hyperparams['n_train_sizes']),\n",
    "        'learning_rates': filter_params.get('learning_rates', hyperparams['learning_rates'])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAnalyzing models with parameters:\")\n",
    "    for param, values in search_space.items():\n",
    "        print(f\"{param}: {values}\")\n",
    "    \n",
    "    results = []\n",
    "    for hidden_size in search_space['hidden_sizes']:\n",
    "        for depth in search_space['depths']:\n",
    "            for n_train in search_space['n_train_sizes']:\n",
    "                for lr in search_space['learning_rates']:\n",
    "                    model_prefix = f'h{hidden_size}_d{depth}_n{n_train}_lr{lr}_g{hyperparams[\"gamma\"]}_{hyperparams[\"mode\"]}'\n",
    "                    \n",
    "                    model_path = os.path.join(results_dir, f'final_model_{model_prefix}_{timestamp}_rank0.pt')\n",
    "                    data_path = os.path.join(results_dir, f'train_dataset_{model_prefix}_{timestamp}_rank0.pt')\n",
    "                    \n",
    "                    if not os.path.exists(model_path) or not os.path.exists(data_path):\n",
    "                        print(f\"Skipping {model_prefix} - files not found\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        print(f\"\\nAnalyzing model: {model_prefix}\")\n",
    "                        result = analyze_model_rd(\n",
    "                            model_path, data_path, n_train, 0,\n",
    "                            hyperparams['d'], hidden_size, depth,\n",
    "                            hyperparams['mode'], hyperparams['gamma']\n",
    "                        )\n",
    "                        results.append(result)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error analyzing {model_prefix}: {e}\")\n",
    "                        continue\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No models found matching the specified parameters!\")\n",
    "        return None\n",
    "    \n",
    "    # Save results\n",
    "    save_path = os.path.join(results_dir, f'rate_distortion_analysis_{timestamp}.json')\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    plot_rd_curves(results, results_dir)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    results_dir = \"/mnt/users/goringn/NNs_vs_Kernels/stair_function/results/msp_anthro_false_mup_lr0005_gamma_1_modelsaved\"\n",
    "    timestamp = \"20250125_153135\"  # Replace with actual timestamp\n",
    "    \n",
    "    # Specify which models to analyze\n",
    "    filter_params = {\n",
    "        'hidden_sizes': [400],        # Only analyze models with hidden size 400\n",
    "        'depths': [4],                # Only analyze models with depth 1\n",
    "        'n_train_sizes': [10,100, 1000, 5000,10000,20000],  # Analyze these training sizes\n",
    "        'learning_rates': [0.005]     # Only this learning rate\n",
    "    }\n",
    "    \n",
    "    # Analysis parameters\n",
    "    analysis_params = {\n",
    "        'latent_dims': [2, 4, 8, 16, 32],\n",
    "        'betas': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        'priors': ['normal', 'laplace', 'student'],\n",
    "        'epochs': 100\n",
    "    }\n",
    "    \n",
    "    # Run analysis\n",
    "    results = analyze_directory(\n",
    "        results_dir=results_dir,\n",
    "        timestamp=timestamp,\n",
    "        filter_params=filter_params\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nAnalysis complete! Results saved and plots generated.\")\n",
    "        # Print some summary statistics\n",
    "        n_models = len(results)\n",
    "        avg_rate = np.mean([\n",
    "            point['rate'] \n",
    "            for result in results \n",
    "            for layer in result['layer_results'] \n",
    "            for point in layer['rd_points']\n",
    "        ])\n",
    "        print(f\"\\nAnalyzed {n_models} models\")\n",
    "        print(f\"Average rate across all models: {avg_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dev_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
