{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(44)\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, d: int, hidden_size: int, depth: int, mode: str = 'special'):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = d\n",
    "        for _ in range(depth):\n",
    "            linear = nn.Linear(prev_dim, hidden_size)\n",
    "\n",
    "            if mode == 'special':\n",
    "                # Special initialization as in original code\n",
    "                gain = nn.init.calculate_gain('relu')\n",
    "                std = gain / np.sqrt(prev_dim)\n",
    "                nn.init.normal_(linear.weight, mean=0.0, std=std)\n",
    "                nn.init.zeros_(linear.bias)\n",
    "            else:\n",
    "                # Standard PyTorch initialization\n",
    "                nn.init.xavier_uniform_(linear.weight)\n",
    "                nn.init.zeros_(linear.bias)\n",
    "\n",
    "            layers.extend([\n",
    "                linear,\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_size\n",
    "\n",
    "        final_layer = nn.Linear(prev_dim, 1)\n",
    "        if mode == 'special':\n",
    "            nn.init.normal_(final_layer.weight, std=0.01)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(final_layer.weight)\n",
    "        nn.init.zeros_(final_layer.bias)\n",
    "        layers.append(final_layer)\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        mse = torch.mean((y_pred - y) ** 2).item()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gk/jzpjxcmj3qb2g84vsk05rpj00000gr/T/ipykernel_97759/1211933010.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f'stair_function/results/msp_NN_gpu_test/final_model_h800_d4_n{n_train}.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepNN(30, 800, 4)\n",
    "n_train = 20000\n",
    "\n",
    "state_dict = torch.load(f'stair_function/results/msp_NN_gpu_test/final_model_h800_d4_n{n_train}.pt')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matching_rows(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of rows in tensor_a that exist in tensor_b.\n",
    "\n",
    "    Args:\n",
    "        tensor_a (torch.Tensor): Tensor of shape (m, d).\n",
    "        tensor_b (torch.Tensor): Tensor of shape (n, d).\n",
    "\n",
    "    Returns:\n",
    "        int: Number of matching rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate that both tensors are 2D\n",
    "    if tensor_a.dim() != 2 or tensor_b.dim() != 2:\n",
    "        raise ValueError(\"Both tensors must be 2D (matrices).\")\n",
    "\n",
    "    # Validate that both tensors have the same number of columns\n",
    "    if tensor_a.size(1) != tensor_b.size(1):\n",
    "        raise ValueError(\"Both tensors must have the same number of columns (features).\")\n",
    "\n",
    "    # Validate that both tensors have the same data type\n",
    "    if tensor_a.dtype != tensor_b.dtype:\n",
    "        raise ValueError(\"Both tensors must have the same data type.\")\n",
    "\n",
    "    # Move tensors to CPU and convert to NumPy arrays\n",
    "    a_np = tensor_a.cpu().numpy()\n",
    "    b_np = tensor_b.cpu().numpy()\n",
    "\n",
    "    # View each row as a single entity by creating a structured dtype\n",
    "    # This allows us to treat each row as a unique record\n",
    "    a_view = a_np.view([('', a_np.dtype)] * a_np.shape[1])\n",
    "    b_view = b_np.view([('', b_np.dtype)] * b_np.shape[1])\n",
    "\n",
    "    # Use NumPy's in1d to check for each row in a_view if it exists in b_view\n",
    "    matches = np.isin(a_view, b_view)\n",
    "\n",
    "    # Count the number of matches\n",
    "    count = np.sum(matches)\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'stair_function/results/no_overlap/train_data_h800_d4_n{n_train}_lr0.001_standard.pkl', 'rb') as f:\n",
    "    X_train, y_train = pickle.load(f)\n",
    "\n",
    "with open('stair_function/results/no_overlap/test_data.pkl', 'rb') as f:\n",
    "    X_test, y_test = pickle.load(f)\n",
    "\n",
    "int(count_matching_rows(X_train, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.0001\n",
      "Test error: 0.0000\n"
     ]
    }
   ],
   "source": [
    "train_error = evaluate_model(model, X_train, y_train)\n",
    "test_error = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "print(f'Train error: {train_error:.4f}')\n",
    "print(f'Test error: {test_error:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 800])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "def pad_final_linear_layer(model, target_output_features=16):\n",
    "    \"\"\"\n",
    "    Pads the final nn.Linear layer's output features to the target number by adding dummy outputs with zero weights.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model.\n",
    "        target_output_features (int): Desired number of output features (must be divisible by 16).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The modified model with the padded final linear layer.\n",
    "    \"\"\"\n",
    "    # Deep copy the model to avoid in-place modifications\n",
    "    model_padded = copy.deepcopy(model)\n",
    "\n",
    "    # Find the last nn.Linear layer\n",
    "    linear_layers = [module for module in model_padded.modules() if isinstance(module, nn.Linear)]\n",
    "    if not linear_layers:\n",
    "        raise ValueError(\"No nn.Linear layer found in the model.\")\n",
    "\n",
    "    final_layer = linear_layers[-1]\n",
    "\n",
    "    current_out_features = final_layer.out_features\n",
    "    in_features = final_layer.in_features\n",
    "\n",
    "    if current_out_features >= target_output_features:\n",
    "        raise ValueError(f\"Current output features ({current_out_features}) >= target ({target_output_features}).\")\n",
    "\n",
    "    # Calculate number of dummy outputs to add\n",
    "    num_to_add = target_output_features - current_out_features\n",
    "\n",
    "    # Pad the weights with zeros\n",
    "    with torch.no_grad():\n",
    "        # Create a tensor of zeros for the new weights\n",
    "        dummy_weights = torch.zeros((num_to_add, in_features), dtype=final_layer.weight.dtype, device=final_layer.weight.device)\n",
    "        # Concatenate the existing weights with the dummy weights\n",
    "        final_layer.weight = nn.Parameter(torch.cat([final_layer.weight, dummy_weights], dim=0))\n",
    "\n",
    "        if final_layer.bias is not None:\n",
    "            # Create a tensor of zeros for the new biases\n",
    "            dummy_bias = torch.zeros(num_to_add, dtype=final_layer.bias.dtype, device=final_layer.bias.device)\n",
    "            # Concatenate the existing biases with the dummy biases\n",
    "            final_layer.bias = nn.Parameter(torch.cat([final_layer.bias, dummy_bias], dim=0))\n",
    "\n",
    "    return model_padded\n",
    "\n",
    "padded_model = pad_final_linear_layer(model, target_output_features=16)\n",
    "padded_model.state_dict()['network.8.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from torchao.quantization import (\n",
    "    quantize_,\n",
    "    int8_dynamic_activation_int8_weight,\n",
    "    int8_weight_only,\n",
    "    int4_weight_only,\n",
    ")\n",
    "from torchao.quantization import DEFAULT_INT4_AUTOQUANT_CLASS_LIST\n",
    "from torchao.quantization.autoquant import AUTOQUANT_CACHE\n",
    "\n",
    "def quantize_fp16(model):\n",
    "    model_fp16 = copy.deepcopy(model)\n",
    "    model_fp16.half()\n",
    "    return model_fp16\n",
    "\n",
    "def quantize_int8_weight_only(model):\n",
    "    model_int8_wo = copy.deepcopy(model)\n",
    "    quantize_(model_int8_wo, int8_weight_only())\n",
    "    return model_int8_wo\n",
    "\n",
    "def quantize_int8_dynamic(model):\n",
    "    model_int8 = copy.deepcopy(model)\n",
    "    quantize_(model_int8, int8_dynamic_activation_int8_weight())\n",
    "    return model_int8\n",
    "\n",
    "def quantize_int4_weight_only(model):\n",
    "    model_int4_wo = copy.deepcopy(padded_model).to(torch.bfloat16)\n",
    "    quantize_(model_int4_wo, int4_weight_only(group_size=32, use_hqq=False))  # Adjust parameters as needed\n",
    "    return model_int4_wo\n",
    "\n",
    "fp16_model = quantize_fp16(model)\n",
    "int8_wo_model = quantize_int8_weight_only(model)\n",
    "int8_model = quantize_int8_dynamic(model)\n",
    "int4_wo_model = quantize_int4_weight_only(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 7795.278\n",
      "model:  fp16  \t Size (KB): 3899.214\n",
      "model:  int8_wo  \t Size (KB): 2002.706\n",
      "model:  int8  \t Size (KB): 2003.474\n",
      "model:  int4_wo  \t Size (KB): 1606.006\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model, \"fp32\")\n",
    "print_size_of_model(fp16_model, \"fp16\")\n",
    "print_size_of_model(int8_wo_model, \"int8_wo\")\n",
    "print_size_of_model(int8_model, \"int8\")\n",
    "print_size_of_model(int4_wo_model, \"int4_wo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: fp32\n",
      "Train error: 0.0001\n",
      "Test error: 0.0000\n",
      "Model: int8_wo\n",
      "Train error: 0.0005\n",
      "Test error: 0.0005\n",
      "Model: int8\n",
      "Train error: 0.0017\n",
      "Test error: 0.0018\n"
     ]
    }
   ],
   "source": [
    "models = {'fp32': model, 'int8_wo': int8_wo_model, 'int8': int8_model}\n",
    "\n",
    "for k in models:\n",
    "    print(f'Model: {k}')\n",
    "    m = models[k]\n",
    "    train_error = evaluate_model(m, X_train, y_train)\n",
    "    test_error = evaluate_model(m, X_test, y_test)\n",
    "    print(f'Train error: {train_error:.4f}')\n",
    "    print(f'Test error: {test_error:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: fp16\n",
      "Train error: 0.0001\n",
      "Test error: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print('Model: fp16')\n",
    "train_error = evaluate_model(fp16_model, X_train.half(), y_train.half())\n",
    "test_error = evaluate_model(fp16_model, X_test.half(), y_test.half())\n",
    "print(f'Train error: {train_error:.4f}')\n",
    "print(f'Test error: {test_error:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: int4_wo\n",
      "Train error: 0.0245\n",
      "Test error: 0.0236\n"
     ]
    }
   ],
   "source": [
    "def evaluate_int4(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)[:, 0]\n",
    "        mse = torch.mean((y_pred - y) ** 2).item()\n",
    "    return mse\n",
    "\n",
    "\n",
    "print('Model: int4_wo')\n",
    "train_error = evaluate_int4(int4_wo_model, X_train.to(torch.bfloat16), y_train.to(torch.bfloat16))\n",
    "test_error = evaluate_int4(int4_wo_model, X_test.to(torch.bfloat16), y_test.to(torch.bfloat16))\n",
    "print(f'Train error: {train_error:.4f}')\n",
    "print(f'Test error: {test_error:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "features",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
