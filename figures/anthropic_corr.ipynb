{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fdd021cbb20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/users/goringn/NNs_vs_Kernels/env_dev_1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total results loaded: 866\n",
      "Loaded 866 NN results and 2346 NTK results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2561990/1987803352.py:326: UserWarning: Glyph 8804 (\\N{LESS-THAN OR EQUAL TO}) missing from font(s) cmr10.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_2561990/3853216516.py:297: UserWarning: Glyph 8804 (\\N{LESS-THAN OR EQUAL TO}) missing from font(s) cmr10.\n",
      "  threshold_fig.savefig(os.path.join(output_dir, 'threshold_analysis.png'), dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No dataset file found for prefix: h10_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h10_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h10_d4_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h10_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h10_d1_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h10_d1_n800_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h10_d1_n1000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h10_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h10_d4_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h10_d1_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h10_d1_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h10_d4_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h10_d1_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h10_d4_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h10_d4_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h10_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h10_d1_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h10_d1_n20000_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h10_d1_n30000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h10_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h20_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h20_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h20_d1_n100_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h20_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h20_d4_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h20_d1_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h20_d1_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h20_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h20_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h20_d4_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h20_d4_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h20_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h20_d1_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h20_d1_n4500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h20_d1_n5000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h20_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h20_d4_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h20_d1_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h20_d1_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h20_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h30_d4_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h30_d1_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h30_d1_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h30_d4_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h30_d1_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h30_d4_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h30_d4_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h30_d1_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h30_d1_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h30_d1_n2500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h30_d1_n3000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h30_d1_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h30_d4_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h30_d1_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h30_d1_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h30_d4_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h30_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h30_d4_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h30_d4_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h30_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h40_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h40_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h40_d4_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h40_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h40_d1_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h40_d1_n800_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h40_d1_n1000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h40_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h40_d4_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h40_d1_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h40_d1_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h40_d4_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h40_d1_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h40_d4_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h40_d4_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h40_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h40_d1_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h40_d1_n20000_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h40_d1_n30000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h40_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h50_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h50_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h50_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h50_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h50_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h50_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h50_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h50_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h50_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h50_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h50_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h50_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h50_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h50_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h50_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h50_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h50_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h50_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h50_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h50_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h75_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h75_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h75_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h75_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h75_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h75_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h75_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h75_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h75_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h75_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h75_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h75_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h75_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h75_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h75_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h85_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h85_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h85_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h85_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h85_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h85_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h85_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h85_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h85_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h85_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h85_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h85_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h85_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h85_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h85_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h100_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h100_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h100_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h100_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h100_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h100_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h100_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h100_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h100_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h100_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h100_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h100_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h100_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h100_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h100_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h120_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h120_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h120_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h120_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h120_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h120_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h120_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h120_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h120_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h120_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h120_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h120_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h120_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h120_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h120_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h150_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h150_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h150_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h150_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h150_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h150_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h150_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h150_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h150_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h150_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h150_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h150_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h150_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h150_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h150_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h200_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h200_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h200_d1_n100_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h200_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h200_d4_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h200_d1_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h200_d1_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h200_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h200_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h200_d4_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h200_d4_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h200_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h200_d1_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h200_d1_n4500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h200_d1_n5000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h200_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h200_d4_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h200_d1_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h200_d1_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h200_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h300_d4_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h300_d1_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h300_d1_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h300_d4_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h300_d1_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h300_d4_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h300_d4_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h300_d1_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h300_d1_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h300_d1_n2500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h300_d1_n3000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h300_d1_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h300_d4_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h300_d1_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h300_d1_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h300_d4_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h300_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h300_d4_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h300_d4_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h300_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h400_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h400_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h400_d4_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h400_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h400_d1_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h400_d1_n800_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h400_d1_n1000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h400_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h400_d4_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h400_d1_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h400_d1_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h400_d4_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h400_d1_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h400_d4_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h400_d4_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h400_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h400_d1_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h400_d1_n20000_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h400_d1_n30000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h400_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h500_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h500_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h500_d1_n100_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h500_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h500_d4_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h500_d1_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h500_d1_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h500_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h500_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h500_d4_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h500_d4_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h500_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h500_d1_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h500_d1_n4500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h500_d1_n5000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h500_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h500_d4_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h500_d1_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h500_d1_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h500_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h600_d4_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h600_d1_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h600_d1_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h600_d4_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h600_d1_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h600_d4_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h600_d4_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h600_d1_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h600_d1_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h600_d1_n2500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h600_d1_n3000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h600_d1_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h600_d4_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h600_d1_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h600_d1_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h600_d4_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h600_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h600_d4_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h600_d4_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h600_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h800_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h800_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h800_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h800_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h800_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h800_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h800_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h800_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h800_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h800_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h800_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h800_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h800_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h800_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h800_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h1000_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h1000_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h1000_d4_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h1000_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h1000_d1_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h1000_d1_n800_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h1000_d1_n1000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h1000_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1000_d4_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h1000_d1_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h1000_d1_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h1000_d4_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1000_d1_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h1000_d4_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h1000_d4_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h1000_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h1000_d1_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h1000_d1_n20000_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h1000_d1_n30000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h1000_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1500_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h1500_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h1500_d1_n100_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h1500_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1500_d4_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h1500_d1_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h1500_d1_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h1500_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1500_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h1500_d4_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h1500_d4_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h1500_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h1500_d1_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h1500_d1_n4500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h1500_d1_n5000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h1500_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1500_d4_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h1500_d1_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h1500_d1_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h1500_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h2000_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h2000_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h2000_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h2000_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h2000_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h2000_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h2000_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h2000_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h2000_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h2000_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h2000_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h2000_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h2000_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h2000_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h2000_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h2000_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h2000_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h2000_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h2000_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h2000_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h3000_d4_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h3000_d1_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h3000_d1_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h3000_d4_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h3000_d1_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h3000_d4_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h3000_d4_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h3000_d1_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h3000_d1_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h3000_d1_n2500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h3000_d1_n3000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h3000_d1_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h3000_d4_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h3000_d1_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h3000_d1_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h3000_d4_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h3000_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h3000_d4_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h3000_d4_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h3000_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h4000_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h4000_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h4000_d4_n100_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h4000_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h4000_d1_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h4000_d1_n800_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h4000_d1_n1000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h4000_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h4000_d4_n2000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h4000_d1_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h4000_d1_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h4000_d4_n3500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h4000_d1_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h4000_d4_n4500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h4000_d4_n5000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h4000_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h4000_d1_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h4000_d1_n20000_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h4000_d1_n30000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h4000_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h5000_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h5000_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h5000_d1_n100_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h5000_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h5000_d4_n400_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h5000_d1_n800_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h5000_d1_n1000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h5000_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h5000_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h5000_d4_n2500_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h5000_d4_n3000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h5000_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h5000_d1_n4000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h5000_d1_n4500_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h5000_d1_n5000_lr0.05_mup_pennington, rank: 1\n",
      "Warning: No dataset file found for prefix: h5000_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h5000_d4_n10000_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h5000_d1_n20000_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h5000_d1_n30000_lr0.05_mup_pennington, rank: 9\n",
      "Warning: No dataset file found for prefix: h5000_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h8000_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h8000_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h8000_d1_n100_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h8000_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h8000_d4_n400_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h8000_d4_n800_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h8000_d4_n1000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h8000_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h8000_d1_n2000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h8000_d1_n2500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h8000_d1_n3000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h8000_d1_n3500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h8000_d4_n4000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h8000_d4_n4500_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h8000_d4_n5000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h8000_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h8000_d1_n10000_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h8000_d1_n20000_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h8000_d1_n30000_lr0.05_mup_pennington, rank: 5\n",
      "Warning: No dataset file found for prefix: h8000_d1_n40000_lr0.05_mup_pennington, rank: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2561990/3853216516.py:206: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No dataset file found for prefix: h10_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h10_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h10_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h10_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h10_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h10_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h20_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h20_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h20_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h20_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h20_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h20_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h30_d4_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h30_d1_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h30_d4_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h30_d1_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h30_d4_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h30_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h40_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h40_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h40_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h40_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h40_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h40_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h50_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h50_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h50_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h50_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h50_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h50_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h75_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h75_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h75_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h85_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h85_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h85_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h100_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h100_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h100_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h120_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h120_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h120_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h150_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h150_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h150_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h200_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h200_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h200_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h200_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h200_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h200_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h300_d4_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h300_d1_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h300_d4_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h300_d1_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h300_d4_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h300_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h400_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h400_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h400_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h400_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h400_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h400_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h500_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h500_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h500_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h500_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h500_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h500_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h600_d4_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h600_d1_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h600_d4_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h600_d1_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h600_d4_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h600_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h800_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h800_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h800_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h1000_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h1000_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h1000_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h1000_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1000_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h1000_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1500_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h1500_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h1500_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1500_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1500_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h1500_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h2000_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h2000_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h2000_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h2000_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h2000_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h2000_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h3000_d4_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h3000_d1_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h3000_d4_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h3000_d1_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h3000_d4_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h3000_d1_n40000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h4000_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h4000_d4_n50_lr0.05_mup_pennington, rank: 10\n",
      "Warning: No dataset file found for prefix: h4000_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h4000_d1_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h4000_d1_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h4000_d1_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h5000_d1_n10_lr0.05_mup_pennington, rank: 3\n",
      "Warning: No dataset file found for prefix: h5000_d1_n50_lr0.05_mup_pennington, rank: 2\n",
      "Warning: No dataset file found for prefix: h5000_d1_n200_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h5000_d4_n1500_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h5000_d1_n8000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h5000_d4_n40000_lr0.05_mup_pennington, rank: 0\n",
      "Warning: No dataset file found for prefix: h8000_d1_n10_lr0.05_mup_pennington, rank: 7\n",
      "Warning: No dataset file found for prefix: h8000_d1_n50_lr0.05_mup_pennington, rank: 6\n",
      "Warning: No dataset file found for prefix: h8000_d1_n200_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h8000_d4_n1500_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h8000_d4_n8000_lr0.05_mup_pennington, rank: 4\n",
      "Warning: No dataset file found for prefix: h8000_d1_n40000_lr0.05_mup_pennington, rank: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, d: int, hidden_size: int, depth: int, mode: str = 'standard'):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dim = d\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = d\n",
    "        \n",
    "        for layer_idx in range(depth):\n",
    "            linear = torch.nn.Linear(prev_dim, hidden_size)\n",
    "            \n",
    "            if mode == 'standard':\n",
    "                torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                torch.nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            layers.extend([\n",
    "                linear,\n",
    "                torch.nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_size\n",
    "        \n",
    "        final_layer = torch.nn.Linear(prev_dim, 1)\n",
    "        if mode == 'standard':\n",
    "            torch.nn.init.xavier_uniform_(final_layer.weight)\n",
    "        torch.nn.init.zeros_(final_layer.bias)\n",
    "        layers.append(final_layer)\n",
    "        \n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def load_rank_results(result_files):\n",
    "    \"\"\"Load and combine results from multiple rank files\"\"\"\n",
    "    combined_results = []\n",
    "    empty_files = []\n",
    "    \n",
    "    for file_path in result_files:\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    empty_files.append(file_path)\n",
    "                    continue\n",
    "                    \n",
    "                results = json.loads(content)\n",
    "                if isinstance(results, list):\n",
    "                    combined_results.extend(results)\n",
    "                else:\n",
    "                    combined_results.append(results)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if empty_files:\n",
    "        print(\"\\nThe following files were empty:\")\n",
    "        for f in empty_files:\n",
    "            print(f\"  - {f}\")\n",
    "            \n",
    "    print(f\"\\nTotal results loaded: {len(combined_results)}\")\n",
    "    return combined_results\n",
    "\n",
    "def load_experiment_data(results_dir: str) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Load experiment results from rank-based result files\"\"\"\n",
    "    nn_files = list(Path(results_dir).glob(\"results*.json\"))\n",
    "    if not nn_files:\n",
    "        raise ValueError(f\"No result files found in {results_dir}\")\n",
    "    \n",
    "    # Load results from all rank files\n",
    "    nn_results = load_rank_results(nn_files)\n",
    "    \n",
    "    # Load hyperparameters from any rank file (they should be the same)\n",
    "    hyperparams_file = next(Path(results_dir).glob(\"hyperparameters*.json\"))\n",
    "    with open(hyperparams_file, 'r') as f:\n",
    "        hyperparams = json.load(f)\n",
    "    \n",
    "    return nn_results, hyperparams\n",
    "\n",
    "def find_model_file(results_dir: str, model_prefix: str, rank: int) -> Optional[str]:\n",
    "    \"\"\"Find a model file matching the prefix and rank across different timestamps\"\"\"\n",
    "    # Try both timestamps and model file patterns\n",
    "    timestamps = [\"20250107_150050\", \"20250107_151953\"]\n",
    "    patterns = [\n",
    "        f\"final_model_{model_prefix}_{{}}_rank{rank}.pt\",\n",
    "        f\"initial_model_{model_prefix}_{{}}_rank{rank}.pt\"\n",
    "    ]\n",
    "    \n",
    "    for timestamp in timestamps:\n",
    "        for pattern in patterns:\n",
    "            filename = pattern.format(timestamp)\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                return filepath\n",
    "    return None\n",
    "\n",
    "def find_data_file(results_dir: str, model_prefix: str, rank: int) -> Optional[str]:\n",
    "    \"\"\"Find a data file matching the prefix and rank across different timestamps\"\"\"\n",
    "    timestamps = [\"20241230_145424\", \"20241230_145221\"]\n",
    "    for timestamp in timestamps:\n",
    "        filename = f\"train_data_{model_prefix}_{timestamp}_rank{rank}.pt\"\n",
    "        filepath = os.path.join(results_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            return filepath\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def load_model_and_data(data_dir: str, results_dir: str, result: Dict) -> Optional[Tuple[torch.nn.Module, torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"Load the model and its corresponding training data\"\"\"\n",
    "    try:\n",
    "        hyperparams_file = next(Path(results_dir).glob(\"hyperparameters*.json\"))\n",
    "        with open(hyperparams_file, 'r') as f:\n",
    "            hyperparams = json.load(f)\n",
    "        input_dim = hyperparams['d']\n",
    "        \n",
    "        hidden_size = result['hidden_size']\n",
    "        depth = result['depth']\n",
    "        n_train = result['n_train']\n",
    "        lr = result.get('learning_rate', result.get('lr'))\n",
    "        mode = result.get('mode', 'mup_pennington')  # Update default mode if needed\n",
    "        shuffled = result.get('shuffled', False)\n",
    "        rank = result.get('worker_rank', 0)\n",
    "        \n",
    "        model_prefix = f'h{hidden_size}_d{depth}_n{n_train}_lr{lr}_{mode}'\n",
    "        if shuffled:\n",
    "            model_prefix += '_shuffled'\n",
    "        \n",
    "        # Find model and data files\n",
    "        model_path = find_model_file(results_dir, model_prefix, rank)\n",
    "        train_dataset_path = find_data_file(results_dir, model_prefix, rank)\n",
    "        \n",
    "        if not model_path:\n",
    "            print(f\"Warning: No model file found for prefix: {model_prefix}, rank: {rank}\")\n",
    "            return None\n",
    "            \n",
    "        if not train_dataset_path:\n",
    "            print(f\"Warning: No dataset file found for prefix: {model_prefix}, rank: {rank}\")\n",
    "            return None\n",
    "        \n",
    "        model = DeepNN(input_dim, hidden_size, depth, mode=mode)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        train_data = torch.load(train_dataset_path)\n",
    "        X_train = train_data['X']\n",
    "        y_train = train_data['y']\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        model = model.to(device)\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        \n",
    "        return model, X_train, y_train\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model and data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def plot_low_dim_ratio(results: List[Dict], results_dir: str, threshold: float = 0.2):\n",
    "    \"\"\"Plot ratio of features with dimensionality below threshold vs training size\"\"\"\n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for hidden_size in hidden_sizes:\n",
    "        ratios = []\n",
    "        valid_train_sizes = []\n",
    "        for n_train in train_sizes:\n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model_data = load_model_and_data(results_dir, results_dir, result)\n",
    "                \n",
    "                if model_data is not None:\n",
    "                    model, _, _ = model_data\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    ratio = np.mean(feature_dims < threshold)\n",
    "                    ratios.append(ratio)\n",
    "                    valid_train_sizes.append(n_train)\n",
    "        \n",
    "        if ratios:\n",
    "            plt.plot(valid_train_sizes, ratios, '-o', label=f'h={hidden_size}')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel(f'Ratio of Features with Dim < {threshold}')\n",
    "    plt.title('Low-Dimensional Feature Ratio vs Training Size')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def create_feature_dim_histograms(results: List[Dict], results_dir: str):\n",
    "    \"\"\"Create histograms of feature dimensionality for different training sizes\"\"\"\n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    n_rows = len(hidden_sizes)\n",
    "    n_cols = min(6, len(train_sizes))  # Limit number of columns for readability\n",
    "    selected_train_sizes = np.logspace(np.log10(min(train_sizes)), \n",
    "                                     np.log10(max(train_sizes)), \n",
    "                                     n_cols).astype(int)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, hidden_size in enumerate(hidden_sizes):\n",
    "        for j, n_train in enumerate(selected_train_sizes):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Find closest available training size\n",
    "            closest_n_train = min(train_sizes, key=lambda x: abs(x - n_train))\n",
    "            \n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == closest_n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model_data = load_model_and_data(results_dir, results_dir, result)\n",
    "                \n",
    "                if model_data is not None:\n",
    "                    model, _, _ = model_data\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    \n",
    "                    ax.hist(np.log1p(feature_dims), bins=30, alpha=0.7)\n",
    "                    ax.set_title(f'h={hidden_size}, n={closest_n_train}')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'No data available', \n",
    "                           ha='center', va='center',\n",
    "                           transform=ax.transAxes)\n",
    "                \n",
    "            if j == 0:\n",
    "                ax.set_ylabel('Count')\n",
    "            if i == n_rows-1:\n",
    "                ax.set_xlabel('log(1 + dim)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_feature_dimensionality(model: torch.nn.Module) -> np.ndarray:\n",
    "    \"\"\"Analyze feature dimensionality for layer 1 of the model\"\"\"\n",
    "    first_layer = None\n",
    "    for layer in model.network:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            first_layer = layer\n",
    "            break\n",
    "    \n",
    "    if first_layer is None:\n",
    "        raise ValueError(\"No linear layer found in model\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        features = first_layer.weight.data\n",
    "        feature_norms = torch.norm(features, dim=0, keepdim=True)\n",
    "        normalized_features = features / (feature_norms + 1e-8)\n",
    "        feature_dots = normalized_features.T @ features\n",
    "        feature_dots_squared = feature_dots ** 2\n",
    "        feature_denominator = torch.sum(feature_dots_squared, dim=1)\n",
    "        feature_numerator = torch.sum(features * features, dim=0)\n",
    "        feature_dims = feature_numerator / (feature_denominator + 1e-8)\n",
    "        \n",
    "        return feature_dims.cpu().numpy()\n",
    "\n",
    "def main():\n",
    "    # Set your paths here\n",
    "    nn_results_dir = \"/mnt/users/goringn/NNs_vs_Kernels/stair_function/results/msp_NN_grid_0701_mup_lr005\"\n",
    "    ntk_results_path = \"//mnt/users/goringn/NNs_vs_Kernels/stair_function/results/msp_NN_grid_1812_spectral/final_results_20241219_015151.json\"\n",
    "    output_dir = \"analysis_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load results\n",
    "    nn_results, hyperparams = load_experiment_data(nn_results_dir)\n",
    "    with open(ntk_results_path, 'r') as f:\n",
    "        ntk_results = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(nn_results)} NN results and {len(ntk_results)} NTK results\")\n",
    "    \n",
    "    # Create threshold plot\n",
    "    threshold_fig = create_threshold_plot(nn_results, ntk_results)\n",
    "    threshold_fig.savefig(os.path.join(output_dir, 'threshold_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create low-dimensional ratio plot\n",
    "    ratio_fig = plot_low_dim_ratio(nn_results, nn_results_dir)\n",
    "    ratio_fig.savefig(os.path.join(output_dir, 'low_dim_ratio.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create feature dimensionality histograms\n",
    "    hist_fig = create_feature_dim_histograms(nn_results, nn_results_dir)\n",
    "    hist_fig.savefig(os.path.join(output_dir, 'feature_dim_histograms.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close('all')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 350\u001b[0m\n\u001b[1;32m    347\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 350\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 329\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    326\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Load results\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m nn_results, hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mload_experiment_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_results_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ntk_results_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    331\u001b[0m     ntk_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "Cell \u001b[0;32mIn[10], line 121\u001b[0m, in \u001b[0;36mload_experiment_data\u001b[0;34m(results_dir)\u001b[0m\n\u001b[1;32m    118\u001b[0m     empty_files\u001b[38;5;241m.\u001b[39mappend(file_path)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    123\u001b[0m     combined_results\u001b[38;5;241m.\u001b[39mextend(results)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, d: int, hidden_size: int, depth: int, mode: str = 'standard'):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dim = d\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = d\n",
    "        \n",
    "        for layer_idx in range(depth):\n",
    "            linear = torch.nn.Linear(prev_dim, hidden_size)\n",
    "            \n",
    "            if mode == 'standard':\n",
    "                torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                torch.nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            layers.extend([\n",
    "                linear,\n",
    "                torch.nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_size\n",
    "        \n",
    "        final_layer = torch.nn.Linear(prev_dim, 1)\n",
    "        if mode == 'standard':\n",
    "            torch.nn.init.xavier_uniform_(final_layer.weight)\n",
    "        torch.nn.init.zeros_(final_layer.bias)\n",
    "        layers.append(final_layer)\n",
    "        \n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def find_model_file(results_dir: str, model_prefix: str, rank: int) -> Optional[str]:\n",
    "    \"\"\"Find a model file matching the prefix and rank across different timestamps\"\"\"\n",
    "    # Try both timestamps and model file patterns\n",
    "    timestamps = [\"20250107_150050\", \"20250107_151953\"]\n",
    "    patterns = [\n",
    "        f\"final_model_{model_prefix}_{{}}_rank{rank}.pt\",\n",
    "        f\"initial_model_{model_prefix}_{{}}_rank{rank}.pt\"\n",
    "    ]\n",
    "    \n",
    "    for timestamp in timestamps:\n",
    "        for pattern in patterns:\n",
    "            filename = pattern.format(timestamp)\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                return filepath\n",
    "    return None\n",
    "\n",
    "def load_model(results_dir: str, result: Dict) -> Optional[torch.nn.Module]:\n",
    "    \"\"\"Load just the model without dataset\"\"\"\n",
    "    try:\n",
    "        hyperparams_file = next(Path(results_dir).glob(\"hyperparameters*.json\"))\n",
    "        with open(hyperparams_file, 'r') as f:\n",
    "            hyperparams = json.load(f)\n",
    "        input_dim = hyperparams['d']\n",
    "        \n",
    "        hidden_size = result['hidden_size']\n",
    "        depth = result['depth']\n",
    "        n_train = result['n_train']\n",
    "        lr = result.get('learning_rate', result.get('lr'))\n",
    "        mode = result.get('mode', 'mup_pennington')  # Update default mode if needed\n",
    "        shuffled = result.get('shuffled', False)\n",
    "        rank = result.get('worker_rank', 0)\n",
    "        \n",
    "        model_prefix = f'h{hidden_size}_d{depth}_n{n_train}_lr{lr}_{mode}'\n",
    "        if shuffled:\n",
    "            model_prefix += '_shuffled'\n",
    "        \n",
    "        # Find model file\n",
    "        model_path = find_model_file(results_dir, model_prefix, rank)\n",
    "        \n",
    "        if not model_path:\n",
    "            print(f\"Warning: No model file found for prefix: {model_prefix}, rank: {rank}\")\n",
    "            return None\n",
    "        \n",
    "        model = DeepNN(input_dim, hidden_size, depth, mode=mode)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_experiment_data(results_dir: str) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Load experiment results from rank-based result files\"\"\"\n",
    "    nn_files = list(Path(results_dir).glob(\"results*.json\"))\n",
    "    if not nn_files:\n",
    "        raise ValueError(f\"No result files found in {results_dir}\")\n",
    "    \n",
    "    # Load results from all rank files\n",
    "    combined_results = []\n",
    "    empty_files = []\n",
    "    \n",
    "    for file_path in nn_files:\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    empty_files.append(file_path)\n",
    "                    continue\n",
    "                    \n",
    "                results = json.loads(content)\n",
    "                if isinstance(results, list):\n",
    "                    combined_results.extend(results)\n",
    "                else:\n",
    "                    combined_results.append(results)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if empty_files:\n",
    "        print(\"\\nThe following files were empty:\")\n",
    "        for f in empty_files:\n",
    "            print(f\"  - {f}\")\n",
    "            \n",
    "    print(f\"\\nTotal results loaded: {len(combined_results)}\")\n",
    "    \n",
    "    # Load hyperparameters from any rank file (they should be the same)\n",
    "    hyperparams_file = next(Path(results_dir).glob(\"hyperparameters*.json\"))\n",
    "    with open(hyperparams_file, 'r') as f:\n",
    "        hyperparams = json.load(f)\n",
    "    \n",
    "    return combined_results, hyperparams\n",
    "\n",
    "def analyze_feature_dimensionality(model: torch.nn.Module) -> np.ndarray:\n",
    "    \"\"\"Analyze feature dimensionality for layer 1 of the model\"\"\"\n",
    "    first_layer = None\n",
    "    for layer in model.network:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            first_layer = layer\n",
    "            break\n",
    "    \n",
    "    if first_layer is None:\n",
    "        raise ValueError(\"No linear layer found in model\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        features = first_layer.weight.data\n",
    "        feature_norms = torch.norm(features, dim=0, keepdim=True)\n",
    "        normalized_features = features / (feature_norms + 1e-8)\n",
    "        feature_dots = normalized_features.T @ features\n",
    "        feature_dots_squared = feature_dots ** 2\n",
    "        feature_denominator = torch.sum(feature_dots_squared, dim=1)\n",
    "        feature_numerator = torch.sum(features * features, dim=0)\n",
    "        feature_dims = feature_numerator / (feature_denominator + 1e-8)\n",
    "        \n",
    "        return feature_dims.cpu().numpy()\n",
    "\n",
    "def plot_low_dim_ratio(results: List[Dict], results_dir: str, threshold: float = 0.1):\n",
    "    \"\"\"Plot ratio of features with dimensionality below threshold vs training size\"\"\"\n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for hidden_size in hidden_sizes:\n",
    "        ratios = []\n",
    "        valid_train_sizes = []\n",
    "        for n_train in train_sizes:\n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model = load_model(results_dir, result)\n",
    "                \n",
    "                if model is not None:\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    ratio = np.mean(feature_dims < threshold)\n",
    "                    ratios.append(ratio)\n",
    "                    valid_train_sizes.append(n_train)\n",
    "        \n",
    "        if ratios:\n",
    "            plt.plot(valid_train_sizes, ratios, '-o', label=f'h={hidden_size}')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel(f'Ratio of Features with Dim < {threshold}')\n",
    "    plt.title('Low-Dimensional Feature Ratio vs Training Size')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def create_feature_dim_histograms(results: List[Dict], results_dir: str):\n",
    "    \"\"\"Create histograms of feature dimensionality for different training sizes\"\"\"\n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    n_rows = len(hidden_sizes)\n",
    "    n_cols = min(6, len(train_sizes))  # Limit number of columns for readability\n",
    "    selected_train_sizes = np.logspace(np.log10(min(train_sizes)), \n",
    "                                     np.log10(max(train_sizes)), \n",
    "                                     n_cols).astype(int)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, hidden_size in enumerate(hidden_sizes):\n",
    "        for j, n_train in enumerate(selected_train_sizes):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Find closest available training size\n",
    "            closest_n_train = min(train_sizes, key=lambda x: abs(x - n_train))\n",
    "            \n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == closest_n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model = load_model(results_dir, result)\n",
    "                \n",
    "                if model is not None:\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    \n",
    "                    ax.hist(np.log1p(feature_dims), bins=30, alpha=0.7)\n",
    "                    ax.set_title(f'h={hidden_size}, n={closest_n_train}')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'No data available', \n",
    "                           ha='center', va='center',\n",
    "                           transform=ax.transAxes)\n",
    "                \n",
    "            if j == 0:\n",
    "                ax.set_ylabel('Count')\n",
    "            if i == n_rows-1:\n",
    "                ax.set_xlabel('log(1 + dim)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def create_threshold_plot(nn_results: List[Dict], ntk_results: List[Dict], performance_threshold: float = 80.0):\n",
    "    \"\"\"Create threshold crossing analysis plot\"\"\"\n",
    "    # Extract parameters\n",
    "    depths = sorted(set(r['depth'] for r in nn_results))\n",
    "    learning_rates = sorted(set(r.get('learning_rate', r.get('lr')) for r in nn_results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in nn_results))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(learning_rates), figsize=(6*len(learning_rates), 5))\n",
    "    if len(learning_rates) == 1:\n",
    "        axes = [axes]\n",
    "            \n",
    "    fig.suptitle(f'Training Size at First Performance Threshold Crossing\\n' +\n",
    "                 f'(When NN error first becomes  {100-performance_threshold}% of NTK error)',\n",
    "                 fontsize=16, y=1.02)\n",
    "    \n",
    "    depth_colors = plt.cm.viridis(np.linspace(0, 1, len(depths)))\n",
    "    \n",
    "    for j, lr in enumerate(learning_rates):\n",
    "        ax = axes[j]\n",
    "        \n",
    "        for depth_idx, depth in enumerate(depths):\n",
    "            threshold_data = []\n",
    "            \n",
    "            for hidden_size in hidden_sizes:\n",
    "                ntk_errors_dict = {}\n",
    "                for r in ntk_results:\n",
    "                    if (r['depth'] == depth and \n",
    "                        r['hidden_size'] == hidden_size and \n",
    "                        r['training_mode'] == 'ntk' and\n",
    "                        r['status'] == 'success'):\n",
    "                        ntk_errors_dict[r['n_train']] = r['test_error']\n",
    "                \n",
    "                nn_data = [(r['n_train'], r['test_error']) \n",
    "                          for r in nn_results \n",
    "                          if r['depth'] == depth and \n",
    "                          r['hidden_size'] == hidden_size and \n",
    "                          (r.get('learning_rate', r.get('lr')) == lr)]\n",
    "                \n",
    "                if nn_data:\n",
    "                    nn_points = sorted(nn_data)\n",
    "                    \n",
    "                    for train_size, nn_error in nn_points:\n",
    "                        ntk_error = ntk_errors_dict.get(train_size)\n",
    "                        if ntk_error is not None:\n",
    "                            if nn_error <= ntk_error * (100 - performance_threshold) / 100:\n",
    "                                threshold_data.append((hidden_size, train_size))\n",
    "                                break\n",
    "            \n",
    "            if threshold_data:\n",
    "                hidden_widths, crossing_points = zip(*sorted(threshold_data))\n",
    "                ax.plot(hidden_widths, crossing_points, '-o', linewidth=2, \n",
    "                       markersize=6, color=depth_colors[depth_idx],\n",
    "                       label=f'd={depth}')\n",
    "        \n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlabel('Hidden Width')\n",
    "        if j == 0:\n",
    "            ax.set_ylabel('Training Size at Threshold')\n",
    "        \n",
    "        ax.text(0.05, 0.95, f'lr={lr:.1e}', \n",
    "               transform=ax.transAxes, \n",
    "               verticalalignment='top',\n",
    "               bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "        ax.legend(title='Network Depth', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    # Set your paths here\n",
    "    nn_results_dir = \"/mnt/users/goringn/NNs_vs_Kernels/stair_function/results/msp_NN_grid_0701_mup_lr005\"\n",
    "    ntk_results_path = \"/mnt/users/goringn/NNs_vs_Kernels/stair_function/results/msp_NN_grid_1812_spectral/final_results_20241219_015151.json\"\n",
    "    output_dir = \"analysis_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load results\n",
    "    nn_results, hyperparams = load_experiment_data(nn_results_dir)\n",
    "    with open(ntk_results_path, 'r') as f:\n",
    "        ntk_results = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(nn_results)} NN results and {len(ntk_results)} NTK results\")\n",
    "    \n",
    "    # Create threshold plot\n",
    "    threshold_fig = create_threshold_plot(nn_results, ntk_results)\n",
    "    threshold_fig.savefig(os.path.join(output_dir, 'threshold_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create low-dimensional ratio plot\n",
    "    ratio_fig = plot_low_dim_ratio(nn_results, nn_results_dir)\n",
    "    ratio_fig.savefig(os.path.join(output_dir, 'low_dim_ratio.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create feature dimensionality histograms\n",
    "    hist_fig = create_feature_dim_histograms(nn_results, nn_results_dir)\n",
    "    hist_fig.savefig(os.path.join(output_dir, 'feature_dim_histograms.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close('all')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total results loaded: 866\n",
      "Loaded 866 NN results and 2346 NTK results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2561990/176772683.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, d: int, hidden_size: int, depth: int, mode: str = 'standard'):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dim = d\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = d\n",
    "        \n",
    "        for layer_idx in range(depth):\n",
    "            linear = torch.nn.Linear(prev_dim, hidden_size)\n",
    "            \n",
    "            if mode == 'standard':\n",
    "                torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                torch.nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            layers.extend([\n",
    "                linear,\n",
    "                torch.nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_size\n",
    "        \n",
    "        final_layer = torch.nn.Linear(prev_dim, 1)\n",
    "        if mode == 'standard':\n",
    "            torch.nn.init.xavier_uniform_(final_layer.weight)\n",
    "        torch.nn.init.zeros_(final_layer.bias)\n",
    "        layers.append(final_layer)\n",
    "        \n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def find_model_file(results_dir: str, model_prefix: str, rank: int) -> Optional[str]:\n",
    "    \"\"\"Find a model file matching the prefix and rank across different timestamps\"\"\"\n",
    "    # Try both timestamps and model file patterns\n",
    "    timestamps = [\"20250107_150050\", \"20250107_151953\"]\n",
    "    patterns = [\n",
    "        f\"final_model_{model_prefix}_{{}}_rank{rank}.pt\",\n",
    "        f\"initial_model_{model_prefix}_{{}}_rank{rank}.pt\"\n",
    "    ]\n",
    "    \n",
    "    for timestamp in timestamps:\n",
    "        for pattern in patterns:\n",
    "            filename = pattern.format(timestamp)\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                return filepath\n",
    "    return None\n",
    "\n",
    "def load_model(results_dir: str, result: Dict) -> Optional[torch.nn.Module]:\n",
    "    \"\"\"Load just the model without dataset\"\"\"\n",
    "    try:\n",
    "        hyperparams_file = next(Path(results_dir).glob(\"hyperparameters*.json\"))\n",
    "        with open(hyperparams_file, 'r') as f:\n",
    "            hyperparams = json.load(f)\n",
    "        input_dim = hyperparams['d']\n",
    "        \n",
    "        hidden_size = result['hidden_size']\n",
    "        depth = result['depth']\n",
    "        n_train = result['n_train']\n",
    "        lr = result.get('learning_rate', result.get('lr'))\n",
    "        mode = result.get('mode', 'mup_pennington')  # Update default mode if needed\n",
    "        shuffled = result.get('shuffled', False)\n",
    "        rank = result.get('worker_rank', 0)\n",
    "        \n",
    "        model_prefix = f'h{hidden_size}_d{depth}_n{n_train}_lr{lr}_{mode}'\n",
    "        if shuffled:\n",
    "            model_prefix += '_shuffled'\n",
    "        \n",
    "        # Find model file\n",
    "        model_path = find_model_file(results_dir, model_prefix, rank)\n",
    "        \n",
    "        if not model_path:\n",
    "            print(f\"Warning: No model file found for prefix: {model_prefix}, rank: {rank}\")\n",
    "            return None\n",
    "        \n",
    "        model = DeepNN(input_dim, hidden_size, depth, mode=mode)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_experiment_data(results_dir: str) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Load experiment results from rank-based result files\"\"\"\n",
    "    nn_files = list(Path(results_dir).glob(\"results*.json\"))\n",
    "    if not nn_files:\n",
    "        raise ValueError(f\"No result files found in {results_dir}\")\n",
    "    \n",
    "    # Load results from all rank files\n",
    "    combined_results = []\n",
    "    empty_files = []\n",
    "    \n",
    "    for file_path in nn_files:\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    empty_files.append(file_path)\n",
    "                    continue\n",
    "                    \n",
    "                results = json.loads(content)\n",
    "                if isinstance(results, list):\n",
    "                    combined_results.extend(results)\n",
    "                else:\n",
    "                    combined_results.append(results)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if empty_files:\n",
    "        print(\"\\nThe following files were empty:\")\n",
    "        for f in empty_files:\n",
    "            print(f\"  - {f}\")\n",
    "            \n",
    "    print(f\"\\nTotal results loaded: {len(combined_results)}\")\n",
    "    \n",
    "    # Load hyperparameters from any rank file (they should be the same)\n",
    "    hyperparams_file = next(Path(results_dir).glob(\"hyperparameters*.json\"))\n",
    "    with open(hyperparams_file, 'r') as f:\n",
    "        hyperparams = json.load(f)\n",
    "    \n",
    "    return combined_results, hyperparams\n",
    "\n",
    "def analyze_feature_dimensionality(model: torch.nn.Module) -> np.ndarray:\n",
    "    \"\"\"Analyze feature dimensionality for layer 1 of the model\"\"\"\n",
    "    first_layer = None\n",
    "    for layer in model.network:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            first_layer = layer\n",
    "            break\n",
    "    \n",
    "    if first_layer is None:\n",
    "        raise ValueError(\"No linear layer found in model\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        features = first_layer.weight.data\n",
    "        feature_norms = torch.norm(features, dim=0, keepdim=True)\n",
    "        normalized_features = features / (feature_norms + 1e-8)\n",
    "        feature_dots = normalized_features.T @ features\n",
    "        feature_dots_squared = feature_dots ** 2\n",
    "        feature_denominator = torch.sum(feature_dots_squared, dim=1)\n",
    "        feature_numerator = torch.sum(features * features, dim=0)\n",
    "        feature_dims = feature_numerator / (feature_denominator + 1e-8)\n",
    "        \n",
    "        return feature_dims.cpu().numpy()\n",
    "\n",
    "def plot_low_dim_ratio(results: List[Dict], results_dir: str, threshold: float = 0.1):\n",
    "    \"\"\"Plot ratio of features with dimensionality below threshold vs training size\"\"\"\n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create colormap from red to blue\n",
    "    n_sizes = len(hidden_sizes)\n",
    "    colors = plt.cm.RdBu(np.linspace(0, 1, n_sizes))\n",
    "    \n",
    "    for idx, hidden_size in enumerate(hidden_sizes):\n",
    "        ratios = []\n",
    "        valid_train_sizes = []\n",
    "        for n_train in train_sizes:\n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model = load_model(results_dir, result)\n",
    "                \n",
    "                if model is not None:\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    ratio = np.mean(feature_dims < threshold)\n",
    "                    ratios.append(ratio)\n",
    "                    valid_train_sizes.append(n_train)\n",
    "        \n",
    "        if ratios:\n",
    "            plt.plot(valid_train_sizes, ratios, '-o', \n",
    "                    color=colors[idx], \n",
    "                    alpha=0.7,\n",
    "                    label=f'h={hidden_size}',\n",
    "                    markersize=4)\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel(f'Ratio of Features with Dim < {threshold}')\n",
    "    plt.title('Low-Dimensional Feature Ratio vs Training Size')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def create_feature_dim_histograms(results: List[Dict], results_dir: str):\n",
    "    \"\"\"Create histograms of feature dimensionality for different training sizes\"\"\"\n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    n_rows = len(hidden_sizes)\n",
    "    n_cols = min(6, len(train_sizes))  # Limit number of columns for readability\n",
    "    selected_train_sizes = np.logspace(np.log10(min(train_sizes)), \n",
    "                                     np.log10(max(train_sizes)), \n",
    "                                     n_cols).astype(int)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, hidden_size in enumerate(hidden_sizes):\n",
    "        for j, n_train in enumerate(selected_train_sizes):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Find closest available training size\n",
    "            closest_n_train = min(train_sizes, key=lambda x: abs(x - n_train))\n",
    "            \n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == closest_n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model = load_model(results_dir, result)\n",
    "                \n",
    "                if model is not None:\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    \n",
    "                    ax.hist(np.log1p(feature_dims), bins=30, alpha=0.7)\n",
    "                    ax.set_title(f'h={hidden_size}, n={closest_n_train}')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'No data available', \n",
    "                           ha='center', va='center',\n",
    "                           transform=ax.transAxes)\n",
    "                \n",
    "            if j == 0:\n",
    "                ax.set_ylabel('Count')\n",
    "            if i == n_rows-1:\n",
    "                ax.set_xlabel('log(1 + dim)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def create_threshold_plot(nn_results: List[Dict], ntk_results: List[Dict], performance_threshold: float = 80.0):\n",
    "    \"\"\"Create threshold crossing analysis plot\"\"\"\n",
    "    # Extract parameters\n",
    "    depths = sorted(set(r['depth'] for r in nn_results))\n",
    "    learning_rates = sorted(set(r.get('learning_rate', r.get('lr')) for r in nn_results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in nn_results))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(learning_rates), figsize=(6*len(learning_rates), 5))\n",
    "    if len(learning_rates) == 1:\n",
    "        axes = [axes]\n",
    "            \n",
    "    fig.suptitle(f'Training Size at First Performance Threshold Crossing\\n' +\n",
    "                 f'(When NN error first becomes  {100-performance_threshold}% of NTK error)',\n",
    "                 fontsize=16, y=1.02)\n",
    "    \n",
    "    depth_colors = plt.cm.viridis(np.linspace(0, 1, len(depths)))\n",
    "    \n",
    "    for j, lr in enumerate(learning_rates):\n",
    "        ax = axes[j]\n",
    "        \n",
    "        for depth_idx, depth in enumerate(depths):\n",
    "            threshold_data = []\n",
    "            \n",
    "            for hidden_size in hidden_sizes:\n",
    "                ntk_errors_dict = {}\n",
    "                for r in ntk_results:\n",
    "                    if (r['depth'] == depth and \n",
    "                        r['hidden_size'] == hidden_size and \n",
    "                        r['training_mode'] == 'ntk' and\n",
    "                        r['status'] == 'success'):\n",
    "                        ntk_errors_dict[r['n_train']] = r['test_error']\n",
    "                \n",
    "                nn_data = [(r['n_train'], r['test_error']) \n",
    "                          for r in nn_results \n",
    "                          if r['depth'] == depth and \n",
    "                          r['hidden_size'] == hidden_size and \n",
    "                          (r.get('learning_rate', r.get('lr')) == lr)]\n",
    "                \n",
    "                if nn_data:\n",
    "                    nn_points = sorted(nn_data)\n",
    "                    \n",
    "                    for train_size, nn_error in nn_points:\n",
    "                        ntk_error = ntk_errors_dict.get(train_size)\n",
    "                        if ntk_error is not None:\n",
    "                            if nn_error <= ntk_error * (100 - performance_threshold) / 100:\n",
    "                                threshold_data.append((hidden_size, train_size))\n",
    "                                break\n",
    "            \n",
    "            if threshold_data:\n",
    "                hidden_widths, crossing_points = zip(*sorted(threshold_data))\n",
    "                ax.plot(hidden_widths, crossing_points, '-o', linewidth=2, \n",
    "                       markersize=6, color=depth_colors[depth_idx],\n",
    "                       label=f'd={depth}')\n",
    "        \n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlabel('Hidden Width')\n",
    "        if j == 0:\n",
    "            ax.set_ylabel('Training Size at Threshold')\n",
    "        \n",
    "        ax.text(0.05, 0.95, f'lr={lr:.1e}', \n",
    "               transform=ax.transAxes, \n",
    "               verticalalignment='top',\n",
    "               bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "        ax.legend(title='Network Depth', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    # Set your paths here\n",
    "    nn_results_dir = \"/mnt/users/goringn/NNs_vs_Kernels/stair_function/results/msp_NN_grid_0701_mup_lr005\"\n",
    "    ntk_results_path = \"/mnt/users/goringn/NNs_vs_Kernels/stair_function/results/msp_NN_grid_1812_spectral/final_results_20241219_015151.json\"\n",
    "    output_dir = \"analysis_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load results\n",
    "    nn_results, hyperparams = load_experiment_data(nn_results_dir)\n",
    "    with open(ntk_results_path, 'r') as f:\n",
    "        ntk_results = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(nn_results)} NN results and {len(ntk_results)} NTK results\")\n",
    "    \n",
    "    # Create threshold plot\n",
    "    threshold_fig = create_threshold_plot(nn_results, ntk_results)\n",
    "    threshold_fig.savefig(os.path.join(output_dir, 'threshold_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create low-dimensional ratio plot\n",
    "    ratio_fig = plot_low_dim_ratio(nn_results, nn_results_dir)\n",
    "    ratio_fig.savefig(os.path.join(output_dir, 'low_dim_ratio.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create feature dimensionality histograms\n",
    "    hist_fig = create_feature_dim_histograms(nn_results, nn_results_dir)\n",
    "    hist_fig.savefig(os.path.join(output_dir, 'feature_dim_histograms.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close('all')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total results loaded: 866\n",
      "\n",
      "Loaded 866 NN results\n",
      "\n",
      "Unique configurations found:\n",
      "Width: 800, Depth: 4, N_train: 4000\n",
      "Width: 800, Depth: 4, N_train: 400\n",
      "Width: 800, Depth: 1, N_train: 10000\n",
      "Width: 800, Depth: 1, N_train: 2000\n",
      "Width: 800, Depth: 1, N_train: 10\n",
      "Width: 8000, Depth: 4, N_train: 4000\n",
      "Width: 8000, Depth: 4, N_train: 400\n",
      "Width: 8000, Depth: 1, N_train: 10000\n",
      "Width: 8000, Depth: 1, N_train: 2000\n",
      "Width: 8000, Depth: 1, N_train: 10\n",
      "Width: 2000, Depth: 4, N_train: 4000\n",
      "Width: 2000, Depth: 4, N_train: 400\n",
      "Width: 2000, Depth: 1, N_train: 10000\n",
      "Width: 2000, Depth: 1, N_train: 2000\n",
      "Width: 2000, Depth: 1, N_train: 10\n",
      "Width: 150, Depth: 4, N_train: 4000\n",
      "Width: 150, Depth: 4, N_train: 400\n",
      "Width: 150, Depth: 1, N_train: 10000\n",
      "Width: 150, Depth: 1, N_train: 2000\n",
      "Width: 150, Depth: 1, N_train: 10\n",
      "Width: 120, Depth: 4, N_train: 4000\n",
      "Width: 120, Depth: 4, N_train: 400\n",
      "Width: 120, Depth: 1, N_train: 10000\n",
      "Width: 120, Depth: 1, N_train: 2000\n",
      "Width: 120, Depth: 1, N_train: 10\n",
      "Width: 100, Depth: 4, N_train: 4000\n",
      "Width: 100, Depth: 4, N_train: 400\n",
      "Width: 100, Depth: 1, N_train: 10000\n",
      "Width: 100, Depth: 1, N_train: 2000\n",
      "Width: 100, Depth: 1, N_train: 10\n",
      "Width: 85, Depth: 4, N_train: 4000\n",
      "Width: 85, Depth: 4, N_train: 400\n",
      "Width: 85, Depth: 1, N_train: 10000\n",
      "Width: 85, Depth: 1, N_train: 2000\n",
      "Width: 85, Depth: 1, N_train: 10\n",
      "Width: 75, Depth: 4, N_train: 4000\n",
      "Width: 75, Depth: 4, N_train: 400\n",
      "Width: 75, Depth: 1, N_train: 10000\n",
      "Width: 75, Depth: 1, N_train: 2000\n",
      "Width: 75, Depth: 1, N_train: 10\n",
      "Width: 50, Depth: 4, N_train: 4000\n",
      "Width: 50, Depth: 4, N_train: 400\n",
      "Width: 50, Depth: 1, N_train: 10000\n",
      "Width: 50, Depth: 1, N_train: 2000\n",
      "Width: 50, Depth: 1, N_train: 10\n",
      "Width: 800, Depth: 4, N_train: 10000\n",
      "Width: 800, Depth: 4, N_train: 2000\n",
      "Width: 800, Depth: 4, N_train: 10\n",
      "Width: 800, Depth: 1, N_train: 4000\n",
      "Width: 800, Depth: 1, N_train: 400\n",
      "Width: 8000, Depth: 4, N_train: 10000\n",
      "Width: 8000, Depth: 4, N_train: 2000\n",
      "Width: 8000, Depth: 4, N_train: 10\n",
      "Width: 8000, Depth: 1, N_train: 4000\n",
      "Width: 8000, Depth: 1, N_train: 400\n",
      "Width: 2000, Depth: 4, N_train: 10000\n",
      "Width: 2000, Depth: 4, N_train: 2000\n",
      "Width: 2000, Depth: 4, N_train: 10\n",
      "Width: 2000, Depth: 1, N_train: 4000\n",
      "Width: 2000, Depth: 1, N_train: 400\n",
      "Width: 150, Depth: 4, N_train: 10000\n",
      "Width: 150, Depth: 4, N_train: 2000\n",
      "Width: 150, Depth: 4, N_train: 10\n",
      "Width: 150, Depth: 1, N_train: 4000\n",
      "Width: 150, Depth: 1, N_train: 400\n",
      "Width: 120, Depth: 4, N_train: 10000\n",
      "Width: 120, Depth: 4, N_train: 2000\n",
      "Width: 120, Depth: 4, N_train: 10\n",
      "Width: 120, Depth: 1, N_train: 4000\n",
      "Width: 120, Depth: 1, N_train: 400\n",
      "Width: 100, Depth: 4, N_train: 10000\n",
      "Width: 100, Depth: 4, N_train: 2000\n",
      "Width: 100, Depth: 4, N_train: 10\n",
      "Width: 100, Depth: 1, N_train: 4000\n",
      "Width: 100, Depth: 1, N_train: 400\n",
      "Width: 85, Depth: 4, N_train: 10000\n",
      "Width: 85, Depth: 4, N_train: 2000\n",
      "Width: 85, Depth: 4, N_train: 10\n",
      "Width: 85, Depth: 1, N_train: 4000\n",
      "Width: 85, Depth: 1, N_train: 400\n",
      "Width: 75, Depth: 4, N_train: 10000\n",
      "Width: 75, Depth: 4, N_train: 2000\n",
      "Width: 75, Depth: 4, N_train: 10\n",
      "Width: 75, Depth: 1, N_train: 4000\n",
      "Width: 75, Depth: 1, N_train: 400\n",
      "Width: 50, Depth: 4, N_train: 10000\n",
      "Width: 50, Depth: 4, N_train: 2000\n",
      "Width: 50, Depth: 4, N_train: 10\n",
      "Width: 50, Depth: 1, N_train: 4000\n",
      "Width: 50, Depth: 1, N_train: 400\n",
      "Width: 5000, Depth: 4, N_train: 10000\n",
      "Width: 5000, Depth: 4, N_train: 400\n",
      "Width: 5000, Depth: 1, N_train: 4000\n",
      "Width: 5000, Depth: 1, N_train: 10\n",
      "Width: 4000, Depth: 4, N_train: 2000\n",
      "Width: 4000, Depth: 1, N_train: 10000\n",
      "Width: 4000, Depth: 1, N_train: 400\n",
      "Width: 3000, Depth: 4, N_train: 4000\n",
      "Width: 3000, Depth: 4, N_train: 10\n",
      "Width: 3000, Depth: 1, N_train: 2000\n",
      "Width: 1500, Depth: 4, N_train: 10000\n",
      "Width: 1500, Depth: 4, N_train: 400\n",
      "Width: 1500, Depth: 1, N_train: 4000\n",
      "Width: 1500, Depth: 1, N_train: 10\n",
      "Width: 1000, Depth: 4, N_train: 2000\n",
      "Width: 1000, Depth: 1, N_train: 10000\n",
      "Width: 1000, Depth: 1, N_train: 400\n",
      "Width: 600, Depth: 4, N_train: 4000\n",
      "Width: 600, Depth: 4, N_train: 10\n",
      "Width: 600, Depth: 1, N_train: 2000\n",
      "Width: 500, Depth: 4, N_train: 10000\n",
      "Width: 500, Depth: 4, N_train: 400\n",
      "Width: 500, Depth: 1, N_train: 4000\n",
      "Width: 500, Depth: 1, N_train: 10\n",
      "Width: 400, Depth: 4, N_train: 2000\n",
      "Width: 400, Depth: 1, N_train: 10000\n",
      "Width: 400, Depth: 1, N_train: 400\n",
      "Width: 300, Depth: 4, N_train: 4000\n",
      "Width: 300, Depth: 4, N_train: 10\n",
      "Width: 300, Depth: 1, N_train: 2000\n",
      "Width: 200, Depth: 4, N_train: 10000\n",
      "Width: 200, Depth: 4, N_train: 400\n",
      "Width: 200, Depth: 1, N_train: 4000\n",
      "Width: 200, Depth: 1, N_train: 10\n",
      "Width: 40, Depth: 4, N_train: 2000\n",
      "Width: 40, Depth: 1, N_train: 10000\n",
      "Width: 40, Depth: 1, N_train: 400\n",
      "Width: 30, Depth: 4, N_train: 4000\n",
      "Width: 30, Depth: 4, N_train: 10\n",
      "Width: 30, Depth: 1, N_train: 2000\n",
      "Width: 20, Depth: 4, N_train: 10000\n",
      "Width: 20, Depth: 4, N_train: 400\n",
      "Width: 20, Depth: 1, N_train: 4000\n",
      "Width: 20, Depth: 1, N_train: 10\n",
      "Width: 10, Depth: 4, N_train: 2000\n",
      "Width: 10, Depth: 1, N_train: 10000\n",
      "Width: 10, Depth: 1, N_train: 400\n",
      "Width: 5000, Depth: 4, N_train: 2500\n",
      "Width: 5000, Depth: 1, N_train: 20000\n",
      "Width: 5000, Depth: 1, N_train: 800\n",
      "Width: 4000, Depth: 4, N_train: 4500\n",
      "Width: 4000, Depth: 4, N_train: 50\n",
      "Width: 4000, Depth: 1, N_train: 2500\n",
      "Width: 3000, Depth: 4, N_train: 20000\n",
      "Width: 3000, Depth: 4, N_train: 800\n",
      "Width: 3000, Depth: 1, N_train: 4500\n",
      "Width: 3000, Depth: 1, N_train: 50\n",
      "Width: 1500, Depth: 4, N_train: 2500\n",
      "Width: 1500, Depth: 1, N_train: 20000\n",
      "Width: 1500, Depth: 1, N_train: 800\n",
      "Width: 1000, Depth: 4, N_train: 4500\n",
      "Width: 1000, Depth: 4, N_train: 50\n",
      "Width: 1000, Depth: 1, N_train: 2500\n",
      "Width: 600, Depth: 4, N_train: 20000\n",
      "Width: 600, Depth: 4, N_train: 800\n",
      "Width: 600, Depth: 1, N_train: 4500\n",
      "Width: 600, Depth: 1, N_train: 50\n",
      "Width: 500, Depth: 4, N_train: 2500\n",
      "Width: 500, Depth: 1, N_train: 20000\n",
      "Width: 500, Depth: 1, N_train: 800\n",
      "Width: 400, Depth: 4, N_train: 4500\n",
      "Width: 400, Depth: 4, N_train: 50\n",
      "Width: 400, Depth: 1, N_train: 2500\n",
      "Width: 300, Depth: 4, N_train: 20000\n",
      "Width: 300, Depth: 4, N_train: 800\n",
      "Width: 300, Depth: 1, N_train: 4500\n",
      "Width: 300, Depth: 1, N_train: 50\n",
      "Width: 200, Depth: 4, N_train: 2500\n",
      "Width: 200, Depth: 1, N_train: 20000\n",
      "Width: 200, Depth: 1, N_train: 800\n",
      "Width: 40, Depth: 4, N_train: 4500\n",
      "Width: 40, Depth: 4, N_train: 50\n",
      "Width: 40, Depth: 1, N_train: 2500\n",
      "Width: 30, Depth: 4, N_train: 20000\n",
      "Width: 30, Depth: 4, N_train: 800\n",
      "Width: 30, Depth: 1, N_train: 4500\n",
      "Width: 30, Depth: 1, N_train: 50\n",
      "Width: 20, Depth: 4, N_train: 2500\n",
      "Width: 20, Depth: 1, N_train: 20000\n",
      "Width: 20, Depth: 1, N_train: 800\n",
      "Width: 10, Depth: 4, N_train: 4500\n",
      "Width: 10, Depth: 4, N_train: 50\n",
      "Width: 10, Depth: 1, N_train: 2500\n",
      "Width: 5000, Depth: 4, N_train: 4000\n",
      "Width: 5000, Depth: 4, N_train: 10\n",
      "Width: 5000, Depth: 1, N_train: 2000\n",
      "Width: 4000, Depth: 4, N_train: 10000\n",
      "Width: 4000, Depth: 4, N_train: 400\n",
      "Width: 4000, Depth: 1, N_train: 4000\n",
      "Width: 4000, Depth: 1, N_train: 10\n",
      "Width: 3000, Depth: 4, N_train: 2000\n",
      "Width: 3000, Depth: 1, N_train: 10000\n",
      "Width: 3000, Depth: 1, N_train: 400\n",
      "Width: 1500, Depth: 4, N_train: 4000\n",
      "Width: 1500, Depth: 4, N_train: 10\n",
      "Width: 1500, Depth: 1, N_train: 2000\n",
      "Width: 1000, Depth: 4, N_train: 10000\n",
      "Width: 1000, Depth: 4, N_train: 400\n",
      "Width: 1000, Depth: 1, N_train: 4000\n",
      "Width: 1000, Depth: 1, N_train: 10\n",
      "Width: 600, Depth: 4, N_train: 2000\n",
      "Width: 600, Depth: 1, N_train: 10000\n",
      "Width: 600, Depth: 1, N_train: 400\n",
      "Width: 500, Depth: 4, N_train: 4000\n",
      "Width: 500, Depth: 4, N_train: 10\n",
      "Width: 500, Depth: 1, N_train: 2000\n",
      "Width: 400, Depth: 4, N_train: 10000\n",
      "Width: 400, Depth: 4, N_train: 400\n",
      "Width: 400, Depth: 1, N_train: 4000\n",
      "Width: 400, Depth: 1, N_train: 10\n",
      "Width: 300, Depth: 4, N_train: 2000\n",
      "Width: 300, Depth: 1, N_train: 10000\n",
      "Width: 300, Depth: 1, N_train: 400\n",
      "Width: 200, Depth: 4, N_train: 4000\n",
      "Width: 200, Depth: 4, N_train: 10\n",
      "Width: 200, Depth: 1, N_train: 2000\n",
      "Width: 40, Depth: 4, N_train: 10000\n",
      "Width: 40, Depth: 4, N_train: 400\n",
      "Width: 40, Depth: 1, N_train: 4000\n",
      "Width: 40, Depth: 1, N_train: 10\n",
      "Width: 30, Depth: 4, N_train: 2000\n",
      "Width: 30, Depth: 1, N_train: 10000\n",
      "Width: 30, Depth: 1, N_train: 400\n",
      "Width: 20, Depth: 4, N_train: 4000\n",
      "Width: 20, Depth: 4, N_train: 10\n",
      "Width: 20, Depth: 1, N_train: 2000\n",
      "Width: 10, Depth: 4, N_train: 10000\n",
      "Width: 10, Depth: 4, N_train: 400\n",
      "Width: 10, Depth: 1, N_train: 4000\n",
      "Width: 10, Depth: 1, N_train: 10\n",
      "Width: 5000, Depth: 4, N_train: 20000\n",
      "Width: 5000, Depth: 4, N_train: 800\n",
      "Width: 5000, Depth: 1, N_train: 4500\n",
      "Width: 5000, Depth: 1, N_train: 50\n",
      "Width: 4000, Depth: 4, N_train: 2500\n",
      "Width: 4000, Depth: 1, N_train: 20000\n",
      "Width: 4000, Depth: 1, N_train: 800\n",
      "Width: 3000, Depth: 4, N_train: 4500\n",
      "Width: 3000, Depth: 4, N_train: 50\n",
      "Width: 3000, Depth: 1, N_train: 2500\n",
      "Width: 1500, Depth: 4, N_train: 20000\n",
      "Width: 1500, Depth: 4, N_train: 800\n",
      "Width: 1500, Depth: 1, N_train: 4500\n",
      "Width: 1500, Depth: 1, N_train: 50\n",
      "Width: 1000, Depth: 4, N_train: 2500\n",
      "Width: 1000, Depth: 1, N_train: 20000\n",
      "Width: 1000, Depth: 1, N_train: 800\n",
      "Width: 600, Depth: 4, N_train: 4500\n",
      "Width: 600, Depth: 4, N_train: 50\n",
      "Width: 600, Depth: 1, N_train: 2500\n",
      "Width: 500, Depth: 4, N_train: 20000\n",
      "Width: 500, Depth: 4, N_train: 800\n",
      "Width: 500, Depth: 1, N_train: 4500\n",
      "Width: 500, Depth: 1, N_train: 50\n",
      "Width: 400, Depth: 4, N_train: 2500\n",
      "Width: 400, Depth: 1, N_train: 20000\n",
      "Width: 400, Depth: 1, N_train: 800\n",
      "Width: 300, Depth: 4, N_train: 4500\n",
      "Width: 300, Depth: 4, N_train: 50\n",
      "Width: 300, Depth: 1, N_train: 2500\n",
      "Width: 200, Depth: 4, N_train: 20000\n",
      "Width: 200, Depth: 4, N_train: 800\n",
      "Width: 200, Depth: 1, N_train: 4500\n",
      "Width: 200, Depth: 1, N_train: 50\n",
      "Width: 40, Depth: 4, N_train: 2500\n",
      "Width: 40, Depth: 1, N_train: 20000\n",
      "Width: 40, Depth: 1, N_train: 800\n",
      "Width: 30, Depth: 4, N_train: 4500\n",
      "Width: 30, Depth: 4, N_train: 50\n",
      "Width: 30, Depth: 1, N_train: 2500\n",
      "Width: 20, Depth: 4, N_train: 20000\n",
      "Width: 20, Depth: 4, N_train: 800\n",
      "Width: 20, Depth: 1, N_train: 4500\n",
      "Width: 20, Depth: 1, N_train: 50\n",
      "Width: 10, Depth: 4, N_train: 2500\n",
      "Width: 10, Depth: 1, N_train: 20000\n",
      "Width: 10, Depth: 1, N_train: 800\n",
      "Width: 5000, Depth: 4, N_train: 2000\n",
      "Width: 5000, Depth: 1, N_train: 10000\n",
      "Width: 5000, Depth: 1, N_train: 400\n",
      "Width: 4000, Depth: 4, N_train: 4000\n",
      "Width: 4000, Depth: 4, N_train: 10\n",
      "Width: 4000, Depth: 1, N_train: 2000\n",
      "Width: 3000, Depth: 4, N_train: 10000\n",
      "Width: 3000, Depth: 4, N_train: 400\n",
      "Width: 3000, Depth: 1, N_train: 4000\n",
      "Width: 3000, Depth: 1, N_train: 10\n",
      "Width: 1500, Depth: 4, N_train: 2000\n",
      "Width: 1500, Depth: 1, N_train: 10000\n",
      "Width: 1500, Depth: 1, N_train: 400\n",
      "Width: 1000, Depth: 4, N_train: 4000\n",
      "Width: 1000, Depth: 4, N_train: 10\n",
      "Width: 1000, Depth: 1, N_train: 2000\n",
      "Width: 600, Depth: 4, N_train: 10000\n",
      "Width: 600, Depth: 4, N_train: 400\n",
      "Width: 600, Depth: 1, N_train: 4000\n",
      "Width: 600, Depth: 1, N_train: 10\n",
      "Width: 500, Depth: 4, N_train: 2000\n",
      "Width: 500, Depth: 1, N_train: 10000\n",
      "Width: 500, Depth: 1, N_train: 400\n",
      "Width: 400, Depth: 4, N_train: 4000\n",
      "Width: 400, Depth: 4, N_train: 10\n",
      "Width: 400, Depth: 1, N_train: 2000\n",
      "Width: 300, Depth: 4, N_train: 10000\n",
      "Width: 300, Depth: 4, N_train: 400\n",
      "Width: 300, Depth: 1, N_train: 4000\n",
      "Width: 300, Depth: 1, N_train: 10\n",
      "Width: 200, Depth: 4, N_train: 2000\n",
      "Width: 200, Depth: 1, N_train: 10000\n",
      "Width: 200, Depth: 1, N_train: 400\n",
      "Width: 40, Depth: 4, N_train: 4000\n",
      "Width: 40, Depth: 4, N_train: 10\n",
      "Width: 40, Depth: 1, N_train: 2000\n",
      "Width: 30, Depth: 4, N_train: 10000\n",
      "Width: 30, Depth: 4, N_train: 400\n",
      "Width: 30, Depth: 1, N_train: 4000\n",
      "Width: 30, Depth: 1, N_train: 10\n",
      "Width: 20, Depth: 4, N_train: 2000\n",
      "Width: 20, Depth: 1, N_train: 10000\n",
      "Width: 20, Depth: 1, N_train: 400\n",
      "Width: 10, Depth: 4, N_train: 4000\n",
      "Width: 10, Depth: 4, N_train: 10\n",
      "Width: 10, Depth: 1, N_train: 2000\n",
      "Width: 5000, Depth: 4, N_train: 4500\n",
      "Width: 5000, Depth: 4, N_train: 50\n",
      "Width: 5000, Depth: 1, N_train: 2500\n",
      "Width: 4000, Depth: 4, N_train: 20000\n",
      "Width: 4000, Depth: 4, N_train: 800\n",
      "Width: 4000, Depth: 1, N_train: 4500\n",
      "Width: 4000, Depth: 1, N_train: 50\n",
      "Width: 3000, Depth: 4, N_train: 2500\n",
      "Width: 3000, Depth: 1, N_train: 20000\n",
      "Width: 3000, Depth: 1, N_train: 800\n",
      "Width: 1500, Depth: 4, N_train: 4500\n",
      "Width: 1500, Depth: 4, N_train: 50\n",
      "Width: 1500, Depth: 1, N_train: 2500\n",
      "Width: 1000, Depth: 4, N_train: 20000\n",
      "Width: 1000, Depth: 4, N_train: 800\n",
      "Width: 1000, Depth: 1, N_train: 4500\n",
      "Width: 1000, Depth: 1, N_train: 50\n",
      "Width: 600, Depth: 4, N_train: 2500\n",
      "Width: 600, Depth: 1, N_train: 20000\n",
      "Width: 600, Depth: 1, N_train: 800\n",
      "Width: 500, Depth: 4, N_train: 4500\n",
      "Width: 500, Depth: 4, N_train: 50\n",
      "Width: 500, Depth: 1, N_train: 2500\n",
      "Width: 400, Depth: 4, N_train: 20000\n",
      "Width: 400, Depth: 4, N_train: 800\n",
      "Width: 400, Depth: 1, N_train: 4500\n",
      "Width: 400, Depth: 1, N_train: 50\n",
      "Width: 300, Depth: 4, N_train: 2500\n",
      "Width: 300, Depth: 1, N_train: 20000\n",
      "Width: 300, Depth: 1, N_train: 800\n",
      "Width: 200, Depth: 4, N_train: 4500\n",
      "Width: 200, Depth: 4, N_train: 50\n",
      "Width: 200, Depth: 1, N_train: 2500\n",
      "Width: 40, Depth: 4, N_train: 20000\n",
      "Width: 40, Depth: 4, N_train: 800\n",
      "Width: 40, Depth: 1, N_train: 4500\n",
      "Width: 40, Depth: 1, N_train: 50\n",
      "Width: 30, Depth: 4, N_train: 2500\n",
      "Width: 30, Depth: 1, N_train: 20000\n",
      "Width: 30, Depth: 1, N_train: 800\n",
      "Width: 20, Depth: 4, N_train: 4500\n",
      "Width: 20, Depth: 4, N_train: 50\n",
      "Width: 20, Depth: 1, N_train: 2500\n",
      "Width: 10, Depth: 4, N_train: 20000\n",
      "Width: 10, Depth: 4, N_train: 800\n",
      "Width: 10, Depth: 1, N_train: 4500\n",
      "Width: 10, Depth: 1, N_train: 50\n",
      "Width: 800, Depth: 4, N_train: 4500\n",
      "Width: 800, Depth: 4, N_train: 800\n",
      "Width: 800, Depth: 1, N_train: 20000\n",
      "Width: 800, Depth: 1, N_train: 2500\n",
      "Width: 800, Depth: 1, N_train: 50\n",
      "Width: 8000, Depth: 4, N_train: 4500\n",
      "Width: 8000, Depth: 4, N_train: 800\n",
      "Width: 8000, Depth: 1, N_train: 20000\n",
      "Width: 8000, Depth: 1, N_train: 2500\n",
      "Width: 8000, Depth: 1, N_train: 50\n",
      "Width: 2000, Depth: 4, N_train: 4500\n",
      "Width: 2000, Depth: 4, N_train: 800\n",
      "Width: 2000, Depth: 1, N_train: 20000\n",
      "Width: 2000, Depth: 1, N_train: 2500\n",
      "Width: 2000, Depth: 1, N_train: 50\n",
      "Width: 150, Depth: 4, N_train: 4500\n",
      "Width: 150, Depth: 4, N_train: 800\n",
      "Width: 150, Depth: 1, N_train: 20000\n",
      "Width: 150, Depth: 1, N_train: 2500\n",
      "Width: 150, Depth: 1, N_train: 50\n",
      "Width: 120, Depth: 4, N_train: 4500\n",
      "Width: 120, Depth: 4, N_train: 800\n",
      "Width: 120, Depth: 1, N_train: 20000\n",
      "Width: 120, Depth: 1, N_train: 2500\n",
      "Width: 120, Depth: 1, N_train: 50\n",
      "Width: 100, Depth: 4, N_train: 4500\n",
      "Width: 100, Depth: 4, N_train: 800\n",
      "Width: 100, Depth: 1, N_train: 20000\n",
      "Width: 100, Depth: 1, N_train: 2500\n",
      "Width: 100, Depth: 1, N_train: 50\n",
      "Width: 85, Depth: 4, N_train: 4500\n",
      "Width: 85, Depth: 4, N_train: 800\n",
      "Width: 85, Depth: 1, N_train: 20000\n",
      "Width: 85, Depth: 1, N_train: 2500\n",
      "Width: 85, Depth: 1, N_train: 50\n",
      "Width: 75, Depth: 4, N_train: 4500\n",
      "Width: 75, Depth: 4, N_train: 800\n",
      "Width: 75, Depth: 1, N_train: 20000\n",
      "Width: 75, Depth: 1, N_train: 2500\n",
      "Width: 75, Depth: 1, N_train: 50\n",
      "Width: 50, Depth: 4, N_train: 4500\n",
      "Width: 50, Depth: 4, N_train: 800\n",
      "Width: 50, Depth: 1, N_train: 20000\n",
      "Width: 50, Depth: 1, N_train: 2500\n",
      "Width: 50, Depth: 1, N_train: 50\n",
      "Width: 800, Depth: 4, N_train: 20000\n",
      "Width: 800, Depth: 4, N_train: 2500\n",
      "Width: 800, Depth: 4, N_train: 50\n",
      "Width: 800, Depth: 1, N_train: 4500\n",
      "Width: 800, Depth: 1, N_train: 800\n",
      "Width: 8000, Depth: 4, N_train: 20000\n",
      "Width: 8000, Depth: 4, N_train: 2500\n",
      "Width: 8000, Depth: 4, N_train: 50\n",
      "Width: 8000, Depth: 1, N_train: 4500\n",
      "Width: 8000, Depth: 1, N_train: 800\n",
      "Width: 2000, Depth: 4, N_train: 20000\n",
      "Width: 2000, Depth: 4, N_train: 2500\n",
      "Width: 2000, Depth: 4, N_train: 50\n",
      "Width: 2000, Depth: 1, N_train: 4500\n",
      "Width: 2000, Depth: 1, N_train: 800\n",
      "Width: 150, Depth: 4, N_train: 20000\n",
      "Width: 150, Depth: 4, N_train: 2500\n",
      "Width: 150, Depth: 4, N_train: 50\n",
      "Width: 150, Depth: 1, N_train: 4500\n",
      "Width: 150, Depth: 1, N_train: 800\n",
      "Width: 120, Depth: 4, N_train: 20000\n",
      "Width: 120, Depth: 4, N_train: 2500\n",
      "Width: 120, Depth: 4, N_train: 50\n",
      "Width: 120, Depth: 1, N_train: 4500\n",
      "Width: 120, Depth: 1, N_train: 800\n",
      "Width: 100, Depth: 4, N_train: 20000\n",
      "Width: 100, Depth: 4, N_train: 2500\n",
      "Width: 100, Depth: 4, N_train: 50\n",
      "Width: 100, Depth: 1, N_train: 4500\n",
      "Width: 100, Depth: 1, N_train: 800\n",
      "Width: 85, Depth: 4, N_train: 20000\n",
      "Width: 85, Depth: 4, N_train: 2500\n",
      "Width: 85, Depth: 4, N_train: 50\n",
      "Width: 85, Depth: 1, N_train: 4500\n",
      "Width: 85, Depth: 1, N_train: 800\n",
      "Width: 75, Depth: 4, N_train: 20000\n",
      "Width: 75, Depth: 4, N_train: 2500\n",
      "Width: 75, Depth: 4, N_train: 50\n",
      "Width: 75, Depth: 1, N_train: 4500\n",
      "Width: 75, Depth: 1, N_train: 800\n",
      "Width: 50, Depth: 4, N_train: 20000\n",
      "Width: 50, Depth: 4, N_train: 2500\n",
      "Width: 50, Depth: 4, N_train: 50\n",
      "Width: 50, Depth: 1, N_train: 4500\n",
      "Width: 50, Depth: 1, N_train: 800\n",
      "Width: 5000, Depth: 4, N_train: 40000\n",
      "Width: 5000, Depth: 4, N_train: 1500\n",
      "Width: 5000, Depth: 1, N_train: 8000\n",
      "Width: 5000, Depth: 1, N_train: 200\n",
      "Width: 4000, Depth: 4, N_train: 3500\n",
      "Width: 4000, Depth: 1, N_train: 40000\n",
      "Width: 4000, Depth: 1, N_train: 1500\n",
      "Width: 3000, Depth: 4, N_train: 8000\n",
      "Width: 3000, Depth: 4, N_train: 200\n",
      "Width: 3000, Depth: 1, N_train: 3500\n",
      "Width: 1500, Depth: 4, N_train: 40000\n",
      "Width: 1500, Depth: 4, N_train: 1500\n",
      "Width: 1500, Depth: 1, N_train: 8000\n",
      "Width: 1500, Depth: 1, N_train: 200\n",
      "Width: 1000, Depth: 4, N_train: 3500\n",
      "Width: 1000, Depth: 1, N_train: 40000\n",
      "Width: 1000, Depth: 1, N_train: 1500\n",
      "Width: 600, Depth: 4, N_train: 8000\n",
      "Width: 600, Depth: 4, N_train: 200\n",
      "Width: 600, Depth: 1, N_train: 3500\n",
      "Width: 500, Depth: 4, N_train: 40000\n",
      "Width: 500, Depth: 4, N_train: 1500\n",
      "Width: 500, Depth: 1, N_train: 8000\n",
      "Width: 500, Depth: 1, N_train: 200\n",
      "Width: 400, Depth: 4, N_train: 3500\n",
      "Width: 400, Depth: 1, N_train: 40000\n",
      "Width: 400, Depth: 1, N_train: 1500\n",
      "Width: 300, Depth: 4, N_train: 8000\n",
      "Width: 300, Depth: 4, N_train: 200\n",
      "Width: 300, Depth: 1, N_train: 3500\n",
      "Width: 200, Depth: 4, N_train: 40000\n",
      "Width: 200, Depth: 4, N_train: 1500\n",
      "Width: 200, Depth: 1, N_train: 8000\n",
      "Width: 200, Depth: 1, N_train: 200\n",
      "Width: 40, Depth: 4, N_train: 3500\n",
      "Width: 40, Depth: 1, N_train: 40000\n",
      "Width: 40, Depth: 1, N_train: 1500\n",
      "Width: 30, Depth: 4, N_train: 8000\n",
      "Width: 30, Depth: 4, N_train: 200\n",
      "Width: 30, Depth: 1, N_train: 3500\n",
      "Width: 20, Depth: 4, N_train: 40000\n",
      "Width: 20, Depth: 4, N_train: 1500\n",
      "Width: 20, Depth: 1, N_train: 8000\n",
      "Width: 20, Depth: 1, N_train: 200\n",
      "Width: 10, Depth: 4, N_train: 3500\n",
      "Width: 10, Depth: 1, N_train: 40000\n",
      "Width: 10, Depth: 1, N_train: 1500\n",
      "Width: 5000, Depth: 4, N_train: 8000\n",
      "Width: 5000, Depth: 4, N_train: 200\n",
      "Width: 5000, Depth: 1, N_train: 3500\n",
      "Width: 4000, Depth: 4, N_train: 40000\n",
      "Width: 4000, Depth: 4, N_train: 1500\n",
      "Width: 4000, Depth: 1, N_train: 8000\n",
      "Width: 4000, Depth: 1, N_train: 200\n",
      "Width: 3000, Depth: 4, N_train: 3500\n",
      "Width: 3000, Depth: 1, N_train: 40000\n",
      "Width: 3000, Depth: 1, N_train: 1500\n",
      "Width: 1500, Depth: 4, N_train: 8000\n",
      "Width: 1500, Depth: 4, N_train: 200\n",
      "Width: 1500, Depth: 1, N_train: 3500\n",
      "Width: 1000, Depth: 4, N_train: 40000\n",
      "Width: 1000, Depth: 4, N_train: 1500\n",
      "Width: 1000, Depth: 1, N_train: 8000\n",
      "Width: 1000, Depth: 1, N_train: 200\n",
      "Width: 600, Depth: 4, N_train: 3500\n",
      "Width: 600, Depth: 1, N_train: 40000\n",
      "Width: 600, Depth: 1, N_train: 1500\n",
      "Width: 500, Depth: 4, N_train: 8000\n",
      "Width: 500, Depth: 4, N_train: 200\n",
      "Width: 500, Depth: 1, N_train: 3500\n",
      "Width: 400, Depth: 4, N_train: 40000\n",
      "Width: 400, Depth: 4, N_train: 1500\n",
      "Width: 400, Depth: 1, N_train: 8000\n",
      "Width: 400, Depth: 1, N_train: 200\n",
      "Width: 300, Depth: 4, N_train: 3500\n",
      "Width: 300, Depth: 1, N_train: 40000\n",
      "Width: 300, Depth: 1, N_train: 1500\n",
      "Width: 200, Depth: 4, N_train: 8000\n",
      "Width: 200, Depth: 4, N_train: 200\n",
      "Width: 200, Depth: 1, N_train: 3500\n",
      "Width: 40, Depth: 4, N_train: 40000\n",
      "Width: 40, Depth: 4, N_train: 1500\n",
      "Width: 40, Depth: 1, N_train: 8000\n",
      "Width: 40, Depth: 1, N_train: 200\n",
      "Width: 30, Depth: 4, N_train: 3500\n",
      "Width: 30, Depth: 1, N_train: 40000\n",
      "Width: 30, Depth: 1, N_train: 1500\n",
      "Width: 20, Depth: 4, N_train: 8000\n",
      "Width: 20, Depth: 4, N_train: 200\n",
      "Width: 20, Depth: 1, N_train: 3500\n",
      "Width: 10, Depth: 4, N_train: 40000\n",
      "Width: 10, Depth: 4, N_train: 1500\n",
      "Width: 10, Depth: 1, N_train: 8000\n",
      "Width: 10, Depth: 1, N_train: 200\n",
      "Width: 5000, Depth: 4, N_train: 3500\n",
      "Width: 5000, Depth: 1, N_train: 40000\n",
      "Width: 5000, Depth: 1, N_train: 1500\n",
      "Width: 4000, Depth: 4, N_train: 8000\n",
      "Width: 4000, Depth: 4, N_train: 200\n",
      "Width: 4000, Depth: 1, N_train: 3500\n",
      "Width: 3000, Depth: 4, N_train: 40000\n",
      "Width: 3000, Depth: 4, N_train: 1500\n",
      "Width: 3000, Depth: 1, N_train: 8000\n",
      "Width: 3000, Depth: 1, N_train: 200\n",
      "Width: 1500, Depth: 4, N_train: 3500\n",
      "Width: 1500, Depth: 1, N_train: 40000\n",
      "Width: 1500, Depth: 1, N_train: 1500\n",
      "Width: 1000, Depth: 4, N_train: 8000\n",
      "Width: 1000, Depth: 4, N_train: 200\n",
      "Width: 1000, Depth: 1, N_train: 3500\n",
      "Width: 600, Depth: 4, N_train: 40000\n",
      "Width: 600, Depth: 4, N_train: 1500\n",
      "Width: 600, Depth: 1, N_train: 8000\n",
      "Width: 600, Depth: 1, N_train: 200\n",
      "Width: 500, Depth: 4, N_train: 3500\n",
      "Width: 500, Depth: 1, N_train: 40000\n",
      "Width: 500, Depth: 1, N_train: 1500\n",
      "Width: 400, Depth: 4, N_train: 8000\n",
      "Width: 400, Depth: 4, N_train: 200\n",
      "Width: 400, Depth: 1, N_train: 3500\n",
      "Width: 300, Depth: 4, N_train: 40000\n",
      "Width: 300, Depth: 4, N_train: 1500\n",
      "Width: 300, Depth: 1, N_train: 8000\n",
      "Width: 300, Depth: 1, N_train: 200\n",
      "Width: 200, Depth: 4, N_train: 3500\n",
      "Width: 200, Depth: 1, N_train: 40000\n",
      "Width: 200, Depth: 1, N_train: 1500\n",
      "Width: 40, Depth: 4, N_train: 8000\n",
      "Width: 40, Depth: 4, N_train: 200\n",
      "Width: 40, Depth: 1, N_train: 3500\n",
      "Width: 30, Depth: 4, N_train: 40000\n",
      "Width: 30, Depth: 4, N_train: 1500\n",
      "Width: 30, Depth: 1, N_train: 8000\n",
      "Width: 30, Depth: 1, N_train: 200\n",
      "Width: 20, Depth: 4, N_train: 3500\n",
      "Width: 20, Depth: 1, N_train: 40000\n",
      "Width: 20, Depth: 1, N_train: 1500\n",
      "Width: 10, Depth: 4, N_train: 8000\n",
      "Width: 10, Depth: 4, N_train: 200\n",
      "Width: 10, Depth: 1, N_train: 3500\n",
      "Width: 800, Depth: 4, N_train: 8000\n",
      "Width: 800, Depth: 4, N_train: 1500\n",
      "Width: 800, Depth: 1, N_train: 40000\n",
      "Width: 800, Depth: 1, N_train: 3500\n",
      "Width: 800, Depth: 1, N_train: 200\n",
      "Width: 8000, Depth: 4, N_train: 8000\n",
      "Width: 8000, Depth: 4, N_train: 1500\n",
      "Width: 8000, Depth: 1, N_train: 40000\n",
      "Width: 8000, Depth: 1, N_train: 3500\n",
      "Width: 8000, Depth: 1, N_train: 200\n",
      "Width: 2000, Depth: 4, N_train: 8000\n",
      "Width: 2000, Depth: 4, N_train: 1500\n",
      "Width: 2000, Depth: 1, N_train: 40000\n",
      "Width: 2000, Depth: 1, N_train: 3500\n",
      "Width: 2000, Depth: 1, N_train: 200\n",
      "Width: 150, Depth: 4, N_train: 8000\n",
      "Width: 150, Depth: 4, N_train: 1500\n",
      "Width: 150, Depth: 1, N_train: 40000\n",
      "Width: 150, Depth: 1, N_train: 3500\n",
      "Width: 150, Depth: 1, N_train: 200\n",
      "Width: 120, Depth: 4, N_train: 8000\n",
      "Width: 120, Depth: 4, N_train: 1500\n",
      "Width: 120, Depth: 1, N_train: 40000\n",
      "Width: 120, Depth: 1, N_train: 3500\n",
      "Width: 120, Depth: 1, N_train: 200\n",
      "Width: 100, Depth: 4, N_train: 8000\n",
      "Width: 100, Depth: 4, N_train: 1500\n",
      "Width: 100, Depth: 1, N_train: 40000\n",
      "Width: 100, Depth: 1, N_train: 3500\n",
      "Width: 100, Depth: 1, N_train: 200\n",
      "Width: 85, Depth: 4, N_train: 8000\n",
      "Width: 85, Depth: 4, N_train: 1500\n",
      "Width: 85, Depth: 1, N_train: 40000\n",
      "Width: 85, Depth: 1, N_train: 3500\n",
      "Width: 85, Depth: 1, N_train: 200\n",
      "Width: 75, Depth: 4, N_train: 8000\n",
      "Width: 75, Depth: 4, N_train: 1500\n",
      "Width: 75, Depth: 1, N_train: 40000\n",
      "Width: 75, Depth: 1, N_train: 3500\n",
      "Width: 75, Depth: 1, N_train: 200\n",
      "Width: 50, Depth: 4, N_train: 8000\n",
      "Width: 50, Depth: 4, N_train: 1500\n",
      "Width: 50, Depth: 1, N_train: 40000\n",
      "Width: 50, Depth: 1, N_train: 3500\n",
      "Width: 50, Depth: 1, N_train: 200\n",
      "Width: 800, Depth: 4, N_train: 40000\n",
      "Width: 800, Depth: 4, N_train: 3500\n",
      "Width: 800, Depth: 4, N_train: 200\n",
      "Width: 800, Depth: 1, N_train: 8000\n",
      "Width: 800, Depth: 1, N_train: 1500\n",
      "Width: 8000, Depth: 4, N_train: 40000\n",
      "Width: 5000, Depth: 4, N_train: 3000\n",
      "Width: 5000, Depth: 1, N_train: 30000\n",
      "Width: 5000, Depth: 1, N_train: 1000\n",
      "Width: 4000, Depth: 4, N_train: 5000\n",
      "Width: 4000, Depth: 4, N_train: 100\n",
      "Width: 4000, Depth: 1, N_train: 3000\n",
      "Width: 3000, Depth: 4, N_train: 30000\n",
      "Width: 3000, Depth: 4, N_train: 1000\n",
      "Width: 3000, Depth: 1, N_train: 5000\n",
      "Width: 3000, Depth: 1, N_train: 100\n",
      "Width: 1500, Depth: 4, N_train: 3000\n",
      "Width: 1500, Depth: 1, N_train: 30000\n",
      "Width: 1500, Depth: 1, N_train: 1000\n",
      "Width: 1000, Depth: 4, N_train: 5000\n",
      "Width: 1000, Depth: 4, N_train: 100\n",
      "Width: 1000, Depth: 1, N_train: 3000\n",
      "Width: 600, Depth: 4, N_train: 30000\n",
      "Width: 600, Depth: 4, N_train: 1000\n",
      "Width: 600, Depth: 1, N_train: 5000\n",
      "Width: 600, Depth: 1, N_train: 100\n",
      "Width: 500, Depth: 4, N_train: 3000\n",
      "Width: 500, Depth: 1, N_train: 30000\n",
      "Width: 500, Depth: 1, N_train: 1000\n",
      "Width: 400, Depth: 4, N_train: 5000\n",
      "Width: 400, Depth: 4, N_train: 100\n",
      "Width: 400, Depth: 1, N_train: 3000\n",
      "Width: 300, Depth: 4, N_train: 30000\n",
      "Width: 300, Depth: 4, N_train: 1000\n",
      "Width: 300, Depth: 1, N_train: 5000\n",
      "Width: 300, Depth: 1, N_train: 100\n",
      "Width: 200, Depth: 4, N_train: 3000\n",
      "Width: 200, Depth: 1, N_train: 30000\n",
      "Width: 200, Depth: 1, N_train: 1000\n",
      "Width: 40, Depth: 4, N_train: 5000\n",
      "Width: 40, Depth: 4, N_train: 100\n",
      "Width: 40, Depth: 1, N_train: 3000\n",
      "Width: 30, Depth: 4, N_train: 30000\n",
      "Width: 30, Depth: 4, N_train: 1000\n",
      "Width: 30, Depth: 1, N_train: 5000\n",
      "Width: 30, Depth: 1, N_train: 100\n",
      "Width: 20, Depth: 4, N_train: 3000\n",
      "Width: 20, Depth: 1, N_train: 30000\n",
      "Width: 20, Depth: 1, N_train: 1000\n",
      "Width: 10, Depth: 4, N_train: 5000\n",
      "Width: 10, Depth: 4, N_train: 100\n",
      "Width: 10, Depth: 1, N_train: 3000\n",
      "Width: 800, Depth: 4, N_train: 5000\n",
      "Width: 800, Depth: 4, N_train: 1000\n",
      "Width: 800, Depth: 1, N_train: 30000\n",
      "Width: 800, Depth: 1, N_train: 3000\n",
      "Width: 800, Depth: 1, N_train: 100\n",
      "Width: 8000, Depth: 4, N_train: 5000\n",
      "Width: 8000, Depth: 4, N_train: 1000\n",
      "Width: 8000, Depth: 1, N_train: 30000\n",
      "Width: 8000, Depth: 1, N_train: 3000\n",
      "Width: 8000, Depth: 1, N_train: 100\n",
      "Width: 2000, Depth: 4, N_train: 5000\n",
      "Width: 2000, Depth: 4, N_train: 1000\n",
      "Width: 2000, Depth: 1, N_train: 30000\n",
      "Width: 2000, Depth: 1, N_train: 3000\n",
      "Width: 2000, Depth: 1, N_train: 100\n",
      "Width: 150, Depth: 4, N_train: 5000\n",
      "Width: 150, Depth: 4, N_train: 1000\n",
      "Width: 150, Depth: 1, N_train: 30000\n",
      "Width: 150, Depth: 1, N_train: 3000\n",
      "Width: 150, Depth: 1, N_train: 100\n",
      "Width: 120, Depth: 4, N_train: 5000\n",
      "Width: 120, Depth: 4, N_train: 1000\n",
      "Width: 120, Depth: 1, N_train: 30000\n",
      "Width: 120, Depth: 1, N_train: 3000\n",
      "Width: 120, Depth: 1, N_train: 100\n",
      "Width: 100, Depth: 4, N_train: 5000\n",
      "Width: 100, Depth: 4, N_train: 1000\n",
      "Width: 100, Depth: 1, N_train: 30000\n",
      "Width: 100, Depth: 1, N_train: 3000\n",
      "Width: 100, Depth: 1, N_train: 100\n",
      "Width: 85, Depth: 4, N_train: 5000\n",
      "Width: 85, Depth: 4, N_train: 1000\n",
      "Width: 85, Depth: 1, N_train: 30000\n",
      "Width: 85, Depth: 1, N_train: 3000\n",
      "Width: 85, Depth: 1, N_train: 100\n",
      "Width: 75, Depth: 4, N_train: 5000\n",
      "Width: 75, Depth: 4, N_train: 1000\n",
      "Width: 75, Depth: 1, N_train: 30000\n",
      "Width: 75, Depth: 1, N_train: 3000\n",
      "Width: 75, Depth: 1, N_train: 100\n",
      "Width: 50, Depth: 4, N_train: 5000\n",
      "Width: 50, Depth: 4, N_train: 1000\n",
      "Width: 50, Depth: 1, N_train: 30000\n",
      "Width: 50, Depth: 1, N_train: 3000\n",
      "Width: 50, Depth: 1, N_train: 100\n",
      "Width: 800, Depth: 4, N_train: 30000\n",
      "Width: 800, Depth: 4, N_train: 3000\n",
      "Width: 800, Depth: 4, N_train: 100\n",
      "Width: 800, Depth: 1, N_train: 5000\n",
      "Width: 800, Depth: 1, N_train: 1000\n",
      "Width: 8000, Depth: 4, N_train: 30000\n",
      "Width: 8000, Depth: 4, N_train: 3000\n",
      "Width: 8000, Depth: 4, N_train: 100\n",
      "Width: 8000, Depth: 1, N_train: 5000\n",
      "Width: 8000, Depth: 1, N_train: 1000\n",
      "Width: 2000, Depth: 4, N_train: 30000\n",
      "Width: 2000, Depth: 4, N_train: 3000\n",
      "Width: 2000, Depth: 4, N_train: 100\n",
      "Width: 2000, Depth: 1, N_train: 5000\n",
      "Width: 2000, Depth: 1, N_train: 1000\n",
      "Width: 150, Depth: 4, N_train: 30000\n",
      "Width: 150, Depth: 4, N_train: 3000\n",
      "Width: 150, Depth: 4, N_train: 100\n",
      "Width: 150, Depth: 1, N_train: 5000\n",
      "Width: 150, Depth: 1, N_train: 1000\n",
      "Width: 120, Depth: 4, N_train: 30000\n",
      "Width: 120, Depth: 4, N_train: 3000\n",
      "Width: 120, Depth: 4, N_train: 100\n",
      "Width: 120, Depth: 1, N_train: 5000\n",
      "Width: 120, Depth: 1, N_train: 1000\n",
      "Width: 100, Depth: 4, N_train: 30000\n",
      "Width: 100, Depth: 4, N_train: 3000\n",
      "Width: 100, Depth: 4, N_train: 100\n",
      "Width: 100, Depth: 1, N_train: 5000\n",
      "Width: 100, Depth: 1, N_train: 1000\n",
      "Width: 5000, Depth: 4, N_train: 30000\n",
      "Width: 5000, Depth: 4, N_train: 1000\n",
      "Width: 5000, Depth: 1, N_train: 5000\n",
      "Width: 5000, Depth: 1, N_train: 100\n",
      "Width: 4000, Depth: 4, N_train: 3000\n",
      "Width: 4000, Depth: 1, N_train: 30000\n",
      "Width: 4000, Depth: 1, N_train: 1000\n",
      "Width: 3000, Depth: 4, N_train: 5000\n",
      "Width: 3000, Depth: 4, N_train: 100\n",
      "Width: 3000, Depth: 1, N_train: 3000\n",
      "Width: 1500, Depth: 4, N_train: 30000\n",
      "Width: 1500, Depth: 4, N_train: 1000\n",
      "Width: 1500, Depth: 1, N_train: 5000\n",
      "Width: 1500, Depth: 1, N_train: 100\n",
      "Width: 1000, Depth: 4, N_train: 3000\n",
      "Width: 1000, Depth: 1, N_train: 30000\n",
      "Width: 1000, Depth: 1, N_train: 1000\n",
      "Width: 600, Depth: 4, N_train: 5000\n",
      "Width: 600, Depth: 4, N_train: 100\n",
      "Width: 600, Depth: 1, N_train: 3000\n",
      "Width: 500, Depth: 4, N_train: 30000\n",
      "Width: 500, Depth: 4, N_train: 1000\n",
      "Width: 500, Depth: 1, N_train: 5000\n",
      "Width: 500, Depth: 1, N_train: 100\n",
      "Width: 400, Depth: 4, N_train: 3000\n",
      "Width: 400, Depth: 1, N_train: 30000\n",
      "Width: 400, Depth: 1, N_train: 1000\n",
      "Width: 300, Depth: 4, N_train: 5000\n",
      "Width: 300, Depth: 4, N_train: 100\n",
      "Width: 300, Depth: 1, N_train: 3000\n",
      "Width: 200, Depth: 4, N_train: 30000\n",
      "Width: 200, Depth: 4, N_train: 1000\n",
      "Width: 200, Depth: 1, N_train: 5000\n",
      "Width: 200, Depth: 1, N_train: 100\n",
      "Width: 40, Depth: 4, N_train: 3000\n",
      "Width: 40, Depth: 1, N_train: 30000\n",
      "Width: 40, Depth: 1, N_train: 1000\n",
      "Width: 30, Depth: 4, N_train: 5000\n",
      "Width: 30, Depth: 4, N_train: 100\n",
      "Width: 30, Depth: 1, N_train: 3000\n",
      "Width: 20, Depth: 4, N_train: 30000\n",
      "Width: 20, Depth: 4, N_train: 1000\n",
      "Width: 20, Depth: 1, N_train: 5000\n",
      "Width: 20, Depth: 1, N_train: 100\n",
      "Width: 10, Depth: 4, N_train: 3000\n",
      "Width: 10, Depth: 1, N_train: 30000\n",
      "Width: 10, Depth: 1, N_train: 1000\n",
      "Width: 5000, Depth: 4, N_train: 5000\n",
      "Width: 5000, Depth: 4, N_train: 100\n",
      "Width: 5000, Depth: 1, N_train: 3000\n",
      "Width: 4000, Depth: 4, N_train: 30000\n",
      "Width: 4000, Depth: 4, N_train: 1000\n",
      "Width: 4000, Depth: 1, N_train: 5000\n",
      "Width: 4000, Depth: 1, N_train: 100\n",
      "Width: 3000, Depth: 4, N_train: 3000\n",
      "Width: 3000, Depth: 1, N_train: 30000\n",
      "Width: 3000, Depth: 1, N_train: 1000\n",
      "Width: 1500, Depth: 4, N_train: 5000\n",
      "Width: 1500, Depth: 4, N_train: 100\n",
      "Width: 1500, Depth: 1, N_train: 3000\n",
      "Width: 1000, Depth: 4, N_train: 30000\n",
      "Width: 1000, Depth: 4, N_train: 1000\n",
      "Width: 1000, Depth: 1, N_train: 5000\n",
      "Width: 1000, Depth: 1, N_train: 100\n",
      "Width: 600, Depth: 4, N_train: 3000\n",
      "Width: 600, Depth: 1, N_train: 30000\n",
      "Width: 600, Depth: 1, N_train: 1000\n",
      "Width: 500, Depth: 4, N_train: 5000\n",
      "Width: 500, Depth: 4, N_train: 100\n",
      "Width: 500, Depth: 1, N_train: 3000\n",
      "Width: 400, Depth: 4, N_train: 30000\n",
      "Width: 400, Depth: 4, N_train: 1000\n",
      "Width: 400, Depth: 1, N_train: 5000\n",
      "Width: 400, Depth: 1, N_train: 100\n",
      "Width: 300, Depth: 4, N_train: 3000\n",
      "Width: 300, Depth: 1, N_train: 30000\n",
      "Width: 300, Depth: 1, N_train: 1000\n",
      "Width: 200, Depth: 4, N_train: 5000\n",
      "Width: 200, Depth: 4, N_train: 100\n",
      "Width: 200, Depth: 1, N_train: 3000\n",
      "Width: 40, Depth: 4, N_train: 30000\n",
      "Width: 40, Depth: 4, N_train: 1000\n",
      "Width: 40, Depth: 1, N_train: 5000\n",
      "Width: 40, Depth: 1, N_train: 100\n",
      "Width: 30, Depth: 4, N_train: 3000\n",
      "Width: 30, Depth: 1, N_train: 30000\n",
      "Width: 30, Depth: 1, N_train: 1000\n",
      "Width: 20, Depth: 4, N_train: 5000\n",
      "Width: 20, Depth: 4, N_train: 100\n",
      "Width: 20, Depth: 1, N_train: 3000\n",
      "Width: 10, Depth: 4, N_train: 30000\n",
      "Width: 10, Depth: 4, N_train: 1000\n",
      "Width: 10, Depth: 1, N_train: 5000\n",
      "Width: 10, Depth: 1, N_train: 100\n",
      "\n",
      "Found train sizes: [10, 50, 100, 200, 400, 800, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 8000, 10000, 20000, 30000, 40000]\n",
      "Found hidden sizes: [10, 20, 30, 40, 50, 75, 85, 100, 120, 150, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000, 3000, 4000, 5000, 8000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2828382/920722888.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "#### working msp\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_science_style(font_size: float = 8):\n",
    "    \"\"\"Set up consistent science styling with proper log-scale ticks.\"\"\"\n",
    "    plt.style.use('default')\n",
    "    mpl.rcParams.update({\n",
    "        # Font sizes\n",
    "        'font.size': font_size,\n",
    "        'axes.labelsize': font_size,\n",
    "        'xtick.labelsize': font_size,\n",
    "        'ytick.labelsize': font_size,\n",
    "        'legend.fontsize': font_size,\n",
    "        'figure.figsize': (7.5, 3.9),\n",
    "        'figure.dpi': 300,\n",
    "        \n",
    "        # Font settings\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['cmr10', 'Computer Modern Serif', 'DejaVu Serif'],\n",
    "        'text.usetex': False,\n",
    "        'axes.formatter.use_mathtext': True,\n",
    "        'mathtext.fontset': 'cm',\n",
    "        \n",
    "        # Axis settings\n",
    "        'axes.linewidth': 0.5,\n",
    "        'axes.spines.top': True,\n",
    "        'axes.spines.right': True,\n",
    "        'axes.spines.left': True,\n",
    "        'axes.spines.bottom': True,\n",
    "        \n",
    "        # Tick settings\n",
    "        'xtick.direction': 'in',\n",
    "        'ytick.direction': 'in',\n",
    "        'xtick.major.width': 0.5,\n",
    "        'ytick.major.width': 0.5,\n",
    "        'xtick.minor.width': 0.5,\n",
    "        'ytick.minor.width': 0.5,\n",
    "        'xtick.major.size': 3,\n",
    "        'ytick.major.size': 3,\n",
    "        'xtick.minor.size': 1.5,\n",
    "        'ytick.minor.size': 1.5,\n",
    "        'xtick.top': True,\n",
    "        'ytick.right': True,\n",
    "        \n",
    "        # Grid settings\n",
    "        'grid.linewidth': 0.5,\n",
    "        \n",
    "        # Line settings\n",
    "        'lines.linewidth': 2.0,\n",
    "        'lines.markersize': 3,\n",
    "        \n",
    "        # Legend settings\n",
    "        'legend.frameon': False,\n",
    "        'legend.borderpad': 0,\n",
    "        'legend.borderaxespad': 1.0,\n",
    "        'legend.handlelength': 1.0,\n",
    "        'legend.handletextpad': 0.5,\n",
    "    })\n",
    "\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, d: int, hidden_size: int, depth: int, mode: str = 'standard'):\n",
    "        super().__init__()\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        self.mode = mode\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dim = d\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = d\n",
    "        \n",
    "        for layer_idx in range(depth):\n",
    "            linear = torch.nn.Linear(prev_dim, hidden_size)\n",
    "            if mode == 'standard':\n",
    "                torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                torch.nn.init.zeros_(linear.bias)\n",
    "            layers.extend([linear, torch.nn.ReLU()])\n",
    "            prev_dim = hidden_size\n",
    "        \n",
    "        final_layer = torch.nn.Linear(prev_dim, 1)\n",
    "        if mode == 'standard':\n",
    "            torch.nn.init.xavier_uniform_(final_layer.weight)\n",
    "        torch.nn.init.zeros_(final_layer.bias)\n",
    "        layers.append(final_layer)\n",
    "        \n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def load_experiment_data(results_dir: str) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Load experiment results from rank-based result files\"\"\"\n",
    "    nn_files = list(Path(results_dir).glob(\"results*.json\"))\n",
    "    if not nn_files:\n",
    "        raise ValueError(f\"No result files found in {results_dir}\")\n",
    "    \n",
    "    combined_results = []\n",
    "    empty_files = []\n",
    "    \n",
    "    for file_path in nn_files:\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    empty_files.append(file_path)\n",
    "                    continue\n",
    "                results = json.loads(content)\n",
    "                if isinstance(results, list):\n",
    "                    combined_results.extend(results)\n",
    "                else:\n",
    "                    combined_results.append(results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if empty_files:\n",
    "        print(\"\\nEmpty files found:\")\n",
    "        for f in empty_files:\n",
    "            print(f\"  - {f}\")\n",
    "    \n",
    "    print(f\"\\nTotal results loaded: {len(combined_results)}\")\n",
    "    \n",
    "    hyperparams = {\n",
    "        \"ambient_dim\": 30,\n",
    "        \"hidden_sizes\": sorted(set(r['hidden_size'] for r in combined_results)),\n",
    "        \"depths\": sorted(set(r['depth'] for r in combined_results))\n",
    "    }\n",
    "    \n",
    "    for result in combined_results:\n",
    "        result['ambient_dim'] = hyperparams['ambient_dim']\n",
    "    \n",
    "    return combined_results, hyperparams\n",
    "\n",
    "def find_model_file(results_dir: str, model_prefix: str, rank: int) -> Optional[str]:\n",
    "    \"\"\"Find a model file matching the prefix and rank across different timestamps\"\"\"\n",
    "    timestamps = [\"20250107_150050\", \"20250107_151953\"]\n",
    "    patterns = [\n",
    "        f\"final_model_{model_prefix}_{{}}_rank{rank}.pt\",\n",
    "        f\"initial_model_{model_prefix}_{{}}_rank{rank}.pt\"\n",
    "    ]\n",
    "    \n",
    "    for timestamp in timestamps:\n",
    "        for pattern in patterns:\n",
    "            filename = pattern.format(timestamp)\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                return filepath\n",
    "    return None\n",
    "\n",
    "def load_model(results_dir: str, result: Dict) -> Optional[torch.nn.Module]:\n",
    "    \"\"\"Load just the model without dataset\"\"\"\n",
    "    try:\n",
    "        hidden_size = result['hidden_size']\n",
    "        depth = result['depth']\n",
    "        n_train = result['n_train']\n",
    "        lr = result.get('learning_rate', result.get('lr'))\n",
    "        mode = result.get('mode', 'mup_pennington')\n",
    "        rank = result.get('worker_rank', 0)\n",
    "        ambient_dim = result.get('ambient_dim', 20)\n",
    "        \n",
    "        model_prefix = f'h{hidden_size}_d{depth}_n{n_train}_lr{lr}_{mode}'\n",
    "        model_path = find_model_file(results_dir, model_prefix, rank)\n",
    "        \n",
    "        if not model_path:\n",
    "            print(f\"Warning: No model file found for prefix: {model_prefix}, rank: {rank}\")\n",
    "            return None\n",
    "        \n",
    "        model = DeepNN(ambient_dim, hidden_size, depth)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model = model.to(next(model.parameters()).device)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_feature_dimensionality(model: torch.nn.Module) -> np.ndarray:\n",
    "    \"\"\"Analyze feature dimensionality for layer 1 of the model\"\"\"\n",
    "    first_layer = None\n",
    "    for layer in model.network:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            first_layer = layer\n",
    "            break\n",
    "    \n",
    "    if first_layer is None:\n",
    "        raise ValueError(\"No linear layer found in model\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        features = first_layer.weight.data\n",
    "        feature_norms = torch.norm(features, dim=0, keepdim=True)\n",
    "        normalized_features = features / (feature_norms + 1e-8)\n",
    "        feature_dots = normalized_features.T @ features\n",
    "        feature_dots_squared = feature_dots ** 2\n",
    "        feature_denominator = torch.sum(feature_dots_squared, dim=1)\n",
    "        feature_numerator = torch.sum(features * features, dim=0)\n",
    "        feature_dims = feature_numerator / (feature_denominator + 1e-8)\n",
    "        \n",
    "        return feature_dims.cpu().numpy()\n",
    "\n",
    "def plot_low_dim_ratio(results: List[Dict], results_dir: str, threshold: float = 0.2, font_size: float = 8):\n",
    "    \"\"\"Plot ratio of features with dimensionality below threshold vs training size\"\"\"\n",
    "    setup_science_style(font_size=font_size)\n",
    "    \n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    print(\"\\nFound train sizes:\", train_sizes)\n",
    "    print(\"Found hidden sizes:\", hidden_sizes)\n",
    "    \n",
    "    # Create figure with specific size ratio\n",
    "    plt.figure(figsize=(7.5, 3.9))\n",
    "    \n",
    "    # Create color gradient from purple to cyan\n",
    "    num_sizes = len(hidden_sizes)\n",
    "    colors = []\n",
    "    start_rgb = np.array([238, 0, 255])   # #ee00ff (purple)\n",
    "    end_rgb = np.array([0, 251, 255])     # #00fbff (cyan)\n",
    "    \n",
    "    for i in range(num_sizes):\n",
    "        t = i / max(1, num_sizes - 1)\n",
    "        rgb = start_rgb * (1 - t) + end_rgb * t\n",
    "        rgb = rgb.astype(int)\n",
    "        colors.append(f'#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}')\n",
    "    \n",
    "    for idx, hidden_size in enumerate(hidden_sizes):\n",
    "        ratios = []\n",
    "        valid_train_sizes = []\n",
    "        \n",
    "        for n_train in train_sizes:\n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model = load_model(results_dir, result)\n",
    "                \n",
    "                if model is None:\n",
    "                    print(f\"Could not load model for width={hidden_size}, n_train={n_train}\")\n",
    "                    continue\n",
    "                    \n",
    "                feature_dims = analyze_feature_dimensionality(model)\n",
    "                ratio = np.mean(feature_dims < threshold)\n",
    "                ratios.append(ratio)\n",
    "                valid_train_sizes.append(n_train)\n",
    "        \n",
    "        if ratios:\n",
    "            plt.plot(valid_train_sizes, ratios, '-s', \n",
    "                    color=colors[idx],\n",
    "                    linewidth=2.0,\n",
    "                    markersize=3,\n",
    "                    label=f'$N={hidden_size}$')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(r'Training Set Size $m$', labelpad=2)\n",
    "    plt.ylabel(r'Ratio of Features with Dim $< 0.2$', labelpad=2)\n",
    "    \n",
    "    # Configure ticks and grid\n",
    "    ax = plt.gca()\n",
    "    ax.minorticks_on()\n",
    "    ax.tick_params(which='both', direction='in')\n",
    "    plt.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    # Place legend outside\n",
    "    plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left', fontsize=font_size)\n",
    "    \n",
    "    # Adjust layout to prevent legend cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def main():\n",
    "    # Set paths\n",
    "    results_dir = \"/mnt/users/goringn/NNs_vs_Kernels/stair_function/results/msp_NN_grid_0701_mup_lr005\"\n",
    "    output_dir = \"analysis_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load results\n",
    "    nn_results, hyperparams = load_experiment_data(results_dir)\n",
    "    print(f\"\\nLoaded {len(nn_results)} NN results\")\n",
    "    print(\"\\nUnique configurations found:\")\n",
    "    for result in nn_results:\n",
    "        print(f\"Width: {result['hidden_size']}, Depth: {result['depth']}, N_train: {result['n_train']}\")\n",
    "    \n",
    "    # Create plot\n",
    "    ratio_fig = plot_low_dim_ratio(nn_results, results_dir, font_size=7)\n",
    "    \n",
    "    # Save as PDF with high quality\n",
    "    ratio_fig.savefig(os.path.join(output_dir, 'low_dim_ratio.pdf'), \n",
    "                     format='pdf',\n",
    "                     dpi=300, \n",
    "                     bbox_inches='tight',\n",
    "                     facecolor='white',\n",
    "                     edgecolor='black')\n",
    "    \n",
    "    plt.close('all')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total results loaded: 827\n",
      "Loaded 827 NN results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2561990/3385437881.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def load_experiment_data(results_dir: str) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Load experiment results from rank-based result files\"\"\"\n",
    "    nn_files = list(Path(results_dir).glob(\"results*.json\"))\n",
    "    if not nn_files:\n",
    "        raise ValueError(f\"No result files found in {results_dir}\")\n",
    "    \n",
    "    combined_results = []\n",
    "    empty_files = []\n",
    "    \n",
    "    for file_path in nn_files:\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    empty_files.append(file_path)\n",
    "                    continue\n",
    "                    \n",
    "                results = json.loads(content)\n",
    "                if isinstance(results, list):\n",
    "                    combined_results.extend(results)\n",
    "                else:\n",
    "                    combined_results.append(results)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if empty_files:\n",
    "        print(\"\\nThe following files were empty:\")\n",
    "        for f in empty_files:\n",
    "            print(f\"  - {f}\")\n",
    "            \n",
    "    print(f\"\\nTotal results loaded: {len(combined_results)}\")\n",
    "    \n",
    "    # Build hyperparams from defaults and available data\n",
    "    hyperparams = {\n",
    "        \"ambient_dim\": 20,  # Fixed for this dataset\n",
    "        \"hidden_sizes\": sorted(set(r['hidden_size'] for r in combined_results)),\n",
    "        \"depths\": sorted(set(r['depth'] for r in combined_results))\n",
    "    }\n",
    "    \n",
    "    # Add ambient_dim to all results\n",
    "    for result in combined_results:\n",
    "        result['ambient_dim'] = hyperparams['ambient_dim']\n",
    "    \n",
    "    return combined_results, hyperparams\n",
    "\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, d: int, hidden_size: int, depth: int, mode: str = 'standard'):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dim = d\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = d\n",
    "        \n",
    "        for layer_idx in range(depth):\n",
    "            linear = torch.nn.Linear(prev_dim, hidden_size)\n",
    "            \n",
    "            if mode == 'standard':\n",
    "                torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                torch.nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            layers.extend([\n",
    "                linear,\n",
    "                torch.nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_size\n",
    "        \n",
    "        final_layer = torch.nn.Linear(prev_dim, 1)\n",
    "        if mode == 'standard':\n",
    "            torch.nn.init.xavier_uniform_(final_layer.weight)\n",
    "        torch.nn.init.zeros_(final_layer.bias)\n",
    "        layers.append(final_layer)\n",
    "        \n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def find_model_file(results_dir: str, model_prefix: str, rank: int) -> Optional[str]:\n",
    "    \"\"\"Find a model file matching the prefix and rank across different timestamps\"\"\"\n",
    "    timestamps = [\"20241230_145221\", \"20241230_145424\"]\n",
    "    patterns = [\n",
    "        f\"final_model_{model_prefix}_{{}}_rank{rank}.pt\",\n",
    "        f\"initial_model_{model_prefix}_{{}}_rank{rank}.pt\"\n",
    "    ]\n",
    "    \n",
    "    for timestamp in timestamps:\n",
    "        for pattern in patterns:\n",
    "            filename = pattern.format(timestamp)\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                return filepath\n",
    "    return None\n",
    "\n",
    "def load_model(results_dir: str, result: Dict) -> Optional[torch.nn.Module]:\n",
    "    \"\"\"Load just the model without dataset\"\"\"\n",
    "    try:\n",
    "        hidden_size = result['hidden_size']\n",
    "        depth = result['depth']\n",
    "        n_train = result['n_train']\n",
    "        lr = result.get('learning_rate', result.get('lr'))\n",
    "        mode = result.get('mode', 'mup_pennington')\n",
    "        rank = result.get('worker_rank', 0)\n",
    "        ambient_dim = result.get('ambient_dim', 20)  # Default to 20 if not found\n",
    "        \n",
    "        model_prefix = f'h{hidden_size}_d{depth}_n{n_train}_lr{lr}_{mode}'\n",
    "        \n",
    "        # Find model file\n",
    "        model_path = find_model_file(results_dir, model_prefix, rank)\n",
    "        \n",
    "        if not model_path:\n",
    "            print(f\"Warning: No model file found for prefix: h{hidden_size}_d{depth}_n{n_train}_lr{lr}_{mode}, rank: {rank}\")\n",
    "            return None\n",
    "        \n",
    "        model = DeepNN(ambient_dim, hidden_size, depth)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_feature_dimensionality(model: torch.nn.Module) -> np.ndarray:\n",
    "    \"\"\"Analyze feature dimensionality for layer 1 of the model\"\"\"\n",
    "    first_layer = None\n",
    "    for layer in model.network:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            first_layer = layer\n",
    "            break\n",
    "    \n",
    "    if first_layer is None:\n",
    "        raise ValueError(\"No linear layer found in model\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        features = first_layer.weight.data\n",
    "        feature_norms = torch.norm(features, dim=0, keepdim=True)\n",
    "        normalized_features = features / (feature_norms + 1e-8)\n",
    "        feature_dots = normalized_features.T @ features\n",
    "        feature_dots_squared = feature_dots ** 2\n",
    "        feature_denominator = torch.sum(feature_dots_squared, dim=1)\n",
    "        feature_numerator = torch.sum(features * features, dim=0)\n",
    "        feature_dims = feature_numerator / (feature_denominator + 1e-8)\n",
    "        \n",
    "        return feature_dims.cpu().numpy()\n",
    "\n",
    "def plot_low_dim_ratio(results: List[Dict], results_dir: str, threshold: float = 0.2):\n",
    "    \"\"\"Plot ratio of features with dimensionality below threshold vs training size\"\"\"\n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create colormap from red to blue\n",
    "    n_sizes = len(hidden_sizes)\n",
    "    colors = plt.cm.RdBu(np.linspace(0, 1, n_sizes))\n",
    "    \n",
    "    for idx, hidden_size in enumerate(hidden_sizes):\n",
    "        ratios = []\n",
    "        valid_train_sizes = []\n",
    "        for n_train in train_sizes:\n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model = load_model(results_dir, result)\n",
    "                \n",
    "                if model is not None:\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    ratio = np.mean(feature_dims < threshold)\n",
    "                    ratios.append(ratio)\n",
    "                    valid_train_sizes.append(n_train)\n",
    "        \n",
    "        if ratios:\n",
    "            plt.plot(valid_train_sizes, ratios, '-o', \n",
    "                    color=colors[idx], \n",
    "                    alpha=0.7,\n",
    "                    label=f'h={hidden_size}',\n",
    "                    markersize=4)\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel(f'Ratio of Features with Dim < {threshold}')\n",
    "    plt.title('Low-Dimensional Feature Ratio vs Training Size')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def create_feature_dim_histograms(results: List[Dict], results_dir: str):\n",
    "    \"\"\"Create histograms of feature dimensionality for different training sizes\"\"\"\n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    n_rows = len(hidden_sizes)\n",
    "    n_cols = min(6, len(train_sizes))  # Limit number of columns for readability\n",
    "    selected_train_sizes = np.logspace(np.log10(min(train_sizes)), \n",
    "                                     np.log10(max(train_sizes)), \n",
    "                                     n_cols).astype(int)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, hidden_size in enumerate(hidden_sizes):\n",
    "        for j, n_train in enumerate(selected_train_sizes):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Find closest available training size\n",
    "            closest_n_train = min(train_sizes, key=lambda x: abs(x - n_train))\n",
    "            \n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == closest_n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model = load_model(results_dir, result)\n",
    "                \n",
    "                if model is not None:\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    \n",
    "                    ax.hist(np.log1p(feature_dims), bins=30, alpha=0.7)\n",
    "                    ax.set_title(f'h={hidden_size}, n={closest_n_train}')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'No data available', \n",
    "                           ha='center', va='center',\n",
    "                           transform=ax.transAxes)\n",
    "                \n",
    "            if j == 0:\n",
    "                ax.set_ylabel('Count')\n",
    "            if i == n_rows-1:\n",
    "                ax.set_xlabel('log(1 + dim)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    # Set your paths here\n",
    "    results_dir = \"/mnt/users/goringn/NNs_vs_Kernels/low_dim_poly/results/low_dim_poly_NN_2812_mup_lr0001\"\n",
    "    output_dir = \"analysis_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load results\n",
    "    nn_results, hyperparams = load_experiment_data(results_dir)\n",
    "    print(f\"Loaded {len(nn_results)} NN results\")\n",
    "    \n",
    "    # Create low-dimensional ratio plot\n",
    "    ratio_fig = plot_low_dim_ratio(nn_results, results_dir)\n",
    "    ratio_fig.savefig(os.path.join(output_dir, 'low_dim_ratio.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create feature dimensionality histograms\n",
    "    hist_fig = create_feature_dim_histograms(nn_results, results_dir)\n",
    "    hist_fig.savefig(os.path.join(output_dir, 'feature_dim_histograms.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close('all')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total results loaded: 827\n",
      "\n",
      "Loaded 827 NN results\n",
      "\n",
      "Unique configurations found:\n",
      "Width: 5000, Depth: 4, N_train: 60000\n",
      "Width: 5000, Depth: 4, N_train: 2500\n",
      "Width: 5000, Depth: 4, N_train: 50\n",
      "Width: 5000, Depth: 1, N_train: 8000\n",
      "Width: 5000, Depth: 1, N_train: 200\n",
      "Width: 3000, Depth: 4, N_train: 15000\n",
      "Width: 3000, Depth: 4, N_train: 400\n",
      "Width: 3000, Depth: 1, N_train: 30000\n",
      "Width: 3000, Depth: 1, N_train: 800\n",
      "Width: 2000, Depth: 4, N_train: 60000\n",
      "Width: 2000, Depth: 4, N_train: 2500\n",
      "Width: 2000, Depth: 4, N_train: 50\n",
      "Width: 2000, Depth: 1, N_train: 8000\n",
      "Width: 2000, Depth: 1, N_train: 200\n",
      "Width: 800, Depth: 4, N_train: 15000\n",
      "Width: 800, Depth: 4, N_train: 400\n",
      "Width: 800, Depth: 1, N_train: 30000\n",
      "Width: 800, Depth: 1, N_train: 800\n",
      "Width: 600, Depth: 4, N_train: 60000\n",
      "Width: 600, Depth: 4, N_train: 2500\n",
      "Width: 600, Depth: 4, N_train: 50\n",
      "Width: 600, Depth: 1, N_train: 8000\n",
      "Width: 600, Depth: 1, N_train: 200\n",
      "Width: 400, Depth: 4, N_train: 15000\n",
      "Width: 400, Depth: 4, N_train: 400\n",
      "Width: 400, Depth: 1, N_train: 30000\n",
      "Width: 400, Depth: 1, N_train: 800\n",
      "Width: 150, Depth: 4, N_train: 60000\n",
      "Width: 150, Depth: 4, N_train: 2500\n",
      "Width: 150, Depth: 4, N_train: 50\n",
      "Width: 150, Depth: 1, N_train: 8000\n",
      "Width: 150, Depth: 1, N_train: 200\n",
      "Width: 120, Depth: 4, N_train: 15000\n",
      "Width: 120, Depth: 4, N_train: 400\n",
      "Width: 120, Depth: 1, N_train: 30000\n",
      "Width: 120, Depth: 1, N_train: 800\n",
      "Width: 100, Depth: 4, N_train: 60000\n",
      "Width: 100, Depth: 4, N_train: 2500\n",
      "Width: 100, Depth: 4, N_train: 50\n",
      "Width: 100, Depth: 1, N_train: 8000\n",
      "Width: 100, Depth: 1, N_train: 200\n",
      "Width: 85, Depth: 4, N_train: 15000\n",
      "Width: 85, Depth: 4, N_train: 400\n",
      "Width: 85, Depth: 1, N_train: 30000\n",
      "Width: 85, Depth: 1, N_train: 800\n",
      "Width: 75, Depth: 4, N_train: 60000\n",
      "Width: 75, Depth: 4, N_train: 2500\n",
      "Width: 75, Depth: 4, N_train: 50\n",
      "Width: 75, Depth: 1, N_train: 8000\n",
      "Width: 75, Depth: 1, N_train: 200\n",
      "Width: 50, Depth: 4, N_train: 15000\n",
      "Width: 50, Depth: 4, N_train: 400\n",
      "Width: 50, Depth: 1, N_train: 30000\n",
      "Width: 50, Depth: 1, N_train: 800\n",
      "Width: 40, Depth: 4, N_train: 60000\n",
      "Width: 40, Depth: 4, N_train: 2500\n",
      "Width: 40, Depth: 4, N_train: 50\n",
      "Width: 40, Depth: 1, N_train: 8000\n",
      "Width: 40, Depth: 1, N_train: 200\n",
      "Width: 5000, Depth: 4, N_train: 15000\n",
      "Width: 5000, Depth: 4, N_train: 400\n",
      "Width: 5000, Depth: 1, N_train: 30000\n",
      "Width: 5000, Depth: 1, N_train: 800\n",
      "Width: 3000, Depth: 4, N_train: 60000\n",
      "Width: 3000, Depth: 4, N_train: 2500\n",
      "Width: 3000, Depth: 4, N_train: 50\n",
      "Width: 3000, Depth: 1, N_train: 8000\n",
      "Width: 3000, Depth: 1, N_train: 200\n",
      "Width: 2000, Depth: 4, N_train: 15000\n",
      "Width: 2000, Depth: 4, N_train: 400\n",
      "Width: 2000, Depth: 1, N_train: 30000\n",
      "Width: 2000, Depth: 1, N_train: 800\n",
      "Width: 800, Depth: 4, N_train: 60000\n",
      "Width: 800, Depth: 4, N_train: 2500\n",
      "Width: 800, Depth: 4, N_train: 50\n",
      "Width: 800, Depth: 1, N_train: 8000\n",
      "Width: 800, Depth: 1, N_train: 200\n",
      "Width: 600, Depth: 4, N_train: 15000\n",
      "Width: 600, Depth: 4, N_train: 400\n",
      "Width: 600, Depth: 1, N_train: 30000\n",
      "Width: 600, Depth: 1, N_train: 800\n",
      "Width: 400, Depth: 4, N_train: 60000\n",
      "Width: 400, Depth: 4, N_train: 2500\n",
      "Width: 400, Depth: 4, N_train: 50\n",
      "Width: 400, Depth: 1, N_train: 8000\n",
      "Width: 400, Depth: 1, N_train: 200\n",
      "Width: 150, Depth: 4, N_train: 15000\n",
      "Width: 150, Depth: 4, N_train: 400\n",
      "Width: 150, Depth: 1, N_train: 30000\n",
      "Width: 150, Depth: 1, N_train: 800\n",
      "Width: 120, Depth: 4, N_train: 60000\n",
      "Width: 120, Depth: 4, N_train: 2500\n",
      "Width: 120, Depth: 4, N_train: 50\n",
      "Width: 120, Depth: 1, N_train: 8000\n",
      "Width: 120, Depth: 1, N_train: 200\n",
      "Width: 100, Depth: 4, N_train: 15000\n",
      "Width: 100, Depth: 4, N_train: 400\n",
      "Width: 100, Depth: 1, N_train: 30000\n",
      "Width: 100, Depth: 1, N_train: 800\n",
      "Width: 85, Depth: 4, N_train: 60000\n",
      "Width: 85, Depth: 4, N_train: 2500\n",
      "Width: 85, Depth: 4, N_train: 50\n",
      "Width: 85, Depth: 1, N_train: 8000\n",
      "Width: 85, Depth: 1, N_train: 200\n",
      "Width: 75, Depth: 4, N_train: 15000\n",
      "Width: 75, Depth: 4, N_train: 400\n",
      "Width: 75, Depth: 1, N_train: 30000\n",
      "Width: 75, Depth: 1, N_train: 800\n",
      "Width: 50, Depth: 4, N_train: 60000\n",
      "Width: 50, Depth: 4, N_train: 2500\n",
      "Width: 50, Depth: 4, N_train: 50\n",
      "Width: 50, Depth: 1, N_train: 8000\n",
      "Width: 50, Depth: 1, N_train: 200\n",
      "Width: 40, Depth: 4, N_train: 15000\n",
      "Width: 40, Depth: 4, N_train: 400\n",
      "Width: 40, Depth: 1, N_train: 30000\n",
      "Width: 40, Depth: 1, N_train: 800\n",
      "Width: 4000, Depth: 4, N_train: 500\n",
      "Width: 4000, Depth: 1, N_train: 10000\n",
      "Width: 4000, Depth: 1, N_train: 10\n",
      "Width: 1500, Depth: 4, N_train: 500\n",
      "Width: 1500, Depth: 1, N_train: 10000\n",
      "Width: 1500, Depth: 1, N_train: 10\n",
      "Width: 1000, Depth: 4, N_train: 500\n",
      "Width: 1000, Depth: 1, N_train: 10000\n",
      "Width: 1000, Depth: 1, N_train: 10\n",
      "Width: 300, Depth: 4, N_train: 500\n",
      "Width: 300, Depth: 1, N_train: 10000\n",
      "Width: 300, Depth: 1, N_train: 10\n",
      "Width: 200, Depth: 4, N_train: 500\n",
      "Width: 200, Depth: 1, N_train: 10000\n",
      "Width: 200, Depth: 1, N_train: 10\n",
      "Width: 500, Depth: 4, N_train: 500\n",
      "Width: 500, Depth: 1, N_train: 10000\n",
      "Width: 500, Depth: 1, N_train: 10\n",
      "Width: 30, Depth: 4, N_train: 500\n",
      "Width: 30, Depth: 1, N_train: 10000\n",
      "Width: 30, Depth: 1, N_train: 10\n",
      "Width: 20, Depth: 4, N_train: 500\n",
      "Width: 20, Depth: 1, N_train: 10000\n",
      "Width: 20, Depth: 1, N_train: 10\n",
      "Width: 10, Depth: 4, N_train: 500\n",
      "Width: 10, Depth: 1, N_train: 10000\n",
      "Width: 10, Depth: 1, N_train: 10\n",
      "Width: 8000, Depth: 4, N_train: 500\n",
      "Width: 8000, Depth: 1, N_train: 10000\n",
      "Width: 8000, Depth: 1, N_train: 10\n",
      "Width: 4000, Depth: 4, N_train: 1000\n",
      "Width: 4000, Depth: 1, N_train: 20000\n",
      "Width: 4000, Depth: 1, N_train: 100\n",
      "Width: 1500, Depth: 4, N_train: 1000\n",
      "Width: 1500, Depth: 1, N_train: 20000\n",
      "Width: 1500, Depth: 1, N_train: 100\n",
      "Width: 1000, Depth: 4, N_train: 1000\n",
      "Width: 1000, Depth: 1, N_train: 20000\n",
      "Width: 1000, Depth: 1, N_train: 100\n",
      "Width: 300, Depth: 4, N_train: 1000\n",
      "Width: 300, Depth: 1, N_train: 20000\n",
      "Width: 300, Depth: 1, N_train: 100\n",
      "Width: 200, Depth: 4, N_train: 1000\n",
      "Width: 200, Depth: 1, N_train: 20000\n",
      "Width: 200, Depth: 1, N_train: 100\n",
      "Width: 500, Depth: 4, N_train: 1000\n",
      "Width: 500, Depth: 1, N_train: 20000\n",
      "Width: 500, Depth: 1, N_train: 100\n",
      "Width: 30, Depth: 4, N_train: 1000\n",
      "Width: 30, Depth: 1, N_train: 20000\n",
      "Width: 30, Depth: 1, N_train: 100\n",
      "Width: 20, Depth: 4, N_train: 1000\n",
      "Width: 20, Depth: 1, N_train: 20000\n",
      "Width: 20, Depth: 1, N_train: 100\n",
      "Width: 10, Depth: 4, N_train: 1000\n",
      "Width: 10, Depth: 1, N_train: 20000\n",
      "Width: 10, Depth: 1, N_train: 100\n",
      "Width: 8000, Depth: 4, N_train: 1000\n",
      "Width: 8000, Depth: 1, N_train: 20000\n",
      "Width: 8000, Depth: 1, N_train: 100\n",
      "Width: 4000, Depth: 4, N_train: 10000\n",
      "Width: 4000, Depth: 4, N_train: 10\n",
      "Width: 4000, Depth: 1, N_train: 500\n",
      "Width: 1500, Depth: 4, N_train: 10000\n",
      "Width: 1500, Depth: 4, N_train: 10\n",
      "Width: 1500, Depth: 1, N_train: 500\n",
      "Width: 1000, Depth: 4, N_train: 10000\n",
      "Width: 1000, Depth: 4, N_train: 10\n",
      "Width: 1000, Depth: 1, N_train: 500\n",
      "Width: 300, Depth: 4, N_train: 10000\n",
      "Width: 300, Depth: 4, N_train: 10\n",
      "Width: 300, Depth: 1, N_train: 500\n",
      "Width: 200, Depth: 4, N_train: 10000\n",
      "Width: 200, Depth: 4, N_train: 10\n",
      "Width: 200, Depth: 1, N_train: 500\n",
      "Width: 500, Depth: 4, N_train: 10000\n",
      "Width: 500, Depth: 4, N_train: 10\n",
      "Width: 500, Depth: 1, N_train: 500\n",
      "Width: 30, Depth: 4, N_train: 10000\n",
      "Width: 30, Depth: 4, N_train: 10\n",
      "Width: 30, Depth: 1, N_train: 500\n",
      "Width: 20, Depth: 4, N_train: 10000\n",
      "Width: 20, Depth: 4, N_train: 10\n",
      "Width: 20, Depth: 1, N_train: 500\n",
      "Width: 10, Depth: 4, N_train: 10000\n",
      "Width: 10, Depth: 4, N_train: 10\n",
      "Width: 10, Depth: 1, N_train: 500\n",
      "Width: 8000, Depth: 4, N_train: 10000\n",
      "Width: 8000, Depth: 4, N_train: 10\n",
      "Width: 8000, Depth: 1, N_train: 500\n",
      "Width: 4000, Depth: 4, N_train: 40000\n",
      "Width: 4000, Depth: 4, N_train: 300\n",
      "Width: 4000, Depth: 1, N_train: 5000\n",
      "Width: 1500, Depth: 4, N_train: 40000\n",
      "Width: 1500, Depth: 4, N_train: 300\n",
      "Width: 1500, Depth: 1, N_train: 5000\n",
      "Width: 1000, Depth: 4, N_train: 40000\n",
      "Width: 1000, Depth: 4, N_train: 300\n",
      "Width: 1000, Depth: 1, N_train: 5000\n",
      "Width: 300, Depth: 4, N_train: 40000\n",
      "Width: 300, Depth: 4, N_train: 300\n",
      "Width: 300, Depth: 1, N_train: 5000\n",
      "Width: 200, Depth: 4, N_train: 40000\n",
      "Width: 200, Depth: 4, N_train: 300\n",
      "Width: 200, Depth: 1, N_train: 5000\n",
      "Width: 500, Depth: 4, N_train: 40000\n",
      "Width: 500, Depth: 4, N_train: 300\n",
      "Width: 500, Depth: 1, N_train: 5000\n",
      "Width: 30, Depth: 4, N_train: 40000\n",
      "Width: 30, Depth: 4, N_train: 300\n",
      "Width: 30, Depth: 1, N_train: 5000\n",
      "Width: 20, Depth: 4, N_train: 40000\n",
      "Width: 20, Depth: 4, N_train: 300\n",
      "Width: 20, Depth: 1, N_train: 5000\n",
      "Width: 10, Depth: 4, N_train: 40000\n",
      "Width: 10, Depth: 4, N_train: 300\n",
      "Width: 10, Depth: 1, N_train: 5000\n",
      "Width: 8000, Depth: 4, N_train: 40000\n",
      "Width: 8000, Depth: 4, N_train: 300\n",
      "Width: 8000, Depth: 1, N_train: 5000\n",
      "Width: 4000, Depth: 4, N_train: 15000\n",
      "Width: 4000, Depth: 4, N_train: 50\n",
      "Width: 4000, Depth: 1, N_train: 800\n",
      "Width: 1500, Depth: 4, N_train: 15000\n",
      "Width: 1500, Depth: 4, N_train: 50\n",
      "Width: 1500, Depth: 1, N_train: 800\n",
      "Width: 1000, Depth: 4, N_train: 15000\n",
      "Width: 1000, Depth: 4, N_train: 50\n",
      "Width: 1000, Depth: 1, N_train: 800\n",
      "Width: 300, Depth: 4, N_train: 15000\n",
      "Width: 300, Depth: 4, N_train: 50\n",
      "Width: 300, Depth: 1, N_train: 800\n",
      "Width: 200, Depth: 4, N_train: 15000\n",
      "Width: 200, Depth: 4, N_train: 50\n",
      "Width: 200, Depth: 1, N_train: 800\n",
      "Width: 500, Depth: 4, N_train: 15000\n",
      "Width: 500, Depth: 4, N_train: 50\n",
      "Width: 500, Depth: 1, N_train: 800\n",
      "Width: 30, Depth: 4, N_train: 15000\n",
      "Width: 30, Depth: 4, N_train: 50\n",
      "Width: 30, Depth: 1, N_train: 800\n",
      "Width: 20, Depth: 4, N_train: 15000\n",
      "Width: 20, Depth: 4, N_train: 50\n",
      "Width: 20, Depth: 1, N_train: 800\n",
      "Width: 10, Depth: 4, N_train: 15000\n",
      "Width: 10, Depth: 4, N_train: 50\n",
      "Width: 10, Depth: 1, N_train: 800\n",
      "Width: 8000, Depth: 4, N_train: 15000\n",
      "Width: 8000, Depth: 4, N_train: 50\n",
      "Width: 8000, Depth: 1, N_train: 800\n",
      "Width: 4000, Depth: 4, N_train: 60000\n",
      "Width: 4000, Depth: 4, N_train: 400\n",
      "Width: 4000, Depth: 1, N_train: 8000\n",
      "Width: 1500, Depth: 4, N_train: 60000\n",
      "Width: 1500, Depth: 4, N_train: 400\n",
      "Width: 1500, Depth: 1, N_train: 8000\n",
      "Width: 1000, Depth: 4, N_train: 60000\n",
      "Width: 1000, Depth: 4, N_train: 400\n",
      "Width: 1000, Depth: 1, N_train: 8000\n",
      "Width: 300, Depth: 4, N_train: 60000\n",
      "Width: 300, Depth: 4, N_train: 400\n",
      "Width: 300, Depth: 1, N_train: 8000\n",
      "Width: 200, Depth: 4, N_train: 60000\n",
      "Width: 200, Depth: 4, N_train: 400\n",
      "Width: 200, Depth: 1, N_train: 8000\n",
      "Width: 500, Depth: 4, N_train: 60000\n",
      "Width: 500, Depth: 4, N_train: 400\n",
      "Width: 500, Depth: 1, N_train: 8000\n",
      "Width: 30, Depth: 4, N_train: 60000\n",
      "Width: 30, Depth: 4, N_train: 400\n",
      "Width: 30, Depth: 1, N_train: 8000\n",
      "Width: 20, Depth: 4, N_train: 60000\n",
      "Width: 20, Depth: 4, N_train: 400\n",
      "Width: 20, Depth: 1, N_train: 8000\n",
      "Width: 10, Depth: 4, N_train: 60000\n",
      "Width: 10, Depth: 4, N_train: 400\n",
      "Width: 10, Depth: 1, N_train: 8000\n",
      "Width: 8000, Depth: 4, N_train: 400\n",
      "Width: 8000, Depth: 1, N_train: 8000\n",
      "Width: 5000, Depth: 4, N_train: 40000\n",
      "Width: 5000, Depth: 4, N_train: 1000\n",
      "Width: 5000, Depth: 4, N_train: 10\n",
      "Width: 5000, Depth: 1, N_train: 5000\n",
      "Width: 5000, Depth: 1, N_train: 100\n",
      "Width: 3000, Depth: 4, N_train: 10000\n",
      "Width: 3000, Depth: 4, N_train: 300\n",
      "Width: 3000, Depth: 1, N_train: 20000\n",
      "Width: 3000, Depth: 1, N_train: 500\n",
      "Width: 2000, Depth: 4, N_train: 40000\n",
      "Width: 2000, Depth: 4, N_train: 1000\n",
      "Width: 2000, Depth: 4, N_train: 10\n",
      "Width: 2000, Depth: 1, N_train: 5000\n",
      "Width: 2000, Depth: 1, N_train: 100\n",
      "Width: 800, Depth: 4, N_train: 10000\n",
      "Width: 800, Depth: 4, N_train: 300\n",
      "Width: 800, Depth: 1, N_train: 20000\n",
      "Width: 800, Depth: 1, N_train: 500\n",
      "Width: 600, Depth: 4, N_train: 40000\n",
      "Width: 600, Depth: 4, N_train: 1000\n",
      "Width: 600, Depth: 4, N_train: 10\n",
      "Width: 600, Depth: 1, N_train: 5000\n",
      "Width: 600, Depth: 1, N_train: 100\n",
      "Width: 400, Depth: 4, N_train: 10000\n",
      "Width: 400, Depth: 4, N_train: 300\n",
      "Width: 400, Depth: 1, N_train: 20000\n",
      "Width: 400, Depth: 1, N_train: 500\n",
      "Width: 150, Depth: 4, N_train: 40000\n",
      "Width: 150, Depth: 4, N_train: 1000\n",
      "Width: 150, Depth: 4, N_train: 10\n",
      "Width: 150, Depth: 1, N_train: 5000\n",
      "Width: 150, Depth: 1, N_train: 100\n",
      "Width: 120, Depth: 4, N_train: 10000\n",
      "Width: 120, Depth: 4, N_train: 300\n",
      "Width: 120, Depth: 1, N_train: 20000\n",
      "Width: 120, Depth: 1, N_train: 500\n",
      "Width: 100, Depth: 4, N_train: 40000\n",
      "Width: 100, Depth: 4, N_train: 1000\n",
      "Width: 100, Depth: 4, N_train: 10\n",
      "Width: 100, Depth: 1, N_train: 5000\n",
      "Width: 100, Depth: 1, N_train: 100\n",
      "Width: 85, Depth: 4, N_train: 10000\n",
      "Width: 85, Depth: 4, N_train: 300\n",
      "Width: 85, Depth: 1, N_train: 20000\n",
      "Width: 85, Depth: 1, N_train: 500\n",
      "Width: 75, Depth: 4, N_train: 40000\n",
      "Width: 75, Depth: 4, N_train: 1000\n",
      "Width: 75, Depth: 4, N_train: 10\n",
      "Width: 75, Depth: 1, N_train: 5000\n",
      "Width: 75, Depth: 1, N_train: 100\n",
      "Width: 50, Depth: 4, N_train: 10000\n",
      "Width: 50, Depth: 4, N_train: 300\n",
      "Width: 50, Depth: 1, N_train: 20000\n",
      "Width: 50, Depth: 1, N_train: 500\n",
      "Width: 40, Depth: 4, N_train: 40000\n",
      "Width: 40, Depth: 4, N_train: 1000\n",
      "Width: 40, Depth: 4, N_train: 10\n",
      "Width: 40, Depth: 1, N_train: 5000\n",
      "Width: 40, Depth: 1, N_train: 100\n",
      "Width: 5000, Depth: 4, N_train: 10000\n",
      "Width: 5000, Depth: 4, N_train: 300\n",
      "Width: 5000, Depth: 1, N_train: 20000\n",
      "Width: 5000, Depth: 1, N_train: 500\n",
      "Width: 3000, Depth: 4, N_train: 40000\n",
      "Width: 3000, Depth: 4, N_train: 1000\n",
      "Width: 3000, Depth: 4, N_train: 10\n",
      "Width: 3000, Depth: 1, N_train: 5000\n",
      "Width: 3000, Depth: 1, N_train: 100\n",
      "Width: 2000, Depth: 4, N_train: 10000\n",
      "Width: 2000, Depth: 4, N_train: 300\n",
      "Width: 2000, Depth: 1, N_train: 20000\n",
      "Width: 2000, Depth: 1, N_train: 500\n",
      "Width: 800, Depth: 4, N_train: 40000\n",
      "Width: 800, Depth: 4, N_train: 1000\n",
      "Width: 800, Depth: 4, N_train: 10\n",
      "Width: 800, Depth: 1, N_train: 5000\n",
      "Width: 800, Depth: 1, N_train: 100\n",
      "Width: 600, Depth: 4, N_train: 10000\n",
      "Width: 600, Depth: 4, N_train: 300\n",
      "Width: 600, Depth: 1, N_train: 20000\n",
      "Width: 600, Depth: 1, N_train: 500\n",
      "Width: 400, Depth: 4, N_train: 40000\n",
      "Width: 400, Depth: 4, N_train: 1000\n",
      "Width: 400, Depth: 4, N_train: 10\n",
      "Width: 400, Depth: 1, N_train: 5000\n",
      "Width: 400, Depth: 1, N_train: 100\n",
      "Width: 150, Depth: 4, N_train: 10000\n",
      "Width: 150, Depth: 4, N_train: 300\n",
      "Width: 150, Depth: 1, N_train: 20000\n",
      "Width: 150, Depth: 1, N_train: 500\n",
      "Width: 120, Depth: 4, N_train: 40000\n",
      "Width: 120, Depth: 4, N_train: 1000\n",
      "Width: 120, Depth: 4, N_train: 10\n",
      "Width: 120, Depth: 1, N_train: 5000\n",
      "Width: 120, Depth: 1, N_train: 100\n",
      "Width: 100, Depth: 4, N_train: 10000\n",
      "Width: 100, Depth: 4, N_train: 300\n",
      "Width: 100, Depth: 1, N_train: 20000\n",
      "Width: 100, Depth: 1, N_train: 500\n",
      "Width: 85, Depth: 4, N_train: 40000\n",
      "Width: 85, Depth: 4, N_train: 1000\n",
      "Width: 85, Depth: 4, N_train: 10\n",
      "Width: 85, Depth: 1, N_train: 5000\n",
      "Width: 85, Depth: 1, N_train: 100\n",
      "Width: 75, Depth: 4, N_train: 10000\n",
      "Width: 75, Depth: 4, N_train: 300\n",
      "Width: 75, Depth: 1, N_train: 20000\n",
      "Width: 75, Depth: 1, N_train: 500\n",
      "Width: 50, Depth: 4, N_train: 40000\n",
      "Width: 50, Depth: 4, N_train: 1000\n",
      "Width: 50, Depth: 4, N_train: 10\n",
      "Width: 50, Depth: 1, N_train: 5000\n",
      "Width: 50, Depth: 1, N_train: 100\n",
      "Width: 40, Depth: 4, N_train: 10000\n",
      "Width: 40, Depth: 4, N_train: 300\n",
      "Width: 40, Depth: 1, N_train: 20000\n",
      "Width: 40, Depth: 1, N_train: 500\n",
      "Width: 4000, Depth: 4, N_train: 800\n",
      "Width: 4000, Depth: 1, N_train: 15000\n",
      "Width: 4000, Depth: 1, N_train: 50\n",
      "Width: 1500, Depth: 4, N_train: 800\n",
      "Width: 1500, Depth: 1, N_train: 15000\n",
      "Width: 1500, Depth: 1, N_train: 50\n",
      "Width: 1000, Depth: 4, N_train: 800\n",
      "Width: 1000, Depth: 1, N_train: 15000\n",
      "Width: 1000, Depth: 1, N_train: 50\n",
      "Width: 300, Depth: 4, N_train: 800\n",
      "Width: 300, Depth: 1, N_train: 15000\n",
      "Width: 300, Depth: 1, N_train: 50\n",
      "Width: 200, Depth: 4, N_train: 800\n",
      "Width: 200, Depth: 1, N_train: 15000\n",
      "Width: 200, Depth: 1, N_train: 50\n",
      "Width: 500, Depth: 4, N_train: 800\n",
      "Width: 500, Depth: 1, N_train: 15000\n",
      "Width: 500, Depth: 1, N_train: 50\n",
      "Width: 30, Depth: 4, N_train: 800\n",
      "Width: 30, Depth: 1, N_train: 15000\n",
      "Width: 30, Depth: 1, N_train: 50\n",
      "Width: 20, Depth: 4, N_train: 800\n",
      "Width: 20, Depth: 1, N_train: 15000\n",
      "Width: 20, Depth: 1, N_train: 50\n",
      "Width: 10, Depth: 4, N_train: 800\n",
      "Width: 10, Depth: 1, N_train: 15000\n",
      "Width: 10, Depth: 1, N_train: 50\n",
      "Width: 8000, Depth: 4, N_train: 800\n",
      "Width: 8000, Depth: 1, N_train: 15000\n",
      "Width: 8000, Depth: 1, N_train: 50\n",
      "Width: 4000, Depth: 4, N_train: 2500\n",
      "Width: 4000, Depth: 1, N_train: 30000\n",
      "Width: 4000, Depth: 1, N_train: 200\n",
      "Width: 1500, Depth: 4, N_train: 2500\n",
      "Width: 1500, Depth: 1, N_train: 30000\n",
      "Width: 1500, Depth: 1, N_train: 200\n",
      "Width: 1000, Depth: 4, N_train: 2500\n",
      "Width: 1000, Depth: 1, N_train: 30000\n",
      "Width: 1000, Depth: 1, N_train: 200\n",
      "Width: 300, Depth: 4, N_train: 2500\n",
      "Width: 300, Depth: 1, N_train: 30000\n",
      "Width: 300, Depth: 1, N_train: 200\n",
      "Width: 200, Depth: 4, N_train: 2500\n",
      "Width: 200, Depth: 1, N_train: 30000\n",
      "Width: 200, Depth: 1, N_train: 200\n",
      "Width: 500, Depth: 4, N_train: 2500\n",
      "Width: 500, Depth: 1, N_train: 30000\n",
      "Width: 500, Depth: 1, N_train: 200\n",
      "Width: 30, Depth: 4, N_train: 2500\n",
      "Width: 30, Depth: 1, N_train: 30000\n",
      "Width: 30, Depth: 1, N_train: 200\n",
      "Width: 20, Depth: 4, N_train: 2500\n",
      "Width: 20, Depth: 1, N_train: 30000\n",
      "Width: 20, Depth: 1, N_train: 200\n",
      "Width: 10, Depth: 4, N_train: 2500\n",
      "Width: 10, Depth: 1, N_train: 30000\n",
      "Width: 10, Depth: 1, N_train: 200\n",
      "Width: 8000, Depth: 4, N_train: 2500\n",
      "Width: 8000, Depth: 1, N_train: 30000\n",
      "Width: 8000, Depth: 1, N_train: 200\n",
      "Width: 4000, Depth: 4, N_train: 8000\n",
      "Width: 4000, Depth: 1, N_train: 60000\n",
      "Width: 4000, Depth: 1, N_train: 400\n",
      "Width: 1500, Depth: 4, N_train: 8000\n",
      "Width: 1500, Depth: 1, N_train: 60000\n",
      "Width: 1500, Depth: 1, N_train: 400\n",
      "Width: 1000, Depth: 4, N_train: 8000\n",
      "Width: 1000, Depth: 1, N_train: 60000\n",
      "Width: 1000, Depth: 1, N_train: 400\n",
      "Width: 300, Depth: 4, N_train: 8000\n",
      "Width: 300, Depth: 1, N_train: 60000\n",
      "Width: 300, Depth: 1, N_train: 400\n",
      "Width: 200, Depth: 4, N_train: 8000\n",
      "Width: 200, Depth: 1, N_train: 60000\n",
      "Width: 200, Depth: 1, N_train: 400\n",
      "Width: 500, Depth: 4, N_train: 8000\n",
      "Width: 500, Depth: 1, N_train: 60000\n",
      "Width: 500, Depth: 1, N_train: 400\n",
      "Width: 30, Depth: 4, N_train: 8000\n",
      "Width: 30, Depth: 1, N_train: 60000\n",
      "Width: 30, Depth: 1, N_train: 400\n",
      "Width: 20, Depth: 4, N_train: 8000\n",
      "Width: 20, Depth: 1, N_train: 60000\n",
      "Width: 20, Depth: 1, N_train: 400\n",
      "Width: 10, Depth: 4, N_train: 8000\n",
      "Width: 10, Depth: 1, N_train: 60000\n",
      "Width: 10, Depth: 1, N_train: 400\n",
      "Width: 8000, Depth: 4, N_train: 8000\n",
      "Width: 8000, Depth: 1, N_train: 60000\n",
      "Width: 8000, Depth: 1, N_train: 400\n",
      "Width: 4000, Depth: 4, N_train: 30000\n",
      "Width: 4000, Depth: 4, N_train: 200\n",
      "Width: 4000, Depth: 1, N_train: 2500\n",
      "Width: 1500, Depth: 4, N_train: 30000\n",
      "Width: 1500, Depth: 4, N_train: 200\n",
      "Width: 1500, Depth: 1, N_train: 2500\n",
      "Width: 1000, Depth: 4, N_train: 30000\n",
      "Width: 1000, Depth: 4, N_train: 200\n",
      "Width: 1000, Depth: 1, N_train: 2500\n",
      "Width: 300, Depth: 4, N_train: 30000\n",
      "Width: 300, Depth: 4, N_train: 200\n",
      "Width: 300, Depth: 1, N_train: 2500\n",
      "Width: 200, Depth: 4, N_train: 30000\n",
      "Width: 200, Depth: 4, N_train: 200\n",
      "Width: 200, Depth: 1, N_train: 2500\n",
      "Width: 500, Depth: 4, N_train: 30000\n",
      "Width: 500, Depth: 4, N_train: 200\n",
      "Width: 500, Depth: 1, N_train: 2500\n",
      "Width: 30, Depth: 4, N_train: 30000\n",
      "Width: 30, Depth: 4, N_train: 200\n",
      "Width: 30, Depth: 1, N_train: 2500\n",
      "Width: 20, Depth: 4, N_train: 30000\n",
      "Width: 20, Depth: 4, N_train: 200\n",
      "Width: 20, Depth: 1, N_train: 2500\n",
      "Width: 10, Depth: 4, N_train: 30000\n",
      "Width: 10, Depth: 4, N_train: 200\n",
      "Width: 10, Depth: 1, N_train: 2500\n",
      "Width: 8000, Depth: 4, N_train: 30000\n",
      "Width: 8000, Depth: 4, N_train: 200\n",
      "Width: 8000, Depth: 1, N_train: 2500\n",
      "Width: 5000, Depth: 4, N_train: 20000\n",
      "Width: 5000, Depth: 4, N_train: 500\n",
      "Width: 5000, Depth: 1, N_train: 40000\n",
      "Width: 5000, Depth: 1, N_train: 1000\n",
      "Width: 5000, Depth: 1, N_train: 10\n",
      "Width: 3000, Depth: 4, N_train: 5000\n",
      "Width: 3000, Depth: 4, N_train: 100\n",
      "Width: 3000, Depth: 1, N_train: 10000\n",
      "Width: 3000, Depth: 1, N_train: 300\n",
      "Width: 2000, Depth: 4, N_train: 20000\n",
      "Width: 2000, Depth: 4, N_train: 500\n",
      "Width: 2000, Depth: 1, N_train: 40000\n",
      "Width: 2000, Depth: 1, N_train: 1000\n",
      "Width: 2000, Depth: 1, N_train: 10\n",
      "Width: 800, Depth: 4, N_train: 5000\n",
      "Width: 800, Depth: 4, N_train: 100\n",
      "Width: 800, Depth: 1, N_train: 10000\n",
      "Width: 800, Depth: 1, N_train: 300\n",
      "Width: 600, Depth: 4, N_train: 20000\n",
      "Width: 600, Depth: 4, N_train: 500\n",
      "Width: 600, Depth: 1, N_train: 40000\n",
      "Width: 600, Depth: 1, N_train: 1000\n",
      "Width: 600, Depth: 1, N_train: 10\n",
      "Width: 400, Depth: 4, N_train: 5000\n",
      "Width: 400, Depth: 4, N_train: 100\n",
      "Width: 400, Depth: 1, N_train: 10000\n",
      "Width: 400, Depth: 1, N_train: 300\n",
      "Width: 150, Depth: 4, N_train: 20000\n",
      "Width: 150, Depth: 4, N_train: 500\n",
      "Width: 150, Depth: 1, N_train: 40000\n",
      "Width: 150, Depth: 1, N_train: 1000\n",
      "Width: 150, Depth: 1, N_train: 10\n",
      "Width: 120, Depth: 4, N_train: 5000\n",
      "Width: 120, Depth: 4, N_train: 100\n",
      "Width: 120, Depth: 1, N_train: 10000\n",
      "Width: 120, Depth: 1, N_train: 300\n",
      "Width: 100, Depth: 4, N_train: 20000\n",
      "Width: 100, Depth: 4, N_train: 500\n",
      "Width: 100, Depth: 1, N_train: 40000\n",
      "Width: 100, Depth: 1, N_train: 1000\n",
      "Width: 100, Depth: 1, N_train: 10\n",
      "Width: 85, Depth: 4, N_train: 5000\n",
      "Width: 85, Depth: 4, N_train: 100\n",
      "Width: 85, Depth: 1, N_train: 10000\n",
      "Width: 85, Depth: 1, N_train: 300\n",
      "Width: 75, Depth: 4, N_train: 20000\n",
      "Width: 75, Depth: 4, N_train: 500\n",
      "Width: 75, Depth: 1, N_train: 40000\n",
      "Width: 75, Depth: 1, N_train: 1000\n",
      "Width: 75, Depth: 1, N_train: 10\n",
      "Width: 50, Depth: 4, N_train: 5000\n",
      "Width: 50, Depth: 4, N_train: 100\n",
      "Width: 50, Depth: 1, N_train: 10000\n",
      "Width: 50, Depth: 1, N_train: 300\n",
      "Width: 40, Depth: 4, N_train: 20000\n",
      "Width: 40, Depth: 4, N_train: 500\n",
      "Width: 40, Depth: 1, N_train: 40000\n",
      "Width: 40, Depth: 1, N_train: 1000\n",
      "Width: 40, Depth: 1, N_train: 10\n",
      "Width: 5000, Depth: 4, N_train: 5000\n",
      "Width: 5000, Depth: 4, N_train: 100\n",
      "Width: 5000, Depth: 1, N_train: 10000\n",
      "Width: 5000, Depth: 1, N_train: 300\n",
      "Width: 3000, Depth: 4, N_train: 20000\n",
      "Width: 3000, Depth: 4, N_train: 500\n",
      "Width: 3000, Depth: 1, N_train: 40000\n",
      "Width: 3000, Depth: 1, N_train: 1000\n",
      "Width: 3000, Depth: 1, N_train: 10\n",
      "Width: 2000, Depth: 4, N_train: 5000\n",
      "Width: 2000, Depth: 4, N_train: 100\n",
      "Width: 2000, Depth: 1, N_train: 10000\n",
      "Width: 2000, Depth: 1, N_train: 300\n",
      "Width: 800, Depth: 4, N_train: 20000\n",
      "Width: 800, Depth: 4, N_train: 500\n",
      "Width: 800, Depth: 1, N_train: 40000\n",
      "Width: 800, Depth: 1, N_train: 1000\n",
      "Width: 800, Depth: 1, N_train: 10\n",
      "Width: 600, Depth: 4, N_train: 5000\n",
      "Width: 600, Depth: 4, N_train: 100\n",
      "Width: 600, Depth: 1, N_train: 10000\n",
      "Width: 600, Depth: 1, N_train: 300\n",
      "Width: 400, Depth: 4, N_train: 20000\n",
      "Width: 400, Depth: 4, N_train: 500\n",
      "Width: 400, Depth: 1, N_train: 40000\n",
      "Width: 400, Depth: 1, N_train: 1000\n",
      "Width: 400, Depth: 1, N_train: 10\n",
      "Width: 150, Depth: 4, N_train: 5000\n",
      "Width: 150, Depth: 4, N_train: 100\n",
      "Width: 150, Depth: 1, N_train: 10000\n",
      "Width: 150, Depth: 1, N_train: 300\n",
      "Width: 120, Depth: 4, N_train: 20000\n",
      "Width: 120, Depth: 4, N_train: 500\n",
      "Width: 120, Depth: 1, N_train: 40000\n",
      "Width: 120, Depth: 1, N_train: 1000\n",
      "Width: 120, Depth: 1, N_train: 10\n",
      "Width: 100, Depth: 4, N_train: 5000\n",
      "Width: 100, Depth: 4, N_train: 100\n",
      "Width: 100, Depth: 1, N_train: 10000\n",
      "Width: 100, Depth: 1, N_train: 300\n",
      "Width: 85, Depth: 4, N_train: 20000\n",
      "Width: 85, Depth: 4, N_train: 500\n",
      "Width: 85, Depth: 1, N_train: 40000\n",
      "Width: 85, Depth: 1, N_train: 1000\n",
      "Width: 85, Depth: 1, N_train: 10\n",
      "Width: 75, Depth: 4, N_train: 5000\n",
      "Width: 75, Depth: 4, N_train: 100\n",
      "Width: 75, Depth: 1, N_train: 10000\n",
      "Width: 75, Depth: 1, N_train: 300\n",
      "Width: 50, Depth: 4, N_train: 20000\n",
      "Width: 50, Depth: 4, N_train: 500\n",
      "Width: 50, Depth: 1, N_train: 40000\n",
      "Width: 50, Depth: 1, N_train: 1000\n",
      "Width: 50, Depth: 1, N_train: 10\n",
      "Width: 40, Depth: 4, N_train: 5000\n",
      "Width: 40, Depth: 4, N_train: 100\n",
      "Width: 40, Depth: 1, N_train: 10000\n",
      "Width: 40, Depth: 1, N_train: 300\n",
      "Width: 5000, Depth: 4, N_train: 30000\n",
      "Width: 5000, Depth: 4, N_train: 800\n",
      "Width: 5000, Depth: 1, N_train: 60000\n",
      "Width: 5000, Depth: 1, N_train: 2500\n",
      "Width: 5000, Depth: 1, N_train: 50\n",
      "Width: 3000, Depth: 4, N_train: 8000\n",
      "Width: 3000, Depth: 4, N_train: 200\n",
      "Width: 3000, Depth: 1, N_train: 15000\n",
      "Width: 3000, Depth: 1, N_train: 400\n",
      "Width: 2000, Depth: 4, N_train: 30000\n",
      "Width: 2000, Depth: 4, N_train: 800\n",
      "Width: 2000, Depth: 1, N_train: 60000\n",
      "Width: 2000, Depth: 1, N_train: 2500\n",
      "Width: 2000, Depth: 1, N_train: 50\n",
      "Width: 800, Depth: 4, N_train: 8000\n",
      "Width: 800, Depth: 4, N_train: 200\n",
      "Width: 800, Depth: 1, N_train: 15000\n",
      "Width: 800, Depth: 1, N_train: 400\n",
      "Width: 600, Depth: 4, N_train: 30000\n",
      "Width: 600, Depth: 4, N_train: 800\n",
      "Width: 600, Depth: 1, N_train: 60000\n",
      "Width: 600, Depth: 1, N_train: 2500\n",
      "Width: 600, Depth: 1, N_train: 50\n",
      "Width: 400, Depth: 4, N_train: 8000\n",
      "Width: 400, Depth: 4, N_train: 200\n",
      "Width: 400, Depth: 1, N_train: 15000\n",
      "Width: 400, Depth: 1, N_train: 400\n",
      "Width: 150, Depth: 4, N_train: 30000\n",
      "Width: 150, Depth: 4, N_train: 800\n",
      "Width: 150, Depth: 1, N_train: 60000\n",
      "Width: 150, Depth: 1, N_train: 2500\n",
      "Width: 150, Depth: 1, N_train: 50\n",
      "Width: 120, Depth: 4, N_train: 8000\n",
      "Width: 120, Depth: 4, N_train: 200\n",
      "Width: 120, Depth: 1, N_train: 15000\n",
      "Width: 120, Depth: 1, N_train: 400\n",
      "Width: 100, Depth: 4, N_train: 30000\n",
      "Width: 100, Depth: 4, N_train: 800\n",
      "Width: 100, Depth: 1, N_train: 60000\n",
      "Width: 100, Depth: 1, N_train: 2500\n",
      "Width: 100, Depth: 1, N_train: 50\n",
      "Width: 85, Depth: 4, N_train: 8000\n",
      "Width: 85, Depth: 4, N_train: 200\n",
      "Width: 85, Depth: 1, N_train: 15000\n",
      "Width: 85, Depth: 1, N_train: 400\n",
      "Width: 75, Depth: 4, N_train: 30000\n",
      "Width: 75, Depth: 4, N_train: 800\n",
      "Width: 75, Depth: 1, N_train: 60000\n",
      "Width: 75, Depth: 1, N_train: 2500\n",
      "Width: 75, Depth: 1, N_train: 50\n",
      "Width: 50, Depth: 4, N_train: 8000\n",
      "Width: 50, Depth: 4, N_train: 200\n",
      "Width: 50, Depth: 1, N_train: 15000\n",
      "Width: 50, Depth: 1, N_train: 400\n",
      "Width: 40, Depth: 4, N_train: 30000\n",
      "Width: 40, Depth: 4, N_train: 800\n",
      "Width: 40, Depth: 1, N_train: 60000\n",
      "Width: 40, Depth: 1, N_train: 2500\n",
      "Width: 40, Depth: 1, N_train: 50\n",
      "Width: 5000, Depth: 4, N_train: 8000\n",
      "Width: 5000, Depth: 4, N_train: 200\n",
      "Width: 5000, Depth: 1, N_train: 15000\n",
      "Width: 5000, Depth: 1, N_train: 400\n",
      "Width: 3000, Depth: 4, N_train: 30000\n",
      "Width: 3000, Depth: 4, N_train: 800\n",
      "Width: 3000, Depth: 1, N_train: 60000\n",
      "Width: 3000, Depth: 1, N_train: 2500\n",
      "Width: 3000, Depth: 1, N_train: 50\n",
      "Width: 2000, Depth: 4, N_train: 8000\n",
      "Width: 2000, Depth: 4, N_train: 200\n",
      "Width: 2000, Depth: 1, N_train: 15000\n",
      "Width: 2000, Depth: 1, N_train: 400\n",
      "Width: 800, Depth: 4, N_train: 30000\n",
      "Width: 800, Depth: 4, N_train: 800\n",
      "Width: 800, Depth: 1, N_train: 60000\n",
      "Width: 800, Depth: 1, N_train: 2500\n",
      "Width: 800, Depth: 1, N_train: 50\n",
      "Width: 600, Depth: 4, N_train: 8000\n",
      "Width: 600, Depth: 4, N_train: 200\n",
      "Width: 600, Depth: 1, N_train: 15000\n",
      "Width: 600, Depth: 1, N_train: 400\n",
      "Width: 400, Depth: 4, N_train: 30000\n",
      "Width: 400, Depth: 4, N_train: 800\n",
      "Width: 400, Depth: 1, N_train: 60000\n",
      "Width: 400, Depth: 1, N_train: 2500\n",
      "Width: 400, Depth: 1, N_train: 50\n",
      "Width: 150, Depth: 4, N_train: 8000\n",
      "Width: 150, Depth: 4, N_train: 200\n",
      "Width: 150, Depth: 1, N_train: 15000\n",
      "Width: 150, Depth: 1, N_train: 400\n",
      "Width: 120, Depth: 4, N_train: 30000\n",
      "Width: 120, Depth: 4, N_train: 800\n",
      "Width: 120, Depth: 1, N_train: 60000\n",
      "Width: 120, Depth: 1, N_train: 2500\n",
      "Width: 120, Depth: 1, N_train: 50\n",
      "Width: 100, Depth: 4, N_train: 8000\n",
      "Width: 100, Depth: 4, N_train: 200\n",
      "Width: 100, Depth: 1, N_train: 15000\n",
      "Width: 100, Depth: 1, N_train: 400\n",
      "Width: 85, Depth: 4, N_train: 30000\n",
      "Width: 85, Depth: 4, N_train: 800\n",
      "Width: 85, Depth: 1, N_train: 60000\n",
      "Width: 85, Depth: 1, N_train: 2500\n",
      "Width: 85, Depth: 1, N_train: 50\n",
      "Width: 75, Depth: 4, N_train: 8000\n",
      "Width: 75, Depth: 4, N_train: 200\n",
      "Width: 75, Depth: 1, N_train: 15000\n",
      "Width: 75, Depth: 1, N_train: 400\n",
      "Width: 50, Depth: 4, N_train: 30000\n",
      "Width: 50, Depth: 4, N_train: 800\n",
      "Width: 50, Depth: 1, N_train: 60000\n",
      "Width: 50, Depth: 1, N_train: 2500\n",
      "Width: 50, Depth: 1, N_train: 50\n",
      "Width: 40, Depth: 4, N_train: 8000\n",
      "Width: 40, Depth: 4, N_train: 200\n",
      "Width: 40, Depth: 1, N_train: 15000\n",
      "Width: 40, Depth: 1, N_train: 400\n",
      "Width: 4000, Depth: 4, N_train: 5000\n",
      "Width: 4000, Depth: 1, N_train: 40000\n",
      "Width: 4000, Depth: 1, N_train: 300\n",
      "Width: 1500, Depth: 4, N_train: 5000\n",
      "Width: 1500, Depth: 1, N_train: 40000\n",
      "Width: 1500, Depth: 1, N_train: 300\n",
      "Width: 1000, Depth: 4, N_train: 5000\n",
      "Width: 1000, Depth: 1, N_train: 40000\n",
      "Width: 1000, Depth: 1, N_train: 300\n",
      "Width: 300, Depth: 4, N_train: 5000\n",
      "Width: 300, Depth: 1, N_train: 40000\n",
      "Width: 300, Depth: 1, N_train: 300\n",
      "Width: 200, Depth: 4, N_train: 5000\n",
      "Width: 200, Depth: 1, N_train: 40000\n",
      "Width: 200, Depth: 1, N_train: 300\n",
      "Width: 500, Depth: 4, N_train: 5000\n",
      "Width: 500, Depth: 1, N_train: 40000\n",
      "Width: 500, Depth: 1, N_train: 300\n",
      "Width: 30, Depth: 4, N_train: 5000\n",
      "Width: 30, Depth: 1, N_train: 40000\n",
      "Width: 30, Depth: 1, N_train: 300\n",
      "Width: 20, Depth: 4, N_train: 5000\n",
      "Width: 20, Depth: 1, N_train: 40000\n",
      "Width: 20, Depth: 1, N_train: 300\n",
      "Width: 10, Depth: 4, N_train: 5000\n",
      "Width: 10, Depth: 1, N_train: 40000\n",
      "Width: 10, Depth: 1, N_train: 300\n",
      "Width: 8000, Depth: 4, N_train: 5000\n",
      "Width: 8000, Depth: 1, N_train: 40000\n",
      "Width: 8000, Depth: 1, N_train: 300\n",
      "Width: 4000, Depth: 4, N_train: 20000\n",
      "Width: 4000, Depth: 4, N_train: 100\n",
      "Width: 4000, Depth: 1, N_train: 1000\n",
      "Width: 1500, Depth: 4, N_train: 20000\n",
      "Width: 1500, Depth: 4, N_train: 100\n",
      "Width: 1500, Depth: 1, N_train: 1000\n",
      "Width: 1000, Depth: 4, N_train: 20000\n",
      "Width: 1000, Depth: 4, N_train: 100\n",
      "Width: 1000, Depth: 1, N_train: 1000\n",
      "Width: 300, Depth: 4, N_train: 20000\n",
      "Width: 300, Depth: 4, N_train: 100\n",
      "Width: 300, Depth: 1, N_train: 1000\n",
      "Width: 200, Depth: 4, N_train: 20000\n",
      "Width: 200, Depth: 4, N_train: 100\n",
      "Width: 200, Depth: 1, N_train: 1000\n",
      "Width: 500, Depth: 4, N_train: 20000\n",
      "Width: 500, Depth: 4, N_train: 100\n",
      "Width: 500, Depth: 1, N_train: 1000\n",
      "Width: 30, Depth: 4, N_train: 20000\n",
      "Width: 30, Depth: 4, N_train: 100\n",
      "Width: 30, Depth: 1, N_train: 1000\n",
      "Width: 20, Depth: 4, N_train: 20000\n",
      "Width: 20, Depth: 4, N_train: 100\n",
      "Width: 20, Depth: 1, N_train: 1000\n",
      "Width: 10, Depth: 4, N_train: 20000\n",
      "Width: 10, Depth: 4, N_train: 100\n",
      "Width: 10, Depth: 1, N_train: 1000\n",
      "Width: 8000, Depth: 4, N_train: 20000\n",
      "Width: 8000, Depth: 4, N_train: 100\n",
      "Width: 8000, Depth: 1, N_train: 1000\n",
      "\n",
      "Found train sizes: [10, 50, 100, 200, 300, 400, 500, 800, 1000, 2500, 5000, 8000, 10000, 15000, 20000, 30000, 40000, 60000]\n",
      "Found hidden sizes: [10, 20, 30, 40, 50, 75, 85, 100, 120, 150, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000, 3000, 4000, 5000, 8000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2561990/2986714854.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "#### working low_dim\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_science_style(font_size: float = 8):\n",
    "    \"\"\"Set up consistent science styling with proper log-scale ticks.\"\"\"\n",
    "    plt.style.use('default')\n",
    "    mpl.rcParams.update({\n",
    "        # Font sizes\n",
    "        'font.size': font_size,\n",
    "        'axes.labelsize': font_size,\n",
    "        'xtick.labelsize': font_size,\n",
    "        'ytick.labelsize': font_size,\n",
    "        'legend.fontsize': font_size,\n",
    "        'figure.figsize': (4, 2.7),\n",
    "        'figure.dpi': 100,\n",
    "        \n",
    "        # Font settings\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['cmr10', 'Computer Modern Serif', 'DejaVu Serif'],\n",
    "        'text.usetex': False,\n",
    "        'axes.formatter.use_mathtext': True,\n",
    "        'mathtext.fontset': 'cm',\n",
    "        \n",
    "        # Axis settings\n",
    "        'axes.linewidth': 0.5,\n",
    "        'axes.spines.top': True,\n",
    "        'axes.spines.right': True,\n",
    "        'axes.spines.left': True,\n",
    "        'axes.spines.bottom': True,\n",
    "        \n",
    "        # Tick settings\n",
    "        'xtick.direction': 'in',\n",
    "        'ytick.direction': 'in',\n",
    "        'xtick.major.width': 0.5,\n",
    "        'ytick.major.width': 0.5,\n",
    "        'xtick.minor.width': 0.5,\n",
    "        'ytick.minor.width': 0.5,\n",
    "        'xtick.major.size': 3,\n",
    "        'ytick.major.size': 3,\n",
    "        'xtick.minor.size': 1.5,\n",
    "        'ytick.minor.size': 1.5,\n",
    "        'xtick.top': True,\n",
    "        'ytick.right': True,\n",
    "        \n",
    "        # Grid settings\n",
    "        'grid.linewidth': 0.5,\n",
    "        \n",
    "        # Line settings\n",
    "        'lines.linewidth': 2.0,\n",
    "        'lines.markersize': 3,\n",
    "        \n",
    "        # Legend settings\n",
    "        'legend.frameon': False,\n",
    "        'legend.borderpad': 0,\n",
    "        'legend.borderaxespad': 1.0,\n",
    "        'legend.handlelength': 1.0,\n",
    "        'legend.handletextpad': 0.5,\n",
    "    })\n",
    "\n",
    "def load_experiment_data(results_dir: str) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Load experiment results from rank-based result files\"\"\"\n",
    "    nn_files = list(Path(results_dir).glob(\"results*.json\"))\n",
    "    if not nn_files:\n",
    "        raise ValueError(f\"No result files found in {results_dir}\")\n",
    "    \n",
    "    combined_results = []\n",
    "    empty_files = []\n",
    "    \n",
    "    for file_path in nn_files:\n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    empty_files.append(file_path)\n",
    "                    continue\n",
    "                    \n",
    "                results = json.loads(content)\n",
    "                if isinstance(results, list):\n",
    "                    combined_results.extend(results)\n",
    "                else:\n",
    "                    combined_results.append(results)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if empty_files:\n",
    "        print(\"\\nThe following files were empty:\")\n",
    "        for f in empty_files:\n",
    "            print(f\"  - {f}\")\n",
    "            \n",
    "    print(f\"\\nTotal results loaded: {len(combined_results)}\")\n",
    "    \n",
    "    # Build hyperparams from defaults and available data\n",
    "    hyperparams = {\n",
    "        \"ambient_dim\": 20,  # Fixed for this dataset\n",
    "        \"hidden_sizes\": sorted(set(r['hidden_size'] for r in combined_results)),\n",
    "        \"depths\": sorted(set(r['depth'] for r in combined_results))\n",
    "    }\n",
    "    \n",
    "    # Add ambient_dim to all results\n",
    "    for result in combined_results:\n",
    "        result['ambient_dim'] = hyperparams['ambient_dim']\n",
    "    \n",
    "    return combined_results, hyperparams\n",
    "\n",
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, d: int, hidden_size: int, depth: int, mode: str = 'standard'):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.depth = depth\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dim = d\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = d\n",
    "        \n",
    "        for layer_idx in range(depth):\n",
    "            linear = torch.nn.Linear(prev_dim, hidden_size)\n",
    "            \n",
    "            if mode == 'standard':\n",
    "                torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                torch.nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            layers.extend([\n",
    "                linear,\n",
    "                torch.nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_size\n",
    "        \n",
    "        final_layer = torch.nn.Linear(prev_dim, 1)\n",
    "        if mode == 'standard':\n",
    "            torch.nn.init.xavier_uniform_(final_layer.weight)\n",
    "        torch.nn.init.zeros_(final_layer.bias)\n",
    "        layers.append(final_layer)\n",
    "        \n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def find_model_file(results_dir: str, model_prefix: str, rank: int) -> Optional[str]:\n",
    "    \"\"\"Find a model file matching the prefix and rank across different timestamps\"\"\"\n",
    "    timestamps = [\"20241230_145221\", \"20241230_145424\"]\n",
    "    patterns = [\n",
    "        f\"final_model_{model_prefix}_{{}}_rank{rank}.pt\",\n",
    "        f\"initial_model_{model_prefix}_{{}}_rank{rank}.pt\"\n",
    "    ]\n",
    "    \n",
    "    for timestamp in timestamps:\n",
    "        for pattern in patterns:\n",
    "            filename = pattern.format(timestamp)\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                return filepath\n",
    "    return None\n",
    "\n",
    "def load_model(results_dir: str, result: Dict) -> Optional[torch.nn.Module]:\n",
    "    \"\"\"Load just the model without dataset\"\"\"\n",
    "    try:\n",
    "        hidden_size = result['hidden_size']\n",
    "        depth = result['depth']\n",
    "        n_train = result['n_train']\n",
    "        lr = result.get('learning_rate', result.get('lr'))\n",
    "        mode = result.get('mode', 'mup_pennington')\n",
    "        rank = result.get('worker_rank', 0)\n",
    "        ambient_dim = result.get('ambient_dim', 20)\n",
    "        \n",
    "        model_prefix = f'h{hidden_size}_d{depth}_n{n_train}_lr{lr}_{mode}'\n",
    "        \n",
    "        model_path = find_model_file(results_dir, model_prefix, rank)\n",
    "        \n",
    "        if not model_path:\n",
    "            print(f\"Warning: No model file found for prefix: h{hidden_size}_d{depth}_n{n_train}_lr{lr}_{mode}, rank: {rank}\")\n",
    "            return None\n",
    "        \n",
    "        model = DeepNN(ambient_dim, hidden_size, depth)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_feature_dimensionality(model: torch.nn.Module) -> np.ndarray:\n",
    "    \"\"\"Analyze feature dimensionality for layer 1 of the model\"\"\"\n",
    "    first_layer = None\n",
    "    for layer in model.network:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            first_layer = layer\n",
    "            break\n",
    "    \n",
    "    if first_layer is None:\n",
    "        raise ValueError(\"No linear layer found in model\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        features = first_layer.weight.data\n",
    "        feature_norms = torch.norm(features, dim=0, keepdim=True)\n",
    "        normalized_features = features / (feature_norms + 1e-8)\n",
    "        feature_dots = normalized_features.T @ features\n",
    "        feature_dots_squared = feature_dots ** 2\n",
    "        feature_denominator = torch.sum(feature_dots_squared, dim=1)\n",
    "        feature_numerator = torch.sum(features * features, dim=0)\n",
    "        feature_dims = feature_numerator / (feature_denominator + 1e-8)\n",
    "        \n",
    "        return feature_dims.cpu().numpy()\n",
    "\n",
    "def plot_low_dim_ratio(results: List[Dict], results_dir: str, threshold: float = 0.2, font_size: float = 8):\n",
    "    \"\"\"Plot ratio of features with dimensionality below threshold vs training size\"\"\"\n",
    "    # Set up the science style with font size\n",
    "    setup_science_style(font_size=font_size)\n",
    "    \n",
    "    train_sizes = sorted(set(r['n_train'] for r in results))\n",
    "    hidden_sizes = sorted(set(r['hidden_size'] for r in results))\n",
    "    \n",
    "    print(\"\\nFound train sizes:\", train_sizes)\n",
    "    print(\"Found hidden sizes:\", hidden_sizes)\n",
    "    \n",
    "    # Create figure with specific size\n",
    "    plt.figure(figsize=(7.5, 3.9))\n",
    "    \n",
    "    # Create an evenly spaced colormap from purple to cyan\n",
    "    num_sizes = len(hidden_sizes)\n",
    "    colors = []\n",
    "    \n",
    "    # Define start and end colors\n",
    "    start_rgb = np.array([238, 0, 255])   # #ee00ff (purple)\n",
    "    end_rgb = np.array([0, 251, 255])     # #00fbff (cyan)\n",
    "    \n",
    "    for i in range(num_sizes):\n",
    "        t = i / max(1, num_sizes - 1)\n",
    "        rgb = start_rgb * (1 - t) + end_rgb * t\n",
    "        rgb = rgb.astype(int)\n",
    "        colors.append(f'#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}')\n",
    "    \n",
    "    for idx, hidden_size in enumerate(hidden_sizes):\n",
    "        ratios = []\n",
    "        valid_train_sizes = []\n",
    "        for n_train in train_sizes:\n",
    "            matching_results = [r for r in results \n",
    "                              if r['hidden_size'] == hidden_size and r['n_train'] == n_train]\n",
    "            \n",
    "            if matching_results:\n",
    "                result = matching_results[0]\n",
    "                model = load_model(results_dir, result)\n",
    "                \n",
    "                if model is None:\n",
    "                    print(f\"Could not load model for width={hidden_size}, n_train={n_train}\")\n",
    "                \n",
    "                if model is not None:\n",
    "                    feature_dims = analyze_feature_dimensionality(model)\n",
    "                    ratio = np.mean(feature_dims < threshold)\n",
    "                    ratios.append(ratio)\n",
    "                    valid_train_sizes.append(n_train)\n",
    "        \n",
    "        if ratios:\n",
    "            plt.plot(valid_train_sizes, ratios, '-s', \n",
    "                    color=colors[idx % len(colors)],\n",
    "                    linewidth=2.0,\n",
    "                    markersize=3,\n",
    "                    label=f'$N={hidden_size}$')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(r'Training Set Size $m$', labelpad=2)\n",
    "    plt.ylabel(r'Ratio of Features with Dim $< 0.2$', labelpad=2)\n",
    "    \n",
    "    # Configure ticks\n",
    "    ax = plt.gca()\n",
    "    ax.minorticks_on()\n",
    "    ax.tick_params(which='both', direction='in')\n",
    "    \n",
    "    # Add grid with low opacity\n",
    "    plt.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    # Move legend outside\n",
    "    plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left', fontsize=font_size)\n",
    "    \n",
    "    # Adjust layout to prevent legend cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def main():\n",
    "    # Set your paths here\n",
    "    results_dir = \"/mnt/users/goringn/NNs_vs_Kernels/low_dim_poly/results/low_dim_poly_NN_2812_mup_lr0001\"\n",
    "    output_dir = \"analysis_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load results\n",
    "    nn_results, hyperparams = load_experiment_data(results_dir)\n",
    "    print(f\"\\nLoaded {len(nn_results)} NN results\")\n",
    "    print(\"\\nUnique configurations found:\")\n",
    "    for result in nn_results:\n",
    "        print(f\"Width: {result['hidden_size']}, Depth: {result['depth']}, N_train: {result['n_train']}\")\n",
    "    \n",
    "    # Create low-dimensional ratio plot with custom font size\n",
    "    # You can adjust this value to make fonts larger or smaller\n",
    "    ratio_fig = plot_low_dim_ratio(nn_results, results_dir, font_size=7)\n",
    "    \n",
    "    # Save as PDF with high quality\n",
    "    ratio_fig.savefig(os.path.join(output_dir, 'low_dim_ratio.pdf'), \n",
    "                     format='pdf',\n",
    "                     dpi=300, \n",
    "                     bbox_inches='tight',\n",
    "                     facecolor='white',\n",
    "                     edgecolor='black')\n",
    "    \n",
    "    plt.close('all')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dev_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
